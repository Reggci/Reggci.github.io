<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DETR深度理解</title>
      <link href="/2022/04/28/paper/DETR%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/"/>
      <url>/2022/04/28/paper/DETR%E6%B7%B1%E5%BA%A6%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<center> End-to-End Object Detection with Transformers </center><blockquote><ol><li>因为预测是模型预测，而标记是人为标记，得到的预测第一个不一定就直接与认为标记好的目标直接对应，如何将预测到的[class, box]和GT对应？</li></ol></blockquote><p>首先用二分图的方法进行信息匹配，匹配预测和标记之间相似程度，每一个预测目标只能与一个GT对应。对应好后进行损失计算</p><blockquote><ol start="2"><li>decoder输入的object queries如何理解？</li></ol></blockquote><ol><li>可理解成起控制decoder输出各式的作用，因为其本来就是随机初始化，自学习的token</li><li>本文经过可视化后，可以理解为每个token以不同的角度去分析encoder编码后的token信息</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像修改检测</title>
      <link href="/2022/04/28/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E6%94%B9%E6%A3%80%E6%B5%8B/"/>
      <url>/2022/04/28/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E6%94%B9%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="ObjectFormer-for-Image-Manipulation-Detection-and-Localization"><a href="#ObjectFormer-for-Image-Manipulation-Detection-and-Localization" class="headerlink" title=" ObjectFormer for Image Manipulation Detection and Localization "></a><center> ObjectFormer for Image Manipulation Detection and Localization </center></h1><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>现有的图像篡改检测方法通常使用CNN (卷积神经网络) 提取图像内的篡改信息，它们往往关注图像层面（高层）或者像素层面（底层）的视觉一致信息，而没有明确地对物体层面（中层）的表示进行建模。相比之下，我们认为，图像操纵检测不仅要检查某些像素是否具有异常，还要考虑物体之间是否一致。</p><h1 id="技术基础"><a href="#技术基础" class="headerlink" title="技术基础"></a>技术基础</h1><ol><li>transformer能探索不同patch之间的相关性——用于明确地建模物体层面地视觉一致性信息。</li><li>使用可学习的参数作为query，用于发现对象一致性作为中层表征，这能够表征两个特征中特定区域地一致性，通过这种一致性来改进patch-level的一致性重建</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像的高低频</title>
      <link href="/2022/04/28/work/%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E4%BD%8E%E9%A2%91/"/>
      <url>/2022/04/28/work/%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E4%BD%8E%E9%A2%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>不同频率信息在图像结构中有不同的作用。图像的主要成分是低频信息，它形成了图像的基本灰度等级，对图像结构的决定作用较小；中频信息决定了图像的基本结构，形成了图像的主要边缘结构；高频信息形成了图像的边缘和细节，是在中频信息上对图像内容的进一步强化。</p></blockquote><p>图像的频率：灰度值变化剧烈程度的指标，是灰度在平面空间上的梯度。</p><h1 id="什么是低频"><a href="#什么是低频" class="headerlink" title="什么是低频?"></a>什么是低频?</h1><p>低频就是颜色缓慢地变化,也就是灰度缓慢地变化,就代表着那是连续渐变的一块区域,这部分就是低频. 对于一幅图像来说，除去高频的就是低频了，也就是边缘以内的内容为低频，而边缘内的内容就是图像的大部分信息，即图像的大致概貌和轮廓，是图像的近似信息。</p><h1 id="什么是高频"><a href="#什么是高频" class="headerlink" title="什么是高频?"></a>什么是高频?</h1><p>反过来, 高频就是频率变化快.图像中什么时候灰度变化快?就是相邻区域之间灰度相差很大,这就是变化得快.图像中,一个影像与背景的边缘部位,通常会有明显的差别,也就是说变化那条边线那里,灰度变化很快,也即是变化频率高的部位.因此，图像边缘的灰度值变化快，就对应着频率高，即高频显示图像边缘。图像的细节处也是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。</p><p>另外噪声（即噪点）也是这样,在一个像素所在的位置,之所以是噪点,就是因为它与正常的点颜色不一样了，也就是说该像素点灰度值明显不一样了,,也就是灰度有快速地变化了,所以是高频部分，因此有噪声在高频这么一说。</p><p>其实归根到底,是因为我们人眼识别物体就是这样的.假如你穿一个红衣服在红色背景布前拍照,你能很好地识别么?不能,因为衣服与背景融为一体了,没有变化,所以看不出来,除非有灯光从某解度照在人物身上,这样边缘处会出现高亮和阴影,这样我们就能看到一些轮廓线,这些线就是颜色（即灰度）很不一样的地方.</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>self-attention意义分析</title>
      <link href="/2022/04/27/work/self-attention%E6%84%8F%E4%B9%89%E5%88%86%E6%9E%90/"/>
      <url>/2022/04/27/work/self-attention%E6%84%8F%E4%B9%89%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p>Attention Function函数如下：<br>$$ \operatorname{Attention}(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$</p><ol><li>$ QK^{T} $是计算相似度没错，但是为什么这样相乘能计算相似度？由此引出以下问题：</li></ol><blockquote><ul><li>向量的内积是什么？如何计算，几何意义是什么？</li></ul></blockquote><p>向量内积表征两个向量的夹角，表征一个向量在另一个向量上的投影，而这个投影值大，意味着两个向量的相关度高；如果两个向量的夹角是90°，那么这两个向量线性无关。</p><blockquote><ul><li>矩阵与自身转置相乘$ XX^{T} $，代表什么意义？</li></ul></blockquote><p>计算上理解：第一行与其它行内积、第二行与其它行内积…<br>意义上理解：计算每行与其他行的相似度，相当于遍历每行与其他行的相似度，得到的是一个相似度矩阵，其第一行第i个元素对应着原矩阵第一行与第i行相似度。<br>应用上理解：当图片切成patch后，矩阵中的每一行代表每个patch中的所有像素值，计算时相当于计算每个patch与其他patch之间的相似度</p><blockquote><ul><li>相似度矩阵再乘以原矩阵如何理解？</li></ul></blockquote><p>计算上理解：相似度矩阵第一行是原矩阵第一行与其它行的相似度，原矩阵第一列是每行的第一个数值，全部乘积完后的第一行，相当于以第一行为基准，用第一行和其它行相似度对应着调整其它行的占比。<br>应用上理解：用第一个patch与其他patch的相似度调整原矩阵中的第一行（第一个patch），而调整后的第一行是以相似度为基准，对所有patch的加权。</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>张量的广播性质</title>
      <link href="/2022/04/26/work/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E6%80%A7%E8%B4%A8/"/>
      <url>/2022/04/26/work/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E6%80%A7%E8%B4%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="维度解读"><a href="#维度解读" class="headerlink" title="维度解读"></a>维度解读</h1><ul><li>[3, 4, 5]<blockquote><p>该张量包含3个二维张量，每个二维张量都包含4个一维张量，每个一维张量都包含5个标量&#x2F;零维张量</p></blockquote><h1 id="相同维度但不同形状"><a href="#相同维度但不同形状" class="headerlink" title="相同维度但不同形状"></a>相同维度但不同形状</h1><h2 id="1、某个维度的形状不同且其中有一个维度为1"><a href="#1、某个维度的形状不同且其中有一个维度为1" class="headerlink" title="1、某个维度的形状不同且其中有一个维度为1"></a>1、某个维度的形状不同且其中<mark>有一个维度为1</mark></h2><blockquote><p>如形状为[1,4]和[3,4]的两个张量相加相当于分别将a与b的三个一维张量相加（实际上广播的操作是将a的维度扩展，复制其二维数据成三份，此时其尺寸和b相同）</p></blockquote></li></ul><ol><li>[1,4]和[3,4]、[4,1]和[4,3]</li><li>[1,4]和[3,1]也可以相加，相加前相当于分别扩展为[3,4]进行加和操作</li><li>[2,3]和[3,2,1]</li></ol><ul><li>注意：形状不相同的维度只要某个维度为1就能广播，单纯的形状不同且其中没有任何一个维度为1则不能广播</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers</title>
      <link href="/2022/04/25/paper/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B21/"/>
      <url>/2022/04/25/paper/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B21/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Affinity-from-Attention-End-to-End-Weakly-Supervised-Semantic-Segmentation-with-Transformers"><a href="#Learning-Affinity-from-Attention-End-to-End-Weakly-Supervised-Semantic-Segmentation-with-Transformers" class="headerlink" title=" Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers "></a><center> Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers </center></h1><blockquote><p>将transformer引入图像分割，使用MHSA得到的激活图作为预处伪标签，其作为标签还需要细化预伪标签。</p><ol><li>AFA模块来学习transformer中MHSA中的语义亲和力</li><li>自适应细化模块PAR用来有效地控制AFA生成亲和力标签的可靠性并确保伪标签的局部一致性</li></ol></blockquote><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ol><li>语义分割通常需要大量标记labels，故提出使用弱&#x2F;廉价标签，常用的方法是：首先使用一个分类模型生成类激活图作为伪标签，再经过细化后才能被用来作为语义分割网络的训练标签——这意味着需要训练多个模型</li><li>AFA能学习MHSA中的语义亲和力用于改进初始伪标签，但为了确保这种改进的可靠和局部一致性，提出自适应细化模块</li></ol><blockquote><p>模型思想：</p><ol><li>Transformer作为backbone，输入图像</li><li>分支一：提取由transformer得到的语义亲和力信息，通过MLP进行语义亲和力预测得到亲和力特征信息A，</li><li>分支二：对transformer提取到的特征图直接进行分类层，其一将其通过类激活得到初始伪标签B，该标签再由PAR细化处理并得到亲和力特征信息C，[A,C]计算亲和力信息损失；其二经由类别预测直接输出类别损失</li><li>分支三：将由transformer得到的特征图经由decoder重建得到分割特征的预测信息D,将[A, B]进行Random wolk后再PAR细化得到分割模型能用来训练的最终伪标签E，计算[D,E]的预测损失</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimMIM：a Simple Framework for Masked Image Modeling</title>
      <link href="/2022/04/23/paper/%E5%9B%BE%E5%83%8F%E6%8E%A9%E8%86%9C/"/>
      <url>/2022/04/23/paper/%E5%9B%BE%E5%83%8F%E6%8E%A9%E8%86%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="SimMIM-a-Simple-Framework-for-Masked-Image-Modeling"><a href="#SimMIM-a-Simple-Framework-for-Masked-Image-Modeling" class="headerlink" title=" SimMIM:a Simple Framework for Masked Image Modeling "></a><center> SimMIM:a Simple Framework for Masked Image Modeling </center></h1><blockquote><p>本文模型不需要特殊设计，使用简单的离散VAE或聚类实现masking和Tokenization就行<br>探究图像掩膜模型表征好的原因:</p><ol><li>使用中等大小（32）的掩膜对图像随机掩码</li><li>直接对原始像素进行回归预测</li><li>预测头架构简单，且性能不比复杂层差</li></ol></blockquote><h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p>[1]——使用附加网络进行patch tokenization，通过掩膜来破坏短距离连接</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>1.1 掩码的自监督学习范式广泛应用于NLP领域却在计算机视觉表现的不温不火的原因：</p><ol><li>图像具有更强的局部关系，即相互靠近的像素往往是高度相关的。复制靠近的像素可以很好地完成任务，显然通过语义推理并不容易办到。</li><li>视觉信号是原始的、低层次的，而文本分词是由人类产生的高级概念。那么，对低层次信号的预测是否对高层次的视觉识别任务有用？ </li><li>视觉信号是连续的，而文本分词是离散的。那么，如何利用基于分类的掩码语言建模方法处理连续的视觉信号？</li></ol><p>【注意】：掩膜大小为32在很宽的掩膜率范围内性能都能表现的富有竞争力，但是掩膜大小为8就需要掩膜率高达80%才表现良好</p><p>模型共有四个部分：masking strategy——encoder architecture——prediction head</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformer的意义在哪里</title>
      <link href="/2022/04/21/work/transformer%E7%9A%84%E6%84%8F%E4%B9%89%E5%9C%A8%E5%93%AA%E9%87%8C/"/>
      <url>/2022/04/21/work/transformer%E7%9A%84%E6%84%8F%E4%B9%89%E5%9C%A8%E5%93%AA%E9%87%8C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>1.transformer是如何出现的<br>2.transformer如何使用？</p></blockquote><h1 id="transformer的背景"><a href="#transformer的背景" class="headerlink" title="transformer的背景"></a>transformer的背景</h1><ul><li>transformer基于self-attention<ul><li>self-attention比CNN看的更宽更远(感受野大，同时是全局远距离关联)</li><li>self-attention比lstm训练更快</li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>卷积</title>
      <link href="/2022/04/19/work/%E5%8D%B7%E7%A7%AF/"/>
      <url>/2022/04/19/work/%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="转置卷积Transposed-Convolution"><a href="#转置卷积Transposed-Convolution" class="headerlink" title="转置卷积Transposed Convolution"></a>转置卷积Transposed Convolution</h1><blockquote><p>用来对图像进行上采样，扩大图像尺寸</p></blockquote><h2 id="运算步骤："><a href="#运算步骤：" class="headerlink" title="运算步骤："></a>运算步骤：</h2><ol><li>在输入特征图元素间填充s-1行、列0</li><li>在输入特征图四周填充k-p-1行、列0</li><li>将卷积核参数上下、左右翻转</li><li>做正常卷积运算（填充0，步距1）</li></ol><h1 id="膨胀卷积Dilated-convolution"><a href="#膨胀卷积Dilated-convolution" class="headerlink" title="膨胀卷积Dilated convolution"></a>膨胀卷积Dilated convolution</h1><blockquote><p>保证图像尺寸不变，但感受野扩大的卷积</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数以零为中心的意义</title>
      <link href="/2022/04/19/work/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%A5%E9%9B%B6%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E6%84%8F%E4%B9%89/"/>
      <url>/2022/04/19/work/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%A5%E9%9B%B6%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在很多介绍激活函数的文章中看到一条：该函数不以零为中心。并将其列为该激活函数的缺点</p></blockquote><p>要说明激活函数以零为中心的意义，需要做以下证明：</p><ul><li>某次反向传播的方向实则仅与输入的值相关。</li></ul><h1 id="证明："><a href="#证明：" class="headerlink" title="证明："></a>证明：</h1><h2 id="1-神经元单元中输入输出步骤说明"><a href="#1-神经元单元中输入输出步骤说明" class="headerlink" title="1. 神经元单元中输入输出步骤说明"></a>1. 神经元单元中输入输出步骤说明</h2><p>神经网络完全仿照人类大脑的神经元，在某个神经元 $ i $ ：上个神经元的输出作为本次输入 $ X_i $ ，经过神经元 $ i $ 整合之后： $ z(\vec{x};\vec{w},b) &#x3D; \sum_i w_ix_i + b $ ,再通过激活 $ f(z) $得到输出。 </p><h2 id="2-sigmoid和tanh"><a href="#2-sigmoid和tanh" class="headerlink" title="2. sigmoid和tanh"></a>2. sigmoid和tanh</h2><p>本文为说明激活函数以零为中心的意义，选用sigmoid和tanh对比说明</p><ul><li>区别</li></ul><ol><li>sigmoid区间为[0,1]，但tanh以零为中心</li></ol><ul><li>相同（在本文中无关，但是当时困扰我很多）</li></ul><ol><li>其导数均大于0</li></ol><h2 id="3-收敛速度"><a href="#3-收敛速度" class="headerlink" title="3. 收敛速度"></a>3. 收敛速度</h2><p>为得到最优解，迭代轮数多即收敛速度慢；反之</p><h2 id="4-深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。"><a href="#4-深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。" class="headerlink" title="4. 深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。"></a>4. 深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。</h2><p>此处为公式，后期补充</p><h2 id="5-通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关"><a href="#5-通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关" class="headerlink" title="5. 通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关"></a>5. 通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关</h2><h2 id="6-以零为中心的实际影响"><a href="#6-以零为中心的实际影响" class="headerlink" title="6. 以零为中心的实际影响"></a>6. 以零为中心的实际影响</h2><p>如果输入值像sigmoid一样为正数，那么某次反向传播中整体只能向某个方向变化，现在假设：</p><p>即我们希望 $ w_1 $ 适当增大， $ w_2 $ 适当减小，这时就出现一个问题，像上述一样，在某个神经元中梯度实际上是确定的，整体上数值的方向仅取决于输入值的方向，一旦输入值方向都相同（因为输入值是上个神经元的输出，而这个输出的最后一步通过了激活函数）——sigmoid，那么此次更新只能整体朝一个方向更新，那么就无法做到增大 $ w_1 $ 的同时减小 $ w_2 $ ,因为这样需要输入值可正可负（但sigmoid只大于零，这里引出一个问题，下节说明）<br>使用sigmoid这样不以零为中心的激活函数（只有正值或负值），神经网络在某次更新中无法同时满足所有的权重向最优解移动（即方向不是直指最优解），而是在最优解那个方向上左右摇动，这样导致收敛速度缓慢。</p><h2 id="7-当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？"><a href="#7-当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？" class="headerlink" title="7. 当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？"></a>7. 当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？</h2><p>所谓的方向全由输入 $ X_i $ 决定，前提是将其定义在某个神经元单元中考虑输入 $ X_i $ 对方向影响的问题来说，因为本文就是说明激活函数以零为中心的意义，刚好神经元的输入就是上层神经元输出，最重要的是该输出的最后一步是通过了激活函数，我们要用的就是经过激活函数的输出。</p><p>但是脱离了这个限制，实则某次反向传播的最终方向是由下方公式共同决定的，这说明了其中$\frac{\partial L}{\partial f}\frac{\partial f}{\partial z}$是可正可负的，它在整个更新的过程中是起作用的。<br>$$\left{\begin{array}{l}<br>\boldsymbol{x}<em>{i} \cdot \frac{\partial L}{\partial f} \frac{\partial f}{\partial z} \<br>\boldsymbol{x}</em>{j} \cdot \frac{\partial L}{\partial f} \frac{\partial f}{\partial z}<br>\end{array}\right.$$</p><h2 id="8-在sigmoid前提中对于梯度更新方向的进一步解析"><a href="#8-在sigmoid前提中对于梯度更新方向的进一步解析" class="headerlink" title="8. 在sigmoid前提中对于梯度更新方向的进一步解析"></a>8. 在sigmoid前提中对于梯度更新方向的进一步解析</h2><p>上一节中说明，方向是由下方公式共同决定:</p><p>$$x_i\cdot \frac{\partial L}{\partial f}\frac{\partial f}{\partial z}$$</p><p>而在sigmoid函数中,该函数的导数实际上是大于零：</p><p>$$\frac{\partial f}{\partial z}$$</p><p>那么此时更新方向只与输入和loss的导数相关，此时将问题再次限制到某个神经元单元中，loss导数固定，只与输入有关，对于ReLU这样的激活函数也存在同时只更新一个方向的问题，由此便可说明其变种leakyReLU、PReLU等出现的原因——它们的出现让输入可正可负。</p><p>本文实际上已经说明问题，公式补充请参考<a href="https://liam.page/2018/04/17/zero-centered-active-function/">博客</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>心累</title>
      <link href="/2022/04/18/life/%E5%BF%83%E7%B4%AF/"/>
      <url>/2022/04/18/life/%E5%BF%83%E7%B4%AF/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="a72e971753ffe09e9919f440d6efbafb5f7df4d2f9d8db6dcec075954f932315">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visdom教程</title>
      <link href="/2022/04/17/work/Visdom%E6%95%99%E7%A8%8B/"/>
      <url>/2022/04/17/work/Visdom%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install visdom</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开端口</span></span><br><span class="line">python -m visdom.server</span><br></pre></td></tr></table></figure><h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><h2 id="vis-test-vis-image"><a href="#vis-test-vis-image" class="headerlink" title="vis.test(),vis.image()"></a>vis.test(),vis.image()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom   <span class="comment"># 添加visdom库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np   <span class="comment"># 添加numpy库</span></span><br><span class="line">vis = visdom.Visdom(env= <span class="string">&#x27;test&#x27;</span>)   <span class="comment"># 设置环境窗口的名称,如果不设置名称就默认为main</span></span><br><span class="line">vis.text( <span class="keyword">class</span>=<span class="string">&#x27;test&#x27;</span>, win=<span class="string">&#x27;main&#x27;</span>)   <span class="comment"># 使用文本输出</span></span><br><span class="line">vis.image(np.ones(( <span class="number">3</span>,  <span class="number">100</span>,  <span class="number">100</span>)))   <span class="comment"># 绘制一幅尺寸为3 * 100 * 100的图片，图片的像素值全部为1</span></span><br></pre></td></tr></table></figure><h2 id="vis-images"><a href="#vis-images" class="headerlink" title="vis.images()"></a>vis.images()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 新建一个连接客户端</span></span><br><span class="line"><span class="comment"># 指定env = &#x27;test1&#x27;，默认是&#x27;main&#x27;,注意在浏览器界面做环境的切换</span></span><br><span class="line">vis = visdom.Visdom(env=<span class="string">&#x27;test1&#x27;</span>)</span><br><span class="line"><span class="comment"># 绘制正弦函数</span></span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">100</span>, <span class="number">0.01</span>)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line">vis.line(X=x,Y=y, win=<span class="string">&#x27;sinx&#x27;</span>,opts=&#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;y=sin(x)&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># 绘制36张图片随机的彩色图片</span></span><br><span class="line">vis.images(torch.randn(<span class="number">36</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>).numpy(),nrow=<span class="number">6</span>, win=<span class="string">&#x27;imgs&#x27;</span>,opts=&#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;imgs&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure><h2 id="绘制loss函数"><a href="#绘制loss函数" class="headerlink" title="绘制loss函数"></a>绘制loss函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制loss变化趋势，参数一为Y轴的值，参数二为X轴的值，参数三为窗体名称，参数四为表格名称，参数五为更新选项，从第二个点开始可以更新</span></span><br><span class="line">vis.line(Y=np.array([totalloss.item()]), X=np.array([traintime]),</span><br><span class="line">                win=(<span class="string">&#x27;train_loss&#x27;</span>),</span><br><span class="line">                opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train_loss&#x27;</span>),</span><br><span class="line">                update=<span class="literal">None</span> <span class="keyword">if</span> traintime == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;append&#x27;</span></span><br><span class="line">                )</span><br></pre></td></tr></table></figure><h2 id="在一张图上绘制多条loss曲线"><a href="#在一张图上绘制多条loss曲线" class="headerlink" title="在一张图上绘制多条loss曲线"></a>在一张图上绘制多条loss曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化窗口</span></span><br><span class="line">wind = Visdom()</span><br><span class="line"><span class="comment"># 初始化窗口参数</span></span><br><span class="line">wind.line([&#123;<span class="number">0.</span>,<span class="number">0.</span>]],[<span class="number">0.</span>],win = <span class="string">&#x27;train&#x27;</span>,opts = <span class="built_in">dict</span>(title = <span class="string">&#x27;loss&amp;acc&#x27;</span>,legend = [<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;acc&#x27;</span>]))</span><br><span class="line"><span class="comment"># 更新窗口数据</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">loss = <span class="number">0.2</span> * np.random.randn() + <span class="number">1</span></span><br><span class="line">acc = <span class="number">0.1</span> * np.random.randn() + <span class="number">0.5</span></span><br><span class="line">wind.line([[loss, acc]],[step],win = <span class="string">&#x27;train&#x27;</span>,update = <span class="string">&#x27;append&#x27;</span>)</span><br><span class="line">time.sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="实际代码"><a href="#实际代码" class="headerlink" title="实际代码"></a>实际代码</h2><blockquote><p>类定义：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录训练日志，显示生成图，画loss曲线 的类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Logger</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_epochs, batches_epoch</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param n_epochs:  跑多少个epochs</span></span><br><span class="line"><span class="string">        :param batches_epoch:  一个epoch有几个batches</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.viz = Visdom() <span class="comment"># 默认env是main函数</span></span><br><span class="line">        self.n_epochs = n_epochs</span><br><span class="line">        self.batches_epoch = batches_epoch</span><br><span class="line">        self.epoch = <span class="number">1</span> <span class="comment"># 当前epoch数</span></span><br><span class="line">        self.batch = <span class="number">1</span> <span class="comment"># 当前batch数</span></span><br><span class="line">        self.prev_time = time.time()</span><br><span class="line">        self.mean_period = <span class="number">0</span></span><br><span class="line">        self.losses = &#123;&#125;</span><br><span class="line">        self.loss_windows = &#123;&#125; <span class="comment"># 保存loss图的字典集合</span></span><br><span class="line">        self.image_windows = &#123;&#125; <span class="comment"># 保存生成图的字典集合</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, losses=<span class="literal">None</span>, images=<span class="literal">None</span></span>):</span><br><span class="line">        self.mean_period += (time.time() - self.prev_time)</span><br><span class="line">        self.prev_time = time.time()</span><br><span class="line"></span><br><span class="line">        sys.stdout.write(<span class="string">&#x27;Epoch %03d/%03d [%04d/%04d] -- &#x27;</span> % (self.epoch, self.n_epochs, self.batch, self.batches_epoch))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, loss_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(losses.keys()):</span><br><span class="line">            <span class="keyword">if</span> loss_name <span class="keyword">not</span> <span class="keyword">in</span> self.losses:</span><br><span class="line">                self.losses[loss_name] = losses[loss_name].data.item() <span class="comment">#这里losses[loss_name].data是个tensor（包在值外面的数据结构），要用item方法取值</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.losses[loss_name] = losses[loss_name].data.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) == <span class="built_in">len</span>(losses.keys()):</span><br><span class="line">                sys.stdout.write(<span class="string">&#x27;%s: %.4f -- &#x27;</span> % (loss_name, self.losses[loss_name]/self.batch))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sys.stdout.write(<span class="string">&#x27;%s: %.4f | &#x27;</span> % (loss_name, self.losses[loss_name]/self.batch))</span><br><span class="line"></span><br><span class="line">        batches_done = self.batches_epoch * (self.epoch - <span class="number">1</span>) + self.batch</span><br><span class="line">        batches_left = self.batches_epoch * (self.n_epochs - self.epoch) + self.batches_epoch - self.batch</span><br><span class="line">        sys.stdout.write(<span class="string">&#x27;ETA: %s&#x27;</span> % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示生成图</span></span><br><span class="line">        <span class="keyword">for</span> image_name, tensor <span class="keyword">in</span> images.items(): <span class="comment"># 字典.items()是以list形式返回键值对</span></span><br><span class="line">            <span class="keyword">if</span> image_name <span class="keyword">not</span> <span class="keyword">in</span> self.image_windows:</span><br><span class="line">                self.image_windows[image_name] = self.viz.image(tensor2image(tensor.data), opts=&#123;<span class="string">&#x27;title&#x27;</span>:image_name&#125;)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.viz.image(tensor2image(tensor.data), win=self.image_windows[image_name], opts=&#123;<span class="string">&#x27;title&#x27;</span>:image_name&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End of each epoch</span></span><br><span class="line">        <span class="keyword">if</span> (self.batch % self.batches_epoch) == <span class="number">0</span>: <span class="comment"># 一个epoch结束时</span></span><br><span class="line">            <span class="comment"># 绘制loss曲线图</span></span><br><span class="line">            <span class="keyword">for</span> loss_name, loss <span class="keyword">in</span> self.losses.items():</span><br><span class="line">                <span class="keyword">if</span> loss_name <span class="keyword">not</span> <span class="keyword">in</span> self.loss_windows:</span><br><span class="line">                    self.loss_windows[loss_name] = self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]),</span><br><span class="line">                                                                 opts=&#123;<span class="string">&#x27;xlabel&#x27;</span>:<span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;ylabel&#x27;</span>:loss_name, <span class="string">&#x27;title&#x27;</span>:loss_name&#125;)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), win=self.loss_windows[loss_name], update=<span class="string">&#x27;append&#x27;</span>) <span class="comment">#update=&#x27;append&#x27;可以使loss图不断更新</span></span><br><span class="line">                <span class="comment"># 每个epoch重置一次loss</span></span><br><span class="line">                self.losses[loss_name] = <span class="number">0.0</span></span><br><span class="line">            <span class="comment"># 跑完一个epoch，更新一下下面参数</span></span><br><span class="line">            self.epoch += <span class="number">1</span></span><br><span class="line">            self.batch = <span class="number">1</span></span><br><span class="line">            sys.stdout.write(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batch += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>调用代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">logger = Logger(opt.n_epochs, <span class="built_in">len</span>(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.epoch, opt.n_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练日志</span></span><br><span class="line">            <span class="comment"># Progress report (http://localhost:8097) 显示visdom画图的网址</span></span><br><span class="line">            logger.log(&#123;<span class="string">&#x27;loss_G&#x27;</span>: loss_G, <span class="string">&#x27;loss_G_identity&#x27;</span>: (loss_identity_A + loss_identity_B),</span><br><span class="line">                        <span class="string">&#x27;loss_G_GAN&#x27;</span>: (loss_GAN_A2B + loss_GAN_B2A),</span><br><span class="line">                        <span class="string">&#x27;loss_G_cycle&#x27;</span>: (loss_cycle_ABA + loss_cycle_BAB), <span class="string">&#x27;loss_D&#x27;</span>: (loss_D_A + loss_D_B)&#125;,</span><br><span class="line">                       images=&#123;<span class="string">&#x27;real_A&#x27;</span>: real_A, <span class="string">&#x27;real_B&#x27;</span>: real_B, <span class="string">&#x27;fake_A&#x27;</span>: fake_A, <span class="string">&#x27;fake_B&#x27;</span>: fake_B&#125;)</span><br></pre></td></tr></table></figure><h1 id="使用注意："><a href="#使用注意：" class="headerlink" title="使用注意："></a>使用注意：</h1><ul><li>画loss曲线时，输入的X和y因为是整数或者tensor，需要将其包裹在[]中</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数值稳定性引出模型初始化</title>
      <link href="/2022/04/16/work/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BC%95%E5%87%BA%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
      <url>/2022/04/16/work/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BC%95%E5%87%BA%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>当数值过大或过小时都会导致数值问题（这里的数值指的是梯度数值）</p><h1 id="梯度爆炸的问题："><a href="#梯度爆炸的问题：" class="headerlink" title="梯度爆炸的问题："></a>梯度爆炸的问题：</h1><ol><li><p>值超出值域</p><ul><li>对于16位浮点数尤为严重</li></ul></li><li><p>对学习率敏感</p><ul><li>如果学习率太大-&gt;大参数值-&gt;更大的梯度</li><li>如果学习率太小-&gt;训练无进展</li><li>导致在训练过程中不断调整学习率</li></ul></li></ol><h1 id="梯度消失的问题："><a href="#梯度消失的问题：" class="headerlink" title="梯度消失的问题："></a>梯度消失的问题：</h1><ol><li><p>梯度值变为0</p><ul><li>对16位浮点数尤为严重</li></ul></li><li><p>训练没有进展</p><ul><li>不管如何选择学习率——因为梯度太小，学习率已经起不到作用了</li></ul></li><li><p>对于低层尤为严重</p><ul><li>仅仅顶部层训练的好——因为梯度是反向传播，从顶层开始。这意味着接近输入数据的层梯度太小，前几层没有起到作用，那么这样再深的神经网络本质上和浅层神经网络差不多</li><li>无法让神经网络更深</li></ul></li></ol><h1 id="如何让训练更稳定"><a href="#如何让训练更稳定" class="headerlink" title="如何让训练更稳定"></a>如何让训练更稳定</h1><ul><li>让梯度值在合理的范围内<ol><li>将乘法变成加法——如ResNet，LSTM就使用了加法（因为加法相对乘法，其数值变化不会太大）</li><li>归一化</li></ol><ul><li>梯度归一化，梯度裁剪</li></ul><ol start="3"><li>合理的权重初始和激活函数</li></ol></li></ul><h1 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h1><p>所谓的权重初始化，目的是希望将权重在初始化时就尽量将其取值在最优解的附近（将其在合理值区间里进行随机初始化），这样网络就能更快收敛</p><p>对于权重可使用Xviar进行初始化（根据每层的尺寸来变化，不是简单的满足均值和方差为(0,0.01)），对于激活函数可使用tanh和relu，但是sigmoid可调整为<code>4*sigmoid - 2</code></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch的Hook机制</title>
      <link href="/2022/04/16/work/Pytorch%E7%9A%84Hook%E6%9C%BA%E5%88%B6/"/>
      <url>/2022/04/16/work/Pytorch%E7%9A%84Hook%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="简单例子说明前向与反向传播过程"><a href="#简单例子说明前向与反向传播过程" class="headerlink" title="简单例子说明前向与反向传播过程"></a>简单例子说明前向与反向传播过程</h1><blockquote><p>例1：创建一个简单的计算图</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">c.backward()</span><br></pre></td></tr></table></figure><ol><li>使用默认值创建一个张量，其requires_grad默认为False；设置为True时，梯度初始化为1</li><li>当使用mul创建得到一个张量c时，同时会创建一个对应c的backward结点图（用于表明梯度会传递到哪些叶子），其由a和b共两个梯度图构成（该梯度图记录对应叶子的梯度值）。<ul><li>计算梯度流向：c-&gt;a,b</li></ul></li></ol><blockquote><p>例2：在1的基础上增加</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">e = c * d</span><br><span class="line">e.backward()</span><br></pre></td></tr></table></figure><ol><li>在1的基础上创建d，计算e：通过mul得到e，创建对应e反向节点图，其由两个方向：d和c</li><li><mark> 由计算得到的数值对应的反向计算图为节点图（其内表明该操作所包含哪些叶子），自定义创建的变量，其对应的反向计算图为梯度图（用于存储反向传播的grad值）</mark></li></ol><blockquote><p>例3：在2的基础上使用HOOK技术</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line">c.register_hook(c_hook)</span><br><span class="line">c.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(grad))</span><br><span class="line">c.retain_grad()</span><br><span class="line"></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">d.register_hook(<span class="keyword">lambda</span> grad: grad + <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line">e.retain_grad()</span><br><span class="line">e.register_hook(<span class="keyword">lambda</span> grad: grad * <span class="number">2</span>)</span><br><span class="line">e.retain_grad() <span class="comment"># 添加这个会因为上面已经存在而无效，但多添加对没有任何影响</span></span><br><span class="line"></span><br><span class="line">e.backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>在中间节点添加hook需要注意添加hook的顺序，在反向传播中会首先执行最先添加的hook。</li><li>在中间节点和叶节点添加hook有所不同<ol><li>在中间节点添加hook，同时其对应的反向计算图节点也会添加一个backword_hooks，这个hook是pre_hooks的（意味着是在该节点反向传播之前先进行hook操作）</li><li>在叶节点添加的hook不会关联到反向计算图的叶节点图中</li><li>在反向传播过程中，中间节点计算图只需要查看自己的hook即可；但叶节点由于并没有关联，需要先查看叶节点是由注册有hook机制并执行。</li></ol></li></ol><h1 id="为什么要使用HOOK？"><a href="#为什么要使用HOOK？" class="headerlink" title="为什么要使用HOOK？"></a>为什么要使用HOOK？</h1><ul><li>由上述可发现，可使创建变量或使用操作得到节点来更改&#x2F;变动前向传播，但是反向传播在整个过程中只是一行代码loss.backward()，这意味着我们只能查看反向传播结束后叶子节点上的梯度，无法查看甚至修改中间梯度，此时就需要使用HOOK机制，这能让我们访问甚至修改反向传播中的任何梯度</li></ul><h1 id="HOOK的使用注意"><a href="#HOOK的使用注意" class="headerlink" title="HOOK的使用注意"></a>HOOK的使用注意</h1><ul><li>不要使用inplace操作，即不要将梯度覆盖，仅仅运算即可；否则这种操作不仅会影响当前分支，还会导致当前分支上所有子节点梯度有所变化。（除非确定其不会对子节点有任何影响）<blockquote><p>例：在上述代码上展示</p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad + <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># inplace代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad += <span class="number">2</span></span><br></pre></td></tr></table></figure><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><h2 id="匿名函数lambda"><a href="#匿名函数lambda" class="headerlink" title="匿名函数lambda"></a>匿名函数lambda</h2><ul><li>匿名函数lambda如何使用？其又是在sort中的key里实现作用的？<blockquote><ol><li>匿名函数实际上是将简单的函数简化的一种写法，只用一行就能实现原本函数三行（首先是def，再是功能，最后是return）才能完成的事。</li><li>sort中的key和dict中的key不是同一个意思，sort中的key表示一种自定义的规则（即在函数中使用什么样的规则实现排序）</li><li>需要注意的是lambda函数可直接在声明之后立刻赋值：<code> list = [lambda x：x**2(i) for i range(0,10)]</code>，在该式中可看到直接将i复制到了lambda中。</li></ol></blockquote></li></ul><h2 id="反向传播中存在的问题"><a href="#反向传播中存在的问题" class="headerlink" title="反向传播中存在的问题"></a>反向传播中存在的问题</h2><h3 id="混淆概念"><a href="#混淆概念" class="headerlink" title="混淆概念"></a>混淆概念</h3><ol><li>反向传播后释放的是计算图的缓冲区，但是梯度并没有释放，故才需要在代码中进行梯度清零。</li><li>反向传播后，只有叶子节点的梯度值保留，中间变量的梯度值默认不保留，除非使用retain_grad()的方法</li></ol><ul><li>反向传播在更新什么，更新的是权重还是输入值？（即在本文的代码中对a，b在backward后有变化么，如果没有变化，在本文中所谓的权重又是什么？）</li></ul><ol><li>反向传播中更新的是权重，并不会改变输入值的大小。</li></ol><h1 id="遗留"><a href="#遗留" class="headerlink" title="遗留"></a>遗留</h1><p>由于HOOK对于当前阶段并不适用，关于<a href="https://www.bilibili.com/video/BV1MV411t7td?spm_id_from=333.999.0.0">视频</a>中后面在pycharm中的讲解那部分没有学习</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-4-12</title>
      <link href="/2022/04/12/life/2022-4-12/"/>
      <url>/2022/04/12/life/2022-4-12/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="01518544ca6add0616e307616dee4d65e988996498208f5bdf3206651fe5d9dd">b3cf69ad95289fed13e724d57178b1a51fb4439f19aa62b82935113c005d7df75cbaaf50c19978e36a2e6b09ccd7ded776f8679f2354c924be14aab5439f6f567334ab6d711219a82bb6e69bb388f12e4866a7232ed819eb22f557edd84a36434584d49fe81597180f03e5000df7ff3206d26463b588773ac7b6b8bf6a176672056493ca080183a6422e208a2d11d8e2b27eb45ae6002b261ac31a91bcc2b17816d0e61e5153da90372b61aabbe0f7286be5fae26d33298554e4522b5daaeeb6595962bb02905c4601d19be8f41b595fd0c54b274cb2adf7254262ebd02d0218f80d702c713ada2fb317c8253efe3fa49a0a3914361e1395bd96b9774e789ed5f14f413228c899a70be205819126a58c2a64d4616024a447ef17ed05134440233e635039df0c5da3ad11acecebae0a4265cafdfa5ae473d98c258f8b7b638435d621c650e16897650bd39e6435075069ff25eafcad69cb7e2600e2b70d7c5c23cfda01d27aed7abe0a3be68a3e91261dbd631be31911bac030b60e57f8b81b72db5cd1a8b47479bff34ce50dc93622567a2c2746dd2672bf7ed672746ca363ac969e6b143c678ac13a24225096370ae403e5cbd0a10baa9d25d4c23c43df0a872c0082314371a00bc434c193988d2f624301f1296c22bf5b057a89874ebd6d22e9f3aa1bf02842a273c0fc97e5cd89c89f6254f9c8c1d82c31e42accb50cb7fd355db346b42b0a17f3b3bdca5fc5dbe34f73cb18e6bb0900ab47f9ae3a95cac944df8896211fbbd917802a3aad511f51caa1f99bbd653e4a12381819fc5a7a5ac76366a754a6e8cc05d7863a50b34e8fa1a402609d39432b5d2ed9333e06c2581c09cac5d47d5019b10382aeb1b891fc3fc2ddeca3320ca28d2b9a4e57c5890418b5a29f689c0484029a5ff0ac9888ffc0e16863dee83b003cdd09b6097e095aa25d26fcc4763021f352a8a94b4c06672cd5b8d13f4c7e3fdb8430cc48fe43094f8f0b0875b854eed3eaccee678f42397c994865ab5bcd63645c39089c0e1f413aa8da34529a181a832c6fb8cf67997ef01e54ce5650086e699bbfb20bdc895c7242b743b351bb46987f4b0eba7325fe8ab50d99b65bc2eb80a2949b1c7e92848bdcf0bb30aa35efce2e635eeb72a5fab2ebb2a319bfadabe2203b638410662fada75b07a9595d22bd3e9c5abe714f87342774a424a9ceaee6b453333e31e23a</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>邪恶</title>
      <link href="/2022/04/12/life/%E9%82%AA%E6%81%B6/"/>
      <url>/2022/04/12/life/%E9%82%AA%E6%81%B6/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="530805f297517d48ec28992eedde9233284db6ba0b6bec2e0eb6bb04052767eb">365f6828a6d9432dd36f78bf1ae632b0d3d14b9bca3110ec97aee6034e8ebaf640c1d3cf2e3457a7480ec63857b578391ef0a9a8a4b194702c107265b0aeee8b3bed3a38137295bb2fa736a76dae1f50a3f3026e34abe9b4a8be79b622e6d40072a8c48ab1d60dcebe716b4dd603421570332435cef19678c5440fa1bf434d222b8a59f8048d4db4f69dcb51d0bc603fcef0e0f2dc04fca44d022c93ad9ef6dc4659d33d56687dc6b4cb38f5770435ac6b63d7fbbf29066ba9b98c0395d96110d361b463d8ec9660de5e13354fb53e17cfcbd6868389302e3d2b277195e39149fcd6a9009a67df6ea4aa362db672dbe80a82bd7780a2d9c6f9debfece114456e5aa045b16c77b75b510f226ea1784da9</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows</title>
      <link href="/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/"/>
      <url>/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/</url>
      
        <content type="html"><![CDATA[<h2 id="文章概述"><a href="#文章概述" class="headerlink" title="文章概述"></a>文章概述</h2><ul><li>图像迁移过程中出现内容泄露，主要原因是decoder不是无损重建，故文中提出projection-reversion结构，该模块核心思想是projection和reversion是对称的，其各小模块共享，reversion在结构上完全是反过来的projection，这样的结构有效解决了内容泄露</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>StyTr2:Image Style Transfer with Transformers</title>
      <link href="/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/"/>
      <url>/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/</url>
      
        <content type="html"><![CDATA[<center> StyTr2:Image Style Transfer with Transformers </center><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>在传统的风格迁移网络中加入transformer模块，让效果更好</li></ul><h3 id="思路概述："><a href="#思路概述：" class="headerlink" title="思路概述："></a>思路概述：</h3><ol><li>加入transformer模块</li><li>引入内容感知位置编码</li></ol><h2 id="文章提到的关键点以及需要注意的文献"><a href="#文章提到的关键点以及需要注意的文献" class="headerlink" title="文章提到的关键点以及需要注意的文献"></a>文章提到的关键点以及需要注意的文献</h2><h3 id="信息点"><a href="#信息点" class="headerlink" title="信息点"></a>信息点</h3><ol><li>风格转换的思路是将内容图像的二姐统计信息与风格图像对齐来进行优化——[6]-[14]</li><li>可以使用transformer提取不同层但提取相似的结构信息——[24]</li><li>identity loss——[15]</li></ol><h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3><ol><li>内容损失：计算某特征层与输入层的L2Loss</li><li>风格损失：一般与某特征层的均值、方差有关</li></ol><h3 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h3><h4 id="内容感知的位置编码"><a href="#内容感知的位置编码" class="headerlink" title="内容感知的位置编码"></a>内容感知的位置编码</h4><blockquote><p>对图片不再使用物理意义上的位置编码，根据内容来自适应编码，代码操作如下</p></blockquote><ol><li>首先对原图进行AdaptiveAvgPool2d自适应平均池化<ul><li>注意直接使用原图的像素数据</li><li>指定固定大小的输出（AdaptiveAvgPool2d(18)是将原图池化层成[b,c,18,18]大小）</li></ul></li><li>进行1*1的一个卷积，不进行尺寸与通道的上的变化，用于提取特征信息</li><li>将该特征信息的尺寸采样到与风格图像相同尺寸<br>最终的使用跟transformer一样，直接与图片的patch_embedding相加</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>文章是为了加入transformer而引入内容感知的位置编码，在解释上是一种知道答案硬扯问题的感觉，编码思路值得借鉴，技术内容没有新意</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Inpainting with External-internal Learning and Monochromic Bottleneck</title>
      <link href="/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/"/>
      <url>/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-Inpainting-with-External-internal-Learning-and-Monochromic-Bottleneck"><a href="#Image-Inpainting-with-External-internal-Learning-and-Monochromic-Bottleneck" class="headerlink" title=" Image Inpainting with External-internal Learning and Monochromic Bottleneck "></a><center> Image Inpainting with External-internal Learning and Monochromic Bottleneck </center></h1><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>解决图像修复时存在的blunt structures和伪影，以他人模型输出的修复图作为输入进行再调整</li></ul><h3 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h3><ol><li>external learning stage：输入单通道重建缺失的结构和细节，即提取结构和细节特征</li><li>internal learning stage：在上述的特征上进行颜色恢复</li></ol><h2 id="文章解决我个人的疑惑以及需要注意的文献"><a href="#文章解决我个人的疑惑以及需要注意的文献" class="headerlink" title="文章解决我个人的疑惑以及需要注意的文献"></a>文章解决我个人的疑惑以及需要注意的文献</h2><ol><li>最常见的伪影是渗色</li></ol><p>【文献】:[边缘]——20，17； [结构信息]——24； [分割图]——29； [模糊图像]——37，38，36</p><h2 id="模型代码思路"><a href="#模型代码思路" class="headerlink" title="模型代码思路"></a>模型代码思路</h2><ol><li>取得其他图像修复模型输出图像（本文以其他模型输出存在的缺陷为出发点），取其灰度图（单通道）</li><li>对灰度图下采样得到三个尺度的图像（仅resize得到）<ol><li>对最小尺度的灰度图进行颜色重构：输入单通道下采样灰度图，注意是全图重构，不是对masked图像重构，loss计算对象：Loss1（颜色重构图 X mask， 原图像 X mask）。<mark> 需要注意的是颜色重构了全图，包括缺陷处的颜色，但是计算loss只计算非缺陷处的loss，这样做避免了对缺陷处直接计算，但是通过非缺陷处的优化来重构缺陷处的颜色重构）</mark></li><li>对中间尺度的灰度图进行颜色重构：输入为cat（第一步output，中间尺寸灰度图）——四通道，输出三通道的颜色重构图，loss计算同1</li><li>对原图尺度的灰度图进行颜色重构：相同于2</li></ol></li></ol><p>【注意】：每个尺寸的颜色重构网络不共享</p><h2 id="文章关键点"><a href="#文章关键点" class="headerlink" title="文章关键点"></a>文章关键点</h2><p>虽然只计算非缺陷处的图像损失，用来优化网络模型，但重构网络中仍然对缺陷处进行颜色重构，即意味着使用非缺陷优化网络间接重构了缺陷处颜色。（也可以理解为学习非缺失区域的颜色映射，直接应用于确实区域进行着色）</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>loss和反向传播的关系</title>
      <link href="/2022/04/08/work/loss%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
      <url>/2022/04/08/work/loss%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：</p><ul><li>文献中总会出现求f的最小值、最大值，疑惑在于模型怎么知道你最大最小的？模型怎么按照最大最小去变化？</li></ul><ol><li>模型参数的调整和loss输出的值没有直接关系，loss输出可理解为期望与实际值之间直观的数值上的可视化，所谓的损失越小越好就是这里的loss数值越小越好。</li><li>模型权重的调整是以label作为基准线，使用loss计算target和label之间的差异，该差异体现在反向传播时的梯度，该梯度作为把控不断调整模型数据分布向label数据分布靠拢，即模型在label附近徘徊。</li><li>模型会将梯度的反方向作为调整方向，此时loss就会变大变小，方向是模型更新时自适应变化的。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[gan]High-Fidelity GAN Inversion for Image Attribute Editing</title>
      <link href="/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/"/>
      <url>/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/</url>
      
        <content type="html"><![CDATA[<h1 id="High-Fidelity-GAN-Inversion-for-Image-Attribute-Editing"><a href="#High-Fidelity-GAN-Inversion-for-Image-Attribute-Editing" class="headerlink" title=" High-Fidelity GAN Inversion for Image Attribute Editing "></a><center> High-Fidelity GAN Inversion for Image Attribute Editing </center></h1><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>让图像能够编辑，生成自己想要的图像风格</li></ul><h3 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h3><ol><li>确定生成模型<br>使用GAN，但生成图像的中间步骤不可控，且生成效果较差。使用基于编码器的GAN，特点在于encoder得到的latent能够进行更多操作（如本文中的编辑，其需要某种特征，而完整的GAN无法生成中间latent）</li><li>编码器中low-latent存在保真度低的问题，重建效果较差；而high-latent可编辑性差，此时考虑在低层进行细节补偿，然后投影到高层，由此进行互补</li><li>对low-latent进行细节保留处理（该细节信息使用重建图像与原图的失真图），将失真图和原图cat后提取low-latent，将其作为系数调整原图重建的low-latent，再将其重建回high-latent，由此得到目标图像</li><li>以上方法可正常应用于在原图上的所有操作，但是中间输入的是编辑图像，不可回避的问题是编辑图像对于原图有所错位，即第二步骤使用的失真图和编辑图像同样存在错位，故设计一个ADA模块先将两者进行对齐，再进行第二步骤</li></ol><h2 id="文章解决我个人的疑惑以及需要注意的文献"><a href="#文章解决我个人的疑惑以及需要注意的文献" class="headerlink" title="文章解决我个人的疑惑以及需要注意的文献"></a>文章解决我个人的疑惑以及需要注意的文献</h2><ol><li>简单的GAN由于是直接重建图像，而基于编码器可生成latent，这个latent可进行更多操作</li><li>在特征提取的过程中，丢失的信息主要是图像特定的细节，倾向于保留公共信息3</li></ol><p>【文献】:1、17、45、26、34、40、5、7、29、32</p><h2 id="关键词理解"><a href="#关键词理解" class="headerlink" title="关键词理解"></a>关键词理解</h2><ul><li>low-rate：GAN inversion时无法实现高保真（因为细节丢失，信息不完整，无法达到高保真的效果），可理解为低层特征</li><li>high-rate：因信息丰富重建效果好，但正是因为信息保留完整导致图片的编辑性变差，可理解为高层特征</li><li>ADA：使用编辑后，所谓的对齐是因为重建图像的latent不只是原图latent，还存在了其他编辑属性（如年龄），而失真图是重建原图和原图之差，此时失真图和编辑图还有编辑属性这个差集，不处理会导致最终重建效果图存在伪影，ADA可以将失真图和编辑属性自定义对齐</li></ul><h2 id="指出问题："><a href="#指出问题：" class="headerlink" title="指出问题："></a>指出问题：</h2><ol><li>low-rate的latent code在重建和编辑图像时难以保证高保真。</li><li>增大latent code大小虽然能提高GAN反演的准确性，但是导致图像可编辑行变差</li></ol><h2 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h2><ol><li>设计一个结构能得到丰富细节的hig-rate的latent code</li><li>设计一个结构去保证从原图生成的图像和编辑图像存在联系</li></ol><h2 id="文中要点"><a href="#文中要点" class="headerlink" title="文中要点"></a>文中要点</h2><p>原文摘要：To enable real-world image editing, GAN inversion techniques [40] have been recently explored, which aim at projecting images to the latent space of a pre-trained GAN generator.</p><ul><li><pre><code>为了实现真实世界的图像编辑，最近探索了 GAN 反演技术 [40]，其旨在将图像投影到预训练的 GAN 生成器的潜在空间。</code></pre><blockquote><p>问题：</p><ol><li>如何实现真实世界的图像编辑？<blockquote><ul><li>使用GAN version技术将其投影到可编辑的空间（latent space）</li></ul></blockquote></li><li>为什么需要latent space？<blockquote><ul><li>在特征图上进行编辑，而不是直接在原图上进行编辑，反映在文章中是选择图像的某一种属性进行编辑（如年龄，该抽象特征无法直接在原图上进行编辑，故需要提取出原图相对于该属性的特征才能编辑）</li></ul></blockquote></li><li>将其投影到什么空间？<blockquote><ul><li>文中选择将图像投影到与预训练G能够生成的latent space相同的空间中，此时再进行编辑。（个人猜测是为了将两者进行空间对齐，方便两个模块的信息之间进行传递）</li></ul></blockquote></li></ol></blockquote></li></ul><h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ol><li>W &#x3D; encoder(X)<blockquote><p>将输入图像经过encoder得到latent向量W</p></blockquote></li><li>X^ &#x3D; decoder(W)<blockquote><p>将latent W经过decoder进行重建</p></blockquote></li><li>res_gt &#x3D; X^ - X</li><li>res_gt &#x3D; transformer(res_gt)</li><li>res_align &#x3D; align(cat(res_gt, X^))<blockquote><p>进行图像对齐</p></blockquote></li><li>delta &#x3D; res_align - res_gt</li><li>F &#x3D; consulation(delta)<blockquote><p>得到相应位置的权重系数</p></blockquote></li><li>img &#x3D; decoder(W,F)<blockquote><p>调整decoder重建时相同通道数的每个像素值的权重</p></blockquote></li></ol><h2 id="Ec模块"><a href="#Ec模块" class="headerlink" title="Ec模块"></a>Ec模块</h2><ol><li>将经过对齐后的delta输入encoder提取特征</li><li>使用自定义的卷积核对特征进行卷积，提取出两层特征<ul><li>关于这两层的使用是在decoder中，相加后直接将其作为权重系数，然后调整decoder重建时相同通道数的每个像素值的权重</li></ul></li></ol><h2 id="ADA模块"><a href="#ADA模块" class="headerlink" title="ADA模块"></a>ADA模块</h2><p>input[b, 6, h, w]</p><blockquote><p>第一部分：conv，其中stride&#x3D;2，故导致图片尺寸下降</p></blockquote><ol><li>conv_layer1：图像尺寸不变<ul><li>Conv + BatchNorm + PReLU</li></ul></li><li>conv_layer2：图像尺寸减半<ol><li><code>[b, 6, h, w] -&gt; [b, 16, h/2, w/2]：</code>Conv + BatchNorm</li><li><code>[b, 6, h, w] -&gt; [b, 16, h/2, w/2]：</code>BatchNorm + Conv + Conv</li><li><code>[b, 6, h/2, w/2] -&gt; [b, 16, h/2, w/2]：</code> 1 + 2</li></ol></li><li>conv_layer3：图像尺寸减半</li><li>conv_layer4：图像尺寸减半</li></ol><blockquote><p>第二部分：dconv，主要分为两部分</p><ol><li>图像插值上采样</li><li>和conv部分完全相同的卷积层，只不过stride&#x3D;1，不对图片进行下采样</li></ol></blockquote><ol><li>dconv1<ol><li>插值上采样</li><li>上次输出和采样值通道cat输入到_block</li></ol></li><li>dconv2<ol><li>插值上采样</li><li>上次输出和采样值通道cat输入到_block</li></ol></li><li>dconv3<ol><li>上次输出和采样值通道cat输入到_block<br>【输出与原图大小相同的对其重建图像】</li></ol></li></ol><h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><blockquote><p>问题描述：报错很明显为数据类型错误，将float-&gt;long即可，但是无论是将类型变成float64或者long仍然报同样的错误</p><ul><li>经过尝试后发现：将将类型直接转换成float即可</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">600</span>,<span class="number">1</span>,dtype=torch.float64).view(<span class="number">1</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错如下</span></span><br><span class="line">expected scalar <span class="built_in">type</span> Long but found Float</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改如下</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">600</span>,<span class="number">1</span>,dtype=torch.<span class="built_in">float</span>).view(<span class="number">1</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perceptual Loss</title>
      <link href="/2022/04/06/work/Perceptual%20Loss/"/>
      <url>/2022/04/06/work/Perceptual%20Loss/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>生成任务中损失函数分为分为两个阶段：</p><ol><li>计算生成图像与输入图像之间的损失，被称为<mark> Per-Pixel Loss </mark>，经研究发现，如果将原图向任意方向偏移一个像素，这样做实际上本身分辨率和风格均未发生太大变换，但是Per-Pixel Loss却会因这一个像素的偏移出现显著上升，可推论Per-Pixel Loss并未反映&#x2F;约束图像高级特征信息</li><li>基于Per-Pixel Loss的缺陷，提出将约束角度从出入与输出转向约束feature，即Pixel-&gt;feature，故生成了<mark> Perceptual Loss </mark>（意为能感知到高层语义特征）</li></ol><p><a href="https://blog.csdn.net/WhaleAndAnt/article/details/107116360?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164920726816782248548021%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164920726816782248548021&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-107116360.142%5Ev5%5Epc_search_result_control_group,157%5Ev4%5Econtrol&utm_term=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%85%AC%E5%BC%8F&spm=1018.2226.3001.4187">参考博文</a></p><p><a href="https://blog.csdn.net/studyeboy/article/details/118724526?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164920726816782248586959%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164920726816782248586959&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-6-118724526.142%5Ev5%5Epc_search_result_control_group,157%5Ev4%5Econtrol&utm_term=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%85%AC%E5%BC%8F&spm=1018.2226.3001.4187">参考博文</a><br>关于感知损失的使用方法参考文献：<a href="https://arxiv.org/abs/2103.10571">Generic Perceptual Loss for Modeling Structured Output Deppendencies</a></p><h1 id="2-Loss"><a href="#2-Loss" class="headerlink" title="2. Loss"></a>2. Loss</h1><blockquote><p>感知损失目的是约束高层语义信息，其形式大体可总结为两个部分</p></blockquote><ol><li>feature损失<ol><li>高层feature内容损失</li><li>高层feature风格损失</li></ol></li><li>Per-Pixel损失</li></ol><p>$$\text{Perceptual Loss} &#x3D; \text{Loss_feature} + \text{Loss_style} + \text{Loss_Per-Pixel}$$</p><p>为了让输出图像的风格和内容均保持输入的原图与风格图像的特征信息，只约束输入与输出会导致中间层收敛方向不明确（个人猜测），故在中间特征层也使用感知损失来约束，从而确定在整个过程中整个模型均向同一个方向优化</p><ol><li>内容损失<code>Loss_feature</code>作用在较低特征层上即可——保留一些纹理细节</li><li>风格损失<code>Loss_style</code>可以作用在从低到高所有特征层上——保留一些语义上的信息</li></ol><h1 id="3-常见的生成模型损失函数"><a href="#3-常见的生成模型损失函数" class="headerlink" title="3. 常见的生成模型损失函数"></a>3. 常见的生成模型损失函数</h1><p>所谓的感知损失并没有固定的公式，只要是在特征级上的损失即可视为感知损失，下方的损失函数可自行组合</p><h2 id="3-1-Feature-Reconstruction-Loss"><a href="#3-1-Feature-Reconstruction-Loss" class="headerlink" title="3.1 Feature Reconstruction Loss"></a>3.1 Feature Reconstruction Loss</h2><p>$$l_{\text {feat }}^{\phi, j}(\hat{y}, y)&#x3D;\frac{1}{C_{j} H_{j} W_{j}}\left|\phi_{j}(\hat{y})-\phi_{j}(y)\right|_{2}^{2}$$</p><p>计算第j层的特征重建损失，CHW是第j层feature_map的size</p><h2 id="3-2-Style-Reconstruction-Loss"><a href="#3-2-Style-Reconstruction-Loss" class="headerlink" title="3.2 Style Reconstruction Loss"></a>3.2 Style Reconstruction Loss</h2><p>对于风格重建的损失函数，首先要先计算 Gram 矩阵:</p><p>$$G_{j}^{\phi}(x)<em>{c, c^{\prime}}&#x3D;\frac{1}{C</em>{j} H_{j} W_{j}} \sum_{h&#x3D;1}^{H_{j}} \sum_{w&#x3D;1}^{W_{j}} \phi_{j}(x)<em>{h, w, c} \phi</em>{j}(x)_{h, w, c^{\prime}}$$</p><p>产生的 feature_map 的大小为 CjHjWjCjHjWj，可以看成是 CjCj 个特征，这些特征两两之间的内积的计算方式如上。</p><p>$$l_{\text {style }}^{\phi, j}(\hat{y}, y)&#x3D;\left|G_{j}^{\phi}(\hat{y})-G_{j}^{\phi}(y)\right|_{F}^{2}$$</p><p>两张图片，在 loss 网络的每一层都求出 Gram 矩阵，然后对应层之间计算欧式距离，最后将不同层的欧氏距离相加，得到最后的风格损失。</p><h2 id="3-3-Simple-Loss-Function"><a href="#3-3-Simple-Loss-Function" class="headerlink" title="3.3 Simple Loss Function"></a>3.3 Simple Loss Function</h2><h3 id="3-3-1-Pixel-Loss"><a href="#3-3-1-Pixel-Loss" class="headerlink" title="3.3.1 Pixel Loss"></a>3.3.1 Pixel Loss</h3><p>pixel loss 是输出 y^ 和目标 y 之间的欧几里得距离</p><p>$$l_{\text {pixel }}(\hat{y}, y)&#x3D;|\hat{y}-y|_{2}^{2}$$</p><h3 id="3-3-1-Total-Variation-Regularization"><a href="#3-3-1-Total-Variation-Regularization" class="headerlink" title="3.3.1 Total Variation Regularization"></a>3.3.1 Total Variation Regularization</h3><p>Total Variation Loss，实际上是一个平滑项（一个正则化项），目的是使生成的图像在局部上尽可能平滑，而它的定义和马尔科夫随机场（MRF）中使用的平滑项非常相似。</p><p>$$ l_{T V}(\hat{y})&#x3D;\sum_{n}\left|\hat{y}<em>{n+1}-\hat{y}</em>{n}\right|_{2}^{2} $$</p><p>其中 yn+1 和 yn+1 是相邻像素</p><h2 id="3-4-计算判别器之间的损失"><a href="#3-4-计算判别器之间的损失" class="headerlink" title="3.4 计算判别器之间的损失"></a>3.4 计算判别器之间的损失</h2><p>$$ l_{\text {feat }}^{D, l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)&#x3D;\frac{1}{2 C_{l} H_{l} W_{l}}\left|D_{l}(\mathbf{x})-D_{l}\left(\mathbf{x}^{\prime}\right)\right|_{F}^{2} $$</p><p>其中x , x’，x分别表示源和生成图像，D 表示判别器网络，l 表示判别器的第l 层，CHW为特征图size</p><ul><li><input checked="" disabled="" type="checkbox"> 注意这里计算判别器之间的损失：其作用是什么，放在融合网络中可以进行特征补偿么，查看其原文论文为什么这么做</li></ul><ol><li>Li M, Zuo W, Zhang D, et al. Deep Identity-aware Transfer of Facial Attributes.[J]. arXiv: Computer Vision and Pattern Recognition, 2016</li><li>Wang C, Xu C, Wang C, et al. Perceptual Adversarial Networks for Image-to-Image Transformation[J]. IEEE Transactions on Image Processing, 2018, 27(8): 4066-4079</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>匹配低层特征可以保留几何、纹理等低级语义信息，匹配深层特征可以保留内容、风格等高级语义信息。</li><li>预训练网络提取的特征附带预训练时的任务属性，会对生成模型产生影响，所以预训练模型尽量选择与生成任务相关的模型参数。</li><li>选择特征匹配层不必局限于激活层，选择激活层之前的特征进行匹配，可以为生成模型提供更强的监督信息。</li><li>对于具有Ground Truth的任务，深度特征匹配可以设置多层且较强的约束；对于没有Ground Truth仅使用感知损失做重构的任务，深度特征匹配应设置单层且较弱的约束。</li></ul><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ol><li>使用项目两个判别器中的feature计算loss有什么作用，请查看原文献<ul><li>已经解决：因为原项目分类器预训练使用的其他数据集，之后固定了参数，故转向判别器进行约束，其中判别器使用的是自己的数据集</li></ul></li><li>在这些损失函数中使用L1loss和L2loss有什么区别</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>抱歉</title>
      <link href="/2022/04/03/life/%E6%8A%B1%E6%AD%89/"/>
      <url>/2022/04/03/life/%E6%8A%B1%E6%AD%89/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="d97aff4b256e06de5f6b745002518349373595d52c7adb064578799d8eaaac62">7ab9d85c8eda3888a25d4b7fa9c695bd44f6ad7c704f97ef3b670a2230628f08</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>将</title>
      <link href="/2022/04/03/life/%E5%B0%86/"/>
      <url>/2022/04/03/life/%E5%B0%86/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="0b4927115b1be92104c2d0bc157641c0936b0d61c7baf31e7d81bea4cd6e6a31">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Idea</title>
      <link href="/2022/04/02/Idea/"/>
      <url>/2022/04/02/Idea/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="73fc77f843c6aeacab1f0cdc3579a71bddd0bb62b040e0e7bbc7e8b2a1826e77">b37e1c5f21f5af8a2d0feaf56fcb9ee1b85e5698fc43d4497edf61ce25fb459a8ab63d148a137dd47c5985e2afd45fa6fd605ef6c7198f5da845bc1e151e1e9d4207ad8b96f7a00bcf6385a3aa611fefbe63c640a45fc904f5f50aaa90eb46a89c4355d4b4037c51ccc3b1374d987c5c1d01d739bab095b55ae7964a58c1b3d419ef6cc6156e2434121cbba8b66c88849d328bba7deff6d2ffe0cacf756a0fd1d7f1c8a7061dd7c1dbafad0e31487fdf55493e72feb3cfa0a1fe3eb6bb9fc83956a05b5efb0428e938410c950a3db6a37dedc557dc10ca73b354de0b4c935527aeedba1d14d00f1658ab182ef2054149e09bada79d61d6127d44a6752ae7457932b3ad609052a0733b59b2a7518d44296912fba943d9abfc500259c21c7f176c1faf9c55332e699767c31581700bac6700f56ecaac83e6e89b8d92ef2a25abfa6fa6dab599208ba9e716a6995f233912e08aaff891f68aeedb1df9800b423c00e2ec753ac2d8c666ca8b018dbe0651e686b3aba08fda3ac2fe61e1f28e6fa1f53c858f45f7d6945d97b9d1698dc48434e048d0086162eb05e953c9d16cfcc38e22eb7187a904abe2d73df0164568b16869610cd2fa1f31522ae1cb7de957a4314d1050ae024132951a0f2af363ff2f5441986fcb99ab3ffe4044e9b81ead3384822cecd37aab4f515853414847af006d0adad4e21456a8d5b3b05f712207cb2a08c4a93bb4444bbf29e6fdcc1b175324e565a9eef07c1787ca39627c35702de5b26ee9fe9360e80b3b7f9820c5d22ef4ff59775514e0b6cc701138c3a66dd28d32d46091afd7b0fe02156bee9b8e779d3c7ff29da187c84bd1e9a75f4add4249a52cc4f6aadeae8dbcb62defe7ebf32b</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客中渲染数学公式</title>
      <link href="/2022/04/01/blog/hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2022/04/01/blog/hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="一、更换渲染引擎"><a href="#一、更换渲染引擎" class="headerlink" title="一、更换渲染引擎"></a>一、更换渲染引擎</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p><mark>注意此处有坑，更换渲染引擎之后会导致hexo某些markdown语法无法正常显示，以及hexo博客中图片无法显示</mark></p><ul><li>由于无法正常显示，我重新将卸载的引擎重新安装后异常消失了，更换的引擎也就没有卸载了</li></ul><h1 id="二、解决语义冲突"><a href="#二、解决语义冲突" class="headerlink" title="二、解决语义冲突"></a>二、解决语义冲突</h1><ol><li>进入博客根目录：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node_modules\kramed\lib\rules\inline.js</span><br></pre></td></tr></table></figure><ol><li>修改该文件中两处代码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一处位于11行</span></span><br><span class="line">//  escape: /^\\([\\`*&#123;&#125;\[\]()<span class="comment">#$+\-.!_&gt;])/,</span></span><br><span class="line">  escape: /^\\([`*\[\]()<span class="comment">#$+\-.!_&gt;])/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二处位于20行</span></span><br><span class="line">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><br></pre></td></tr></table></figure><ol start="3"><li>开启主题配置文件中mathjax引擎渲染</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意其中per_page可以不开启，但是enable必须开启使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MathJax</span></span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: false</span><br></pre></td></tr></table></figure><ol start="4"><li>没有使用per_page时则在每篇文章开头写</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>三、公式书写注意</p><ol><li>得到的LaTeX公式需要加上<mark>单$</mark>才能成功渲染</li><li>对LaTeX公式加<mark>双$包裹</mark>可将公式居中显示</li><li>注意公式上下的回车空行</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA安装后出现的两种报错</title>
      <link href="/2022/04/01/work/CUDA%E5%AE%89%E8%A3%85%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%8A%A5%E9%94%99/"/>
      <url>/2022/04/01/work/CUDA%E5%AE%89%E8%A3%85%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<blockquote><p>安装CUDA之后，运行代码，会出现两种报错<br>1.cuda安装的版本过高，NVIDIA显卡驱动无法驱动<br>2.cuda安装的是pytorch，其版本和cuda不太适配</p></blockquote><hr><h1 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The NVIDIA driver on your system <span class="keyword">is</span> too old (found version <span class="number">10010</span>).</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 解决：安装版本较低的cuda——查看GPU最高版本要求</li></ul><h1 id="问题二："><a href="#问题二：" class="headerlink" title="问题二："></a>问题二：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Torch <span class="keyword">not</span> compiled <span class="keyword">with</span> CUDA enabled</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 解决：不安装官网给出的pytorch-cudatoolkit那种conda安装命令，使用torch-cu那种pip安装命令</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSIM</title>
      <link href="/2022/03/31/work/SSIM/"/>
      <url>/2022/03/31/work/SSIM/</url>
      
        <content type="html"><![CDATA[<blockquote><p>结构相似性指数SSIM(论文：Image Quality Assessment: From Error Visibility to Structural Similarity)用于度量两幅图像间的结构相似性，结构被定义为三个关键特征：</p><ol><li>亮度</li><li>对比度</li><li>结构</li></ol><p>其中SSIM值越大，图像越相似，两幅图片完全相同时，SSIM&#x3D;1，<mark>故作为损失函数时，应该取负号（即越相似loss越小）</mark>，如：loss &#x3D; 1-SSIM</p></blockquote><p>实际loss使用：</p><p>$$\text{loss}(x, y) &#x3D; \frac{1 - \text{SSIM}(x, y)}{2}$$</p><h1 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h1><p>这个系统计算2幅给定图像之间的结构相似度指数，取值范围[0,1]，其中极端值具有相同的含义。用均值作为亮度的估计，标准差作为对比度的估计，协方差作为结构相似程度的估计</p><h2 id="1-1-亮度"><a href="#1-1-亮度" class="headerlink" title="1.1 亮度"></a>1.1 亮度</h2><p>将图像的平均灰度作为测量估计。用μ表示为：</p><p>$$\mu_{x}&#x3D;\frac{1}{N} \sum_{i&#x3D;1}^{N} x_{i}$$</p><p>对应到图像中：</p><p>$$\mu_{X}&#x3D;\frac{1}{H \times M} \sum_{i&#x3D;1}^{H} \sum_{j&#x3D;1}^{M} X(i, j)$$</p><h2 id="1-2-对比度"><a href="#1-2-对比度" class="headerlink" title="1.2 对比度"></a>1.2 对比度</h2><p>将图像的标准差(方差的平方根)作为测量估计。用σ表示为：</p><p>$$\sigma_{x}&#x3D;\left(\frac{1}{N-1} \sum_{i&#x3D;1}^{N}\left(x_{i}-\mu_{x}\right)^{2}\right)^{\frac{1}{2}}$$</p><p>对应到图像中：</p><p>$$\sigma_{X}&#x3D;\left(\frac{1}{H+W-1} \sum_{i&#x3D;1}^{H} \sum_{j&#x3D;1}^{M}\left(X(i, j)-\mu_{X}\right)^{2}\right)^{\frac{1}{2}}$$</p><h2 id="1-3-结构"><a href="#1-3-结构" class="headerlink" title="1.3 结构"></a>1.3 结构</h2><p>结构比较是通过使用一个合并公式来完成的(后面会详细介绍)，但在本质上，我们用输入信号的标准差来除以它，因此结果有单位标准差，这可以得到一个更稳健的比较。</p><p>$$\left(\mathbf{x}-\mu_{x}\right) &#x2F; \sigma_{x}$$</p><p>其中x是输入图像。</p><h1 id="2-比较函数"><a href="#2-比较函数" class="headerlink" title="2. 比较函数"></a>2. 比较函数</h1><ol><li>亮度比较函数：由函数定义，l(x, y)，如下图所示。μ表示给定图像的平均值。x和y是被比较的两个图像。</li></ol><p>$$l(\mathbf{x}, \mathbf{y})&#x3D;\frac{2 \mu_{x} \mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$$</p><p>其中C1为常数，保证分母为0时的稳定性。C1这样给出：</p><p>$$C_{1}&#x3D;\left(K_{1} L\right)^{2}$$</p><p>其中k1&#x3D;0.01,L代表输入数据的最大值（没有归一化处理的图像取值255，归一化取值1）</p><ol start="2"><li>对比度函数：由函数c(x, y)定义，如下图所示。σ表示给定图像的标准差。x和y是被比较的两个图像。</li></ol><p>$$c(\mathbf{x}, \mathbf{y})&#x3D;\frac{2 \sigma_{x} \sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$$</p><p>其中C2这样给出：</p><p>$$C_{2}&#x3D;\left(K_{2} L\right)^{2}$$</p><p>其中k2&#x3D;0.03</p><p>3.结构比较函数：由函数s(x, y)定义，如下图所示。σ表示给定图像的标准差。x和y是被比较的两个图像。</p><p>$$s(\mathbf{x}, \mathbf{y})&#x3D;\frac{\sigma_{x y}+C_{3}}{\sigma_{x} \sigma_{y}+C_{3}}$$</p><p>其中c3&#x3D;c2&#x2F;2</p><h1 id="3-SSIM测量函数"><a href="#3-SSIM测量函数" class="headerlink" title="3. SSIM测量函数"></a>3. SSIM测量函数</h1><h2 id="3-1-SSIM的定义式"><a href="#3-1-SSIM的定义式" class="headerlink" title="3.1 SSIM的定义式"></a>3.1 SSIM的定义式</h2><p>$$\begin{aligned}<br>\operatorname{SSIM}(x, y) &amp;&#x3D;f(l(x, y), c(x, y), s(x, y)) \<br>&amp;&#x3D;[l(x, y)]^{\alpha}[c(x, y)]^{\beta}[s(x, y)]^{\gamma}<br>\end{aligned}$$</p><p>其中， α、β、γ &gt; 0，用来调整这三个模块的重要性<br>假设α、β、γ都为1，C3 &#x3D; C2 &#x2F; 2 ,则</p><p>$$S S I M(x, y)&#x3D;\frac{\left(2 \mu_{x} \mu_{y}+C_{1}\right)\left(2 \sigma_{x y}+C_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+C_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}\right)}$$</p><p>SSIM函数的值域为[0, 1], 值越大说明图像失真越小，两幅图像越相似。</p><h2 id="3-2-SSIM函数满足的三个条件"><a href="#3-2-SSIM函数满足的三个条件" class="headerlink" title="3.2 SSIM函数满足的三个条件"></a>3.2 SSIM函数满足的三个条件</h2><ol><li>对称性：S(x,y)&#x3D;S(y,x)</li><li>有界性：S(x,y)≤1</li><li>最大值唯一性：当且仅当x&#x3D;y时，S(x,y)&#x3D;1</li></ol><h1 id="4-实际应用"><a href="#4-实际应用" class="headerlink" title="4. 实际应用"></a>4. 实际应用</h1><p>使用在全局范围会导致局部信息受损，故一般使用一个固定大小的滑窗，计算局部SSIM，故此，公式进行局部约束：</p><p>$$\begin{aligned}<br>\mu_{x} &amp;&#x3D;\sum_{i&#x3D;1}^{N} w_{i} x_{i} \<br>\sigma_{x} &amp;&#x3D;\left(\sum_{i&#x3D;1}^{N} w_{i}\left(x_{i}-\mu_{x}\right)^{2}\right)^{\frac{1}{2}} \<br>\sigma_{x y} &amp;&#x3D;\sum_{i&#x3D;1}^{N} w_{i}\left(x_{i}-\mu_{x}\right)\left(y_{i}-\mu_{y}\right)<br>\end{aligned}$$</p><p>其中wi是高斯加权函数。</p><h1 id="5-遗留"><a href="#5-遗留" class="headerlink" title="5. 遗留"></a>5. 遗留</h1><ol><li>均值、标准差、归一化为什么可以分别作为亮度、对比度、结构特征的计算，这三种数据计算方法的几何意义、物理意义是什么？</li><li>SSIM论文中应该有相关信息没有注意到</li><li>为什么2ab&#x2F;(a+b)的形式能够作为对比？</li><li>损失函数后面调整为负值，进行加减等操作会对反向传播造成什么影响？</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCGAN训练思考</title>
      <link href="/2022/03/30/work/DCGAN%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83/"/>
      <url>/2022/03/30/work/DCGAN%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<h1 id="工作说明"><a href="#工作说明" class="headerlink" title="工作说明"></a>工作说明</h1><blockquote><p>用了两天的时间使用MNIST数据集训练DCGAN</p><ol><li>第一天怎么训练，loss D都在两个batch_size内快速下降到0.0x，之后直接趋向于0</li><li>第二天训练的各项数据正常</li></ol></blockquote><h1 id="问题回顾"><a href="#问题回顾" class="headerlink" title="问题回顾"></a>问题回顾</h1><ol><li><p>第一天的训练：</p><ol><li>错误的认为loss D应该保持在0.5左右——保持在0.5左右的是D(x)和D(G(Z))，而不是loss函数</li><li>将real_label设置为0，fake_label设置为1——对训练有一定的影响，必要时可以对调，更好的方法是使用软标签</li></ol></li><li><p>第二天的训练：</p><ol><li>添加D(x)和D(G(Z))可视化曲线</li><li>将标签对调为正常情况</li></ol></li></ol><h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><ul><li>INFO：数据</li></ul><center> D、G[1:1]更新 </center><pre><code class="python">Epoch [0/5] Batch 0/469                 D(x):0.5047，D(G(z)): 0.5004，output: 0.4535，Loss D: 0.6889,loss G: 0.7908Epoch [0/5] Batch 100/469               D(x): 0.9880，D(G(z)): 0.0171，output: 0.0165，Loss D:0.0147， loss G: 4.1140Epoch [e/5] Batch 200/469               D(x):0.8712，D(G(z)): 0.8090，output: 0.6145，Loss D: 0.9081，loss G: 0.4901Epoch [0/5] Batch 300/469               D(x):0.6222，D(G(z)): 0.3736，output: 0.3109，Loss D: 0.4783，loss G: 1.1826Epoch [0/5] Batch 400/469               D(x): 0.6995，D(G(z)): 0.4954，output: 0.1875，Loss D: 0.5354,loss G: 1.7224Epoch [1/5] Batch 0/469                 D(x):0.5371，D(G(z)):0.3516，output: 0.4096，Loss D: 0.5372，loss G: 0.9151Epoch [1/5] Batch 100/469               D(x): 0.5416，D(G(z)):0.4679，output: 0.3907，Loss D: 0.6339,loss G: 0.9563Epoch [1/5] Batch 200/469               D(x):0.5662，D(G(z)):0.4320，output: 0.4054,Loss D: 0.5761， loss G: 0.9195Epoch [1/5] Batch 300/469               D(x): 0.5569，D(G(z)): 0.4211，output:0.3418,Loss D: 0.5769, loss G: 1.0925Epoch [1/5] Batch 400/469               D(x):0.4531，D(G(z)): 0.2992，output: 0.5755，Loss D: 0.5908， loss G: 0.5740Epoch [2/5] Batch 0/469                 D(x): 0.5905，D(G(z)): 0.4527，output: 0.3409，Loss D: 0.5796，loss G: 1.1067Epoch [2/5] Batch 100/469               D(x):0.6021，D(G(z)):0.4192，output: 0.3489,Loss D: 0.5394，loss G: 1.0880Epoch [2/5] Batch 200/469               D(x): 0.5366，D(G(z)): 0.4305，output: 0.4065,Loss D: 0.6069，loss G: 0.9206Epoch [2/5] Batch 300/469               D(x):0.5416，D(G(z)):0.2691，output: 0.4179，Loss D: 0.4877， loss G: 0.9194Epoch [2/5] Batch 400/469               D(x):0.7101，D(G(z)):0.4133，output: 0.1640，Loss D:0.4520，loss G: 1.8622Epoch [3/5] Batch 0/469                 D(x):0.7748，D(G(z)): 0.3042，output: 0.0808，Loss D: 0.3200，loss G: 2.6203Epoch [3/5] Batch 100/469               D(x): 0.6514,D(G(z)):0.1118，output: 0.1721，Loss D: 0.2863，loss G: 1.9265Epoch [3/5] Batch 200/469               D(x):0.8176，D(G(z)):0.3861，output: 0.0538，Loss D: 0.3626,loss G: 3.1119Epoch [3/5] Batch 300/469               D(x): 0.7629，D(G(z)):0.3083，output: 0.0762，Loss D: 0.3354，loss G: 2.7599Epoch [3/5] Batch 400/469               D(x): 0.8042，D(G(z)):0.2689，output: 0.1024，Loss D: 0.2779,loss G: 2.4286Epoch [4/5] Batch 0/469                 D(x):0.8273，D(G(z)): 0.3855，output: 0.0283，Loss D: 0.3615，loss G: 3.8461Epoch [4/5] Batch 100/469               D(x):0.8260，D(G(z)):0.1448,output: 0.1007,Loss D: 0.1850， loss G: 2.5335Epoch [4/5] Batch 200/469               D(x):0.9277，D(G(z)):0.4498,output: 0.0191，Loss D: 0.3639，loss G: 4.2212Epoch [4/5] Batch 300/469               D(x): 0.5809，D(G(z)):0.0402，output: 0.4912，Loss D: 0.3201，loss G: 0.8003Epoch [4/5] Batch 400/469               D(x): 0.9238，D(G(z)):0.5808,output: 0.1350,Loss D: 0.5133, loss G: 2.1491</code></pre><ol><li>从图中可以发现，第一、第二个batch_size明显虚高，是因为GAN的训练特性——在G尚未学习的情况下首先更新D，由此导致前几个batch_size中D的分数较高，不过个人猜测是正常情况</li><li>随着训练epoch增加，D(x)的性能平均保持在0.8左右，而G的分数一直保持在0.3左右，个人猜测是G的学习能力较低——应降低D的学习频率，增加G的学习频率<ul><li>经过测试后发现，减少D的训练频率会导致G偷懒，从噪声生成的所有图片都是一个样子来欺骗D,其中设置G的训练频率是D的[4,14,32]倍——从4倍起就开始偷懒，<mark>故调整训练频率保持在[2,3]倍左右，或者不调整训练频率，改为调整学习率。</mark></li></ul></li></ol><center> D、G[1:2]更新 </center><pre><code class="python">Epoch [0/5] Batch 0/469                     D(x): 0.4946, D(G(z)): 0.4892, output: 0.4453, Loss D: 0.6879, loss G: 0.8089Epoch [0/5] Batch 100/469                   D(x): 0.9524, D(G(z)): 0.0601, output: 0.0536, Loss D: 0.0558, loss G: 2.9602Epoch [0/5] Batch 200/469                   D(x): 0.9416, D(G(z)): 0.1034, output: 0.0751, Loss D: 0.0856, loss G: 2.6562Epoch [0/5] Batch 300/469                   D(x): 0.4917, D(G(z)): 0.4844, output: 0.4827, Loss D: 0.6864, loss G: 0.7284Epoch [0/5] Batch 400/469                   D(x): 0.4993, D(G(z)): 0.4931, output: 0.4904, Loss D: 0.6872, loss G: 0.7127Epoch [1/5] Batch 0/469                     D(x): 0.4806, D(G(z)): 0.3470, output: 0.7397, Loss D: 0.5817, loss G: 0.3027Epoch [1/5] Batch 100/469                   D(x): 0.4667, D(G(z)): 0.4295, output: 0.4518, Loss D: 0.6636, loss G: 0.7955Epoch [1/5] Batch 200/469                   D(x): 0.4734, D(G(z)): 0.4435, output: 0.4776, Loss D: 0.6698, loss G: 0.7406Epoch [1/5] Batch 300/469                   D(x): 0.5026, D(G(z)): 0.4947, output: 0.4927, Loss D: 0.6868, loss G: 0.7092Epoch [1/5] Batch 400/469                   D(x): 0.5637, D(G(z)): 0.5503, output: 0.4108, Loss D: 0.6870, loss G: 0.8902Epoch [2/5] Batch 0/469                     D(x): 0.5009, D(G(z)): 0.4761, output: 0.5022, Loss D: 0.6697, loss G: 0.6893Epoch [2/5] Batch 100/469                   D(x): 0.5442, D(G(z)): 0.5315, output: 0.4591, Loss D: 0.6839, loss G: 0.7789Epoch [2/5] Batch 200/469                   D(x): 0.5498, D(G(z)): 0.5392, output: 0.4371, Loss D: 0.6871, loss G: 0.8279Epoch [2/5] Batch 300/469                   D(x): 0.4320, D(G(z)): 0.4212, output: 0.5299, Loss D: 0.6934, loss G: 0.6352Epoch [2/5] Batch 400/469                   D(x): 0.3814, D(G(z)): 0.3730, output: 0.5444, Loss D: 0.7159, loss G: 0.6087Epoch [3/5] Batch 0/469                     D(x): 0.5367, D(G(z)): 0.5260, output: 0.4762, Loss D: 0.6847, loss G: 0.7421Epoch [3/5] Batch 100/469                   D(x): 0.5263, D(G(z)): 0.5135, output: 0.5199, Loss D: 0.6817, loss G: 0.6546Epoch [3/5] Batch 200/469                   D(x): 0.3974, D(G(z)): 0.3897, output: 0.5477, Loss D: 0.7087, loss G: 0.6024Epoch [3/5] Batch 300/469                   D(x): 0.5390, D(G(z)): 0.5214, output: 0.4688, Loss D: 0.6780, loss G: 0.7582Epoch [3/5] Batch 400/469                   D(x): 0.5181, D(G(z)): 0.5077, output: 0.4879, Loss D: 0.6836, loss G: 0.7180Epoch [4/5] Batch 0/469                     D(x): 0.4994, D(G(z)): 0.4818, output: 0.5079, Loss D: 0.6764, loss G: 0.6781Epoch [4/5] Batch 100/469                   D(x): 0.5411, D(G(z)): 0.5297, output: 0.4642, Loss D: 0.6854, loss G: 0.7683Epoch [4/5] Batch 200/469                   D(x): 0.5262, D(G(z)): 0.5221, output: 0.4577, Loss D: 0.6912, loss G: 0.7824Epoch [4/5] Batch 300/469                   D(x): 0.4609, D(G(z)): 0.4493, output: 0.5305, Loss D: 0.6862, loss G: 0.6345Epoch [4/5] Batch 400/469                   D(x): 0.5205, D(G(z)): 0.5072, output: 0.4688, Loss D: 0.6810, loss G: 0.7585   </code></pre><ul><li>观察发现数值按照预期一样，但是图像可视化中显示生成人眼可辨识的数字在epoch慢于[1:1]速率——故在此不推荐更改训练速率，更改学习率即可</li></ul><h1 id="DCGAN生成手写数字集项目总结"><a href="#DCGAN生成手写数字集项目总结" class="headerlink" title="DCGAN生成手写数字集项目总结"></a>DCGAN生成手写数字集项目总结</h1><ul><li><input checked="" disabled="" type="checkbox"> 拟解决问题</li></ul><blockquote><ol><li>本项目中查看了torchvision.utils.transformers.Normalize函数，在GAN中需要将输入图片经过ToTensor转化到[0,1]之间，在经过Normalize图像矩阵中的数据从[0,1]转化到[-1,1]之间，同时将生成器最后一层生成的数据经过tanh函数映射到[-1,1]之间</li><li>为什么需要将数据归一化，且为什么归一化到[0,1]或[-1,1]之间？<ol><li>加快梯度下降求最优解的速度将数据约束在已知范围内——统一输入数据分布<mark>（深度学习的本质就是在学习数据分布）</mark>，否则网络需要适应不同分布的数据，降低网络训练速度</li><li>有可能提高精度</li></ol></li></ol></blockquote><ul><li><input disabled="" type="checkbox"> 拟生成问题</li></ul><blockquote><p>模型如何通过loss的约束来调整模型，如何确定约束时最大还是最小</p></blockquote><ul><li><input disabled="" type="checkbox"> 项目训练总结</li></ul><blockquote><ol><li>训练直接崩塌可对调label</li><li>可视化数据中D(x)在后期保持高分，D(G)一直保持低分，这种情况是G的学习能力差，调整学习率即可</li><li>中途怀疑batch_size对训练有影响，但是尝试[32,64,128]，经过不仔细观察影响效果不大</li></ol></blockquote><h1 id="遗漏"><a href="#遗漏" class="headerlink" title="遗漏"></a>遗漏</h1><ul><li>本次模型调试中没有可视化曲线图、直方图</li><li>没有可视化特征层</li><li>没有可视化卷积核</li></ul><p>【预计在后面调试融合模型出现问题时会回头查看该模型的各层权重直方图】</p><h1 id="code"><a href="#code" class="headerlink" title="code"></a>code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters etc.</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">LEARNING_RATE = <span class="number">2e-4</span>  <span class="comment"># could also use two lrs, one for gen and one for disc</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">IMAGE_SIZE = <span class="number">64</span></span><br><span class="line">CHANNELS_IMG = <span class="number">1</span></span><br><span class="line">NOISE_DIM = <span class="number">100</span></span><br><span class="line">NUM_EPOCHS = <span class="number">5</span></span><br><span class="line">FEATURES_DISC = <span class="number">64</span></span><br><span class="line">FEATURES_GEN = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(IMAGE_SIZE),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(</span><br><span class="line">            [<span class="number">0.5</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(CHANNELS_IMG)], [<span class="number">0.5</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(CHANNELS_IMG)]</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you train on MNIST, remember to set channels_img to 1</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&quot;dataset/&quot;</span>, train=<span class="literal">True</span>, transform=transforms,</span><br><span class="line">                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># comment mnist above and uncomment below if train on CelebA</span></span><br><span class="line"><span class="comment">#dataset = datasets.ImageFolder(root=&quot;celeb_dataset&quot;, transform=transforms)</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels_img, features_d</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            <span class="comment"># input: N x channels_img x 64 x 64</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                channels_img, features_d, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span></span><br><span class="line">            ),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            <span class="comment"># _block(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">            self._block(features_d, features_d * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            self._block(features_d * <span class="number">2</span>, features_d * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            self._block(features_d * <span class="number">4</span>, features_d * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            <span class="comment"># After all _block img output is 4x4 (Conv2d below makes into 1x1)</span></span><br><span class="line">            nn.Conv2d(features_d * <span class="number">8</span>, <span class="number">1</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels,</span><br><span class="line">                out_channels,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride,</span><br><span class="line">                padding,</span><br><span class="line">                bias=<span class="literal">False</span>,</span><br><span class="line">            ),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.disc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels_noise, channels_img, features_g</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            <span class="comment"># Input: N x channels_noise x 1 x 1</span></span><br><span class="line">            self._block(channels_noise, features_g * <span class="number">16</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>),  <span class="comment"># img: 4x4</span></span><br><span class="line">            self._block(features_g * <span class="number">16</span>, features_g * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 8x8</span></span><br><span class="line">            self._block(features_g * <span class="number">8</span>, features_g * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 16x16</span></span><br><span class="line">            self._block(features_g * <span class="number">4</span>, features_g * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 32x32</span></span><br><span class="line">            nn.ConvTranspose2d(</span><br><span class="line">                features_g * <span class="number">2</span>, channels_img, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span></span><br><span class="line">            ),</span><br><span class="line">            <span class="comment"># Output: N x channels_img x 64 x 64</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(</span><br><span class="line">                in_channels,</span><br><span class="line">                out_channels,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride,</span><br><span class="line">                padding,</span><br><span class="line">                bias=<span class="literal">False</span>,</span><br><span class="line">            ),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># Initializes weights according to the DCGAN paper</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):</span><br><span class="line">            nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)</span><br><span class="line">disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)</span><br><span class="line">initialize_weights(gen)</span><br><span class="line">initialize_weights(disc)</span><br><span class="line"></span><br><span class="line">opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">fixed_noise = torch.randn(<span class="number">32</span>, NOISE_DIM, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">writer_real = SummaryWriter(<span class="string">f&quot;logs/real&quot;</span>)</span><br><span class="line">writer_fake = SummaryWriter(<span class="string">f&quot;logs/fake&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">gen.train()</span><br><span class="line">disc.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Target labels not needed! &lt;3 unsupervised</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (real, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real = real.to(device)</span><br><span class="line">        noise = torch.randn(BATCH_SIZE, NOISE_DIM, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">        fake = gen(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            disc_real = disc(real).reshape(-<span class="number">1</span>)</span><br><span class="line">            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))</span><br><span class="line">            disc_fake = disc(fake.detach()).reshape(-<span class="number">1</span>)</span><br><span class="line">            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))</span><br><span class="line">            loss_disc = (loss_disc_real + loss_disc_fake) / <span class="number">2</span></span><br><span class="line">            disc.zero_grad()</span><br><span class="line">            loss_disc.backward()</span><br><span class="line">            opt_disc.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Train Generator: min log(1 - D(G(z))) &lt;-&gt; max log(D(G(z))</span></span><br><span class="line"></span><br><span class="line">        output = disc(fake).reshape(-<span class="number">1</span>)</span><br><span class="line">        loss_gen = criterion(output, torch.ones_like(output))</span><br><span class="line">        gen.zero_grad()</span><br><span class="line">        loss_gen.backward()</span><br><span class="line">        opt_gen.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print losses occasionally and print to tensorboard</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch [<span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;NUM_EPOCHS&#125;</span>] Batch <span class="subst">&#123;batch_idx&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> \</span></span><br><span class="line"><span class="string">                  D(x): <span class="subst">&#123;disc_real.mean().item():<span class="number">.4</span>f&#125;</span>, D(G(z)): <span class="subst">&#123;disc_fake.mean().item():<span class="number">.4</span>f&#125;</span>, output: <span class="subst">&#123;output.mean().item():<span class="number">.4</span>f&#125;</span>, Loss D: <span class="subst">&#123;loss_disc:<span class="number">.4</span>f&#125;</span>, loss G: <span class="subst">&#123;loss_gen:<span class="number">.4</span>f&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = gen(fixed_noise)</span><br><span class="line">                <span class="comment"># take out (up to) 32 examples</span></span><br><span class="line">                img_grid_real = torchvision.utils.make_grid(</span><br><span class="line">                    real[:<span class="number">32</span>], normalize=<span class="literal">True</span></span><br><span class="line">                )</span><br><span class="line">                img_grid_fake = torchvision.utils.make_grid(</span><br><span class="line">                    fake[:<span class="number">32</span>], normalize=<span class="literal">True</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                writer_fake.add_image(<span class="string">&quot;Fake&quot;</span>, img_grid_fake, global_step=step)</span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>blog-markdown书写规则</title>
      <link href="/2022/03/29/blog/blog-markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/"/>
      <url>/2022/03/29/blog/blog-markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/</url>
      
        <content type="html"><![CDATA[<h1 id="blog-markdown书写规则"><a href="#blog-markdown书写规则" class="headerlink" title=" blog-markdown书写规则 "></a><center> blog-markdown书写规则 </center></h1><ul><li>站内文章链接：<code>&#123;% post_link file_name 'show_name' %&#125;</code><ul><li>post_link: 固定参数（不要删除）</li><li>file_name: 在文件夹中的名称，不需要带后缀</li><li>show_name: 不显示文件名字，自定义显示文字，默认显示title</li><li>检索关键词：站内文章</li></ul></li><li>文字效果：&lt;<code>b/i/u/s/big/mark</code>&gt; ******* &lt;&#x2F;<code>b/i/u/s/big/mark</code>&gt;</li><li>文章首行缩进：<code>&lt;p style=&quot;text-indent:2em&quot; &gt;</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TOC</title>
      <link href="/2022/03/29/LIFE%20TOC/"/>
      <url>/2022/03/29/LIFE%20TOC/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="9f639d429c7674ff66fe62ab04d51225ed44b035936b7c00176f32d88f87657a">901015936e2ec1bc2a4d01c822635a06dbabc75c330bdf0d123f64dbd819e43430d0a05c3025834c03c24ebf318f6df515738cb63f9a216b3cae01030f30249b4c39b0efee78df4317fd591b23e5ad8076fd989204ca0663875ca43ddf17ac047e07e2008653afaaedb91f3cc83954156c99bb710833be778b8c37fc25407c487afe0884174bec091d37da6a9571d6638822e59a719bbf7009d1aafcb89e62fa0636349d228b8d37309a51efaa8d941ba56b5b5b8b8d04f755948b76f08b24857e7ca8bca7f693ca0c01fd66b97c554866778b219b84c49f23ae422bd79123958c5e96aaed7f8d3db31f5b724d88a5e14da4e11fd71f7bb32c7bdf6f4492d3dc48d2fce977628e5e03c356eff68efdc995cbc81c6333af03c0893fe0e39ae7caf788a94b2cfca9dd2c44571a77d66a5c8a785c6b1cb2d710af5e45b93e204cf81703c7e55032eca1fbfdb8f8f712ab5178a84f18da30e19b12ba1295c0f9f1c36c750b809a23c4f1f118e869ecf30ca3b29fb704d12bb1c35a1bcb730f8ca040c384420b00c79e58fd34291c6a3185daaf8cca29fbf9458573ec810fb2db39ba8d7002beee20857e50af80fb0b907c31124e5d536b41b92c3dee6c8badda9a10de8225fff757ad2c3f344f37548f1f54c8bbbdd184efb8df7a0ac3a648a9320c91c6118ffbf10f94b72ee03ce513c27aabed13cedd07293dd95e7a825d2269219e037eaafb8ff8bfb3830b76be6534780e68567570ff91f3c162f7586fe946240cebb2f0d3110d3b29f864a8dc9fae349962a8e051db6fc0e68f9042331a1e26e5370dc9e90766d1230d68c4a986e562712bd816e09dd2d9d53330f8f9a9268e1f3b4031d0a0b781976b5516611f4837d7a2c5951d31e23080bd4a9ef80671ddd1cd2a1ee568d9de1b43fdcaf185f486</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双向</title>
      <link href="/2022/03/29/life/%E5%8F%8C%E5%90%91/"/>
      <url>/2022/03/29/life/%E5%8F%8C%E5%90%91/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c89fb12ebdfd2ce00ed8d2b221fea91a9aa40c737ca8541c01040aa9b4742fe5">d8eb541f534e62ff3f46f2f28f0d52a5a1cc6b8838a78340aa66e10bc3a389411123a8e26eecb6d8e16d64418164ed19020aa495700bffdfd5bc9dce1c28f86effba5c06d69bb9741f76c7c8fcb95c76cdcb028fd22690e1110134dcd9df3c2e255924d15160f6dbc18803f06c762044a161653cab4d7283144587248b2870df513c38b156dcb7ac9c5f4b189b46c69cc001a6f326058c08f5b01e5c8c3770f2fdfe6f53b7df9a1d8f1b4e4e9b8a11cca3cb2e47073a25eb6d685997bb127d7b68b35fe28e9852d6d4e5daece160cc48005baa59fccd715970803a68bd82a82b29be2bdb237b0b30fa5fbf828e7ef621872ae5a7df7548765294d473a419ee0118b52a2026afdab44908f6994aca32bc7ddf7f21868fd60eda6ce13592b6b6b1368b2bc7c83f9d67895d778762136f39c4cc30632c689fd5d4e07360465d5f0eac7e34c9d22f5adafef4007830e740b777bb5021d5305ccf6d0335ab86e995bd4bdffc4dd7199a4411d20de69051c07cf0be3b6ac6ce920d551ecf7c3a96583a7c0ccf88ff5e01436fc33bd3e18b6c5e</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Review</title>
      <link href="/2022/03/28/Review/"/>
      <url>/2022/03/28/Review/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引关键词说明：</span></span><br><span class="line">今日计划：</span><br><span class="line">今日完成：</span><br><span class="line">今日遗留：</span><br><span class="line"></span><br><span class="line">明日计划：</span><br><span class="line"></span><br><span class="line">文献检索：检索到一篇文章且有查阅价值</span><br><span class="line"></span><br><span class="line">说明：明天任务中需要的文件</span><br><span class="line">预计：短期工作任务</span><br><span class="line">记录：某些信息检索的关键</span><br><span class="line">标记：记录值得注意的点</span><br><span class="line"></span><br><span class="line">Stack：该问题短时间难以解决</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Retrospective"><a href="#Retrospective" class="headerlink" title=" Retrospective "></a><center> Retrospective </center></h1><h2 id="2022"><a href="#2022" class="headerlink" title="2022"></a>2022</h2><h3 id="4-17"><a href="#4-17" class="headerlink" title="4.17"></a>4.17</h3><ul><li><input disabled="" type="checkbox"> <strong>今日计划</strong><ul><li><input disabled="" type="checkbox"> 完成代码中的初始化部分、每层的权重可视化部分</li><li><input disabled="" type="checkbox"> 完成visdom实时可视化部分，学习远程可视化</li><li><input disabled="" type="checkbox"> 根据初始化后的训练结果观察昨天的训练结果是否需要分析</li><li><input disabled="" type="checkbox"> 增加网络层数</li></ul></li></ul><hr><h3 id="4-10"><a href="#4-10" class="headerlink" title="4.10"></a>4.10</h3><ul><li><input disabled="" type="checkbox"> <strong>明日计划</strong><ul><li><input disabled="" type="checkbox"> 完成style-transfer的代码阅读，关注其中内容上的位置编码如何实现</li><li><input disabled="" type="checkbox"> 查阅新的style-transfer文献</li></ul></li></ul><hr><h3 id="4-9"><a href="#4-9" class="headerlink" title="4.9"></a>4.9</h3><ul><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong></p><ul><li><input checked="" disabled="" type="checkbox"> 阅读一篇文献、源码，撰写总结博客</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>今日遗留</strong></p><ul><li><input disabled="" type="checkbox"> 关于文献中颜色重构那块需要仔细阅读</li></ul></li></ul><hr><h3 id="4-8"><a href="#4-8" class="headerlink" title="4.8"></a>4.8</h3><ul><li><p><input disabled="" type="checkbox"> <strong>今日计划</strong></p><ul><li><input disabled="" type="checkbox"> 在GAN中0和1的硬标签有什么内层原理，如何跟D训练的loss公式对应</li><li><input disabled="" type="checkbox"> 完成昨天文献中的细节理解，解决昨天提出的问题</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>文献检索</strong></p><ul><li><input disabled="" type="checkbox"> 关于style transfor的文献</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>标记</strong></p><ul><li><input disabled="" type="checkbox"> 文献中标记一篇关于GAN inversion的文献</li></ul></li></ul><hr><h3 id="4-7"><a href="#4-7" class="headerlink" title="4.7"></a>4.7</h3><ul><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong></p><ul><li><input checked="" disabled="" type="checkbox"> <ol><li>阅读一篇基于GAN的高保真图片编辑文献、学习其代码、相关博客，完成进度80%</li></ol></li></ul></li><li><p><input disabled="" type="checkbox"> <strong>今日遗留</strong></p><ul><li><input disabled="" type="checkbox"> ADA模块的实现原理，为什么这样的结构能做到对齐</li><li><input disabled="" type="checkbox"> consulation原理，为什么这样的结构能调整权重，且为何如此设计</li></ul></li></ul><hr><h3 id="4-6"><a href="#4-6" class="headerlink" title="4.6"></a>4.6</h3><ul><li><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong><ul><li><input checked="" disabled="" type="checkbox"> <ol><li>学习感知损失，并完成相关博客</li></ol></li></ul></li></ul><hr><h3 id="4-4"><a href="#4-4" class="headerlink" title="4.4"></a>4.4</h3><ul><li><input disabled="" type="checkbox"> <strong>Stack</strong><ul><li><input disabled="" type="checkbox"> 无法理解作者的解释，关于[Generative Image Inpainting with Contextual Attention]一文中的input[x,ones_like(x),masked]为何concat全一矩阵：加入该层是为了指出图像的边界<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Issues:</span><br><span class="line">during earlier development of this work, we use mirror</span><br><span class="line">padding. However, we find this <span class="keyword">is</span> almost equivalent to</span><br><span class="line">concatenate ones (indicating the boundaries of images) <span class="keyword">as</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line">convolution automatically pad zeros <span class="keyword">as</span> <span class="keyword">in</span> <span class="string">&#x27;SAME&#x27;</span> mode.</span><br></pre></td></tr></table></figure></li></ul></li></ul><hr><h3 id="4-2"><a href="#4-2" class="headerlink" title="4.2"></a>4.2</h3><ul><li><input disabled="" type="checkbox"> <strong>记录</strong><ul><li><input disabled="" type="checkbox"> 关键词：纹理合成、纹理缝合、纹理生成</li></ul></li></ul><hr><h3 id="4-1"><a href="#4-1" class="headerlink" title="4.1"></a>4.1</h3><ul><li><p><input disabled="" type="checkbox"> <strong>今日计划</strong></p><ul><li><input disabled="" type="checkbox"> 写一篇关于GPU信息含义如何理解的博客</li></ul></li><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong></p><ul><li><input checked="" disabled="" type="checkbox"> 阅读SSIM源码，理解SSIM公式，同时在SSIM博客中提出问题</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>明日计划</strong></p><ul><li><input disabled="" type="checkbox"> 完成图像梯度原理理解</li></ul></li><li><p><input disabled="" type="checkbox"> <mark><strong>idea</strong></mark></p><ul><li><p>发现了一篇关于local attention为何起作用的<a href="https://arxiv.org/abs/2106.04263">论文</a>，其中有网络设计准则，值得参考，其推荐博客已经存储在网络结构设计中</p></li><li><p>depth-wise conv有些忘记，需要回顾</p></li></ul></li></ul><hr><h3 id="3-31"><a href="#3-31" class="headerlink" title="3.31"></a>3.31</h3><ul><li><p><input disabled="" type="checkbox"> <strong>今日计划</strong></p><ul><li><input disabled="" type="checkbox"> 训练DIDfuse代码，观察训练时间，必要时可视化模型</li><li><input disabled="" type="checkbox"> 学习异常检测代码</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>遇到问题</strong></p><ul><li><input checked="" disabled="" type="checkbox"> SSIM原理</li><li><input checked="" disabled="" type="checkbox"> 什么是一维向量</li></ul></li><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong></p><ul><li><input checked="" disabled="" type="checkbox"> 如何提取图像梯度</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>说明</strong></p><ul><li><input disabled="" type="checkbox"> 需要使用的代码在Image-Fusion-Transformer、FusionGAN-pytorch-master代码中</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>今日遗留</strong></p><ul><li><input disabled="" type="checkbox"> 几篇关于SSIM的博客</li><li><input disabled="" type="checkbox"> 写一篇关于SSIM的博客</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>预计</strong></p><ul><li><input disabled="" type="checkbox"> 修改代码中关于梯度的loss函数</li></ul></li></ul><hr><h3 id="3-30"><a href="#3-30" class="headerlink" title="3.30"></a>3.30</h3><ul><li><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong><ul><li><input checked="" disabled="" type="checkbox"> 完成DCGAN生成MNIST数据集，分析DCGAN训练不work的原因 </li><li><input checked="" disabled="" type="checkbox"> 阅读清单中遗留文章已查阅</li><li><input checked="" disabled="" type="checkbox"> 搭建完成全卷积siam网络模型，正在训练</li></ul></li><li><input disabled="" type="checkbox"> <strong>明日计划</strong><ul><li><input disabled="" type="checkbox"> 由于训练较慢，初步判断为预测特征图导致，故训练他人的模型进行对比</li></ul></li></ul><hr><h3 id="3-29"><a href="#3-29" class="headerlink" title="3.29"></a>3.29</h3><ul><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong>:</p><ul><li><input checked="" disabled="" type="checkbox"> 进行了DCGAN对MNIST数据集的训练，效果极差，查找发现他人训练好后证明效果还行的源码直接拿来训练也效果极差，初步估计为GAN模型极易崩塌</li><li><input checked="" disabled="" type="checkbox"> 进行了WGAN对MNIST数据集的训练，效果人眼可分辨</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>预计</strong>:</p><ul><li><input disabled="" type="checkbox"> 1.完成遗留任务</li><li><input disabled="" type="checkbox"> 2.进行FGAN或者其他融合网络的训练，主要进行可视化操作（loss、image、特征图、卷积核）</li></ul></li></ul><hr><h3 id="3-28"><a href="#3-28" class="headerlink" title="3.28"></a>3.28</h3><ul><li><p><input checked="" disabled="" type="checkbox"> <strong>今日完成</strong>:</p><ul><li><input checked="" disabled="" type="checkbox"> 1.完成loss、image、histogram可视化代码</li><li><input checked="" disabled="" type="checkbox"> 2.完成风格损失1000-1-50-100的训练</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>今日遗留</strong>:</p><ul><li><input disabled="" type="checkbox"> 1.阅读清单中几篇文章</li><li><input disabled="" type="checkbox"> 2.阅读清单中有两个博主的其他博客需要查看</li><li><input disabled="" type="checkbox"> 3.缓存了几个网页需要查阅</li><li><input disabled="" type="checkbox"> 4.完成风格损失1000-1-50-100的可视化理解</li><li><input disabled="" type="checkbox"> 5.完成内容损失1000-1-50-100的理解</li></ul></li><li><p><input disabled="" type="checkbox"> <strong>预计</strong>:</p><ul><li><input disabled="" type="checkbox"> 1.卷积核的可视化</li><li><input disabled="" type="checkbox"> 2.特征层的可视化</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>模型调整方法探究</title>
      <link href="/2022/03/27/work/%E6%A8%A1%E5%9E%8B%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%8E%A2%E7%A9%B6/"/>
      <url>/2022/03/27/work/%E6%A8%A1%E5%9E%8B%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%8E%A2%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>探究问题：在结合新idea搭建好模型后，该模型毫无疑问不会达到预期的效果，甚至是没有效果，本文欲找出问题所在之处并，进行数据分析，最后调整</p></blockquote><p>总体步骤分解为：<strong>定位——分析——调整</strong></p><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><ol><li>可视化<ol><li>可视化loss数据</li><li>可视化每个卷积层输出的特征层</li><li>可视化卷积核</li></ol></li><li>可疑之处<ol><li>loss变化减缓</li><li>特征层奇怪</li><li>卷积核奇怪</li></ol></li></ol><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="1-出问题的部分在loss数据中如何体现？"><a href="#1-出问题的部分在loss数据中如何体现？" class="headerlink" title="1. 出问题的部分在loss数据中如何体现？"></a>1. 出问题的部分在loss数据中如何体现？</h3><h4 id="1-loss"><a href="#1-loss" class="headerlink" title="1.loss"></a>1.loss</h4><blockquote><p>主要表现为突变</p></blockquote><h4 id="2-histogram"><a href="#2-histogram" class="headerlink" title="2.histogram"></a>2.<strong>histogram</strong></h4><p>【<a href="https://blog.csdn.net/u011668104/article/details/90517879?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164846316016782094836597%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164846316016782094836597&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-90517879.142%5Ev5%5Epc_search_result_control_group,143%5Ev6%5Eregister&utm_term=add_histogram&spm=1018.2226.3001.4187">参考博客</a>】</p><ol><li>训练好的模型权重更加集中，波动范围更小</li><li>初始化很重要，查看初始化权重以及训练好的权重，差异不大的话很有可能网络没有学到任何东西（一般来说其收敛中心有变化，变换范围更小）</li></ol><h3 id="2-探究多任务损失函数系数对训练的影响"><a href="#2-探究多任务损失函数系数对训练的影响" class="headerlink" title="2. 探究多任务损失函数系数对训练的影响"></a>2. 探究多任务损失函数系数对训练的影响</h3><blockquote><p>就风格迁移模型，风格损失刚开始在1200左右，内容损失在[0,20]，</p></blockquote><ol><li><p>将风格损失的权重设置为[1,50,100]实际上在图片上的影响效果基本没有影响，各自图片变化阶段也基本相同——<mark>说明系数并不会影响对应loss约束的特征在训练中提早&#x2F;延后表征</mark></p></li><li><p>但是随着风格损失权重增大（下图1000-1-50-100），可发现风格损失权重越大，内容损失会随着训练次数逐渐升高（越来越抽象且和原图像差距越来越大）——<mark>说明调整loss权重，只是更关注某个loss约束的特征，该特征表征能力保持不变——表现为loss变化幅度基本没有，其他特征表征能力下降——表现为loss原本应该下降或平缓，在增大其他权重时，loss开始上升</mark></p></li></ol><center> content_loss </center><p><img src="/../img/1000-1-50-100.png" alt="训练图片"></p><p>3.从模型的三个loss曲线中观察，权重较大的loss基本没有变换，相反权重较小的loss从原来的收敛变成增加（从上往下风格损失权重：1-50-100）</p><center> content_loss-------------------loss--------------------style_loss </center><p><img src="/../img/1000-1-50-100-all_loss.png" alt="训练图片"></p><ol start="3"><li>直方图基本没有任何变换</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能学习建议</title>
      <link href="/2022/03/26/work/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/"/>
      <url>/2022/03/26/work/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="1783964657b2d4989af6f3221d07e0f40b81e343c8c9496ed0a2813ef2ce4e28">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper</title>
      <link href="/2022/03/25/Paper/"/>
      <url>/2022/03/25/Paper/</url>
      
        <content type="html"><![CDATA[<h1 id="文献总览"><a href="#文献总览" class="headerlink" title="文献总览"></a>文献总览</h1><h2 id="图像修复"><a href="#图像修复" class="headerlink" title="图像修复"></a>图像修复</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Generative Image Inpainting with Contextual Attention</a></td><td align="center">使用背景信息填补前景遮盖，做相似度计算</td><td align="center">None</td></tr><tr><td align="center"><a href="/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/" title="[gan]High-Fidelity GAN Inversion for Image Attribute Editing">[gan]High-Fidelity GAN Inversion for Image Attribute Editing</a></td><td align="center">使用失真图▽的特征图加入到decoder的low space中，还提到有差异的两张图如何对齐</td><td align="center">None</td></tr><tr><td align="center"><a href="/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/" title="Image Inpainting with External-internal Learning and Monochromic Bottleneck">Image Inpainting with External-internal Learning and Monochromic Bottleneck</a></td><td align="center">优化缺失区域来间接引导非缺失区域着色</td><td align="center">None</td></tr></tbody></table><h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/" title="StyTr2:Image Style Transfer with Transformers">StyTr2:Image Style Transfer with Transformers</a></td><td align="center">根据输入图片学习一个位置标签（硬编码-&gt;软编码）</td><td align="center">仅值得参考</td></tr><tr><td align="center"><a href="/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/" title="ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows">ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows</a></td><td align="center">特征提取与图像重建结构完全对称</td><td align="center">None</td></tr></tbody></table><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Training Generative Adversarial Networks in One Stage</a></td><td align="center">将GAN的双阶段训练改为单阶段训练，但无法多次训练D，只训练一次G</td><td align="center">训练源码，没有任何效果</td></tr></tbody></table><h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Axial attention in multidimesional transformers</a></td><td align="center">将整幅图片进行patch转换成仅关注轴向信息，有效降低transformer计算量</td><td align="center">None</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客参考资料</title>
      <link href="/2022/03/18/blog/%E5%8D%9A%E5%AE%A2%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
      <url>/2022/03/18/blog/%E5%8D%9A%E5%AE%A2%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<p>主题配置：<a href="https://butterfly.js.org/posts/ceeb73f/">butterfly</a></p><p>学习视频：<a href="https://www.bilibili.com/video/BV1Rg41177pX/?spm_id_from=333.788">视频</a></p><p>博客推荐：<a href="https://zfe.space/">小冰博客</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练loss总览</title>
      <link href="/2022/03/17/fragments/%E8%AE%AD%E7%BB%83loss%E6%80%BB%E8%A7%88/"/>
      <url>/2022/03/17/fragments/%E8%AE%AD%E7%BB%83loss%E6%80%BB%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>第一次使用tensorboard可视化loss</p></blockquote><p><img src="/../img/md1.jpg" alt="训练图片"></p>]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Fragments </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo部署中SSH相关问题</title>
      <link href="/2022/03/17/blog/hexo%E9%83%A8%E7%BD%B2%E4%B8%ADSSH%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
      <url>/2022/03/17/blog/hexo%E9%83%A8%E7%BD%B2%E4%B8%ADSSH%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：将github账户作为仓库存储代码，或者将其作为远端时，出现相关报错或者提示——存在ssh相关信息</p></blockquote><h1 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h1><p>下方是在hexo中将Github作为远端，使用<code>heox d</code>远端推送时出现ssh密钥失效的问题，具体错误代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fatal: unable to access <span class="string">&#x27;https://github.com/Reggci/Reggci.github.io/&#x27;</span>: OpenSSL SSL_read: Connection was reset, errno <span class="number">10054</span></span><br><span class="line">FATAL &#123;</span><br><span class="line">  err: Error: Spawn failed</span><br><span class="line">      at ChildProcess.&lt;anonymous&gt; (E:\blog\node_modules\hexo-util\lib\spawn.js:<span class="number">51</span>:<span class="number">21</span>)</span><br><span class="line">      at ChildProcess.emit (node:events:<span class="number">390</span>:<span class="number">28</span>)</span><br><span class="line">      at ChildProcess.cp.emit (E:\blog\node_modules\cross-spawn\lib\enoent.js:<span class="number">34</span>:<span class="number">29</span>)</span><br><span class="line">      at Process.ChildProcess._handle.onexit (node:internal/child_process:<span class="number">290</span>:<span class="number">12</span>) &#123;</span><br><span class="line">    code: <span class="number">128</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125; Something<span class="string">&#x27;s wrong. Maybe you can find the solution here: %s https://hexo.io/docs/troubleshooting.html</span></span><br></pre></td></tr></table></figure><ul><li>解决：</li></ul><blockquote><ol><li>可能是时间久了后本地与Github连接失效</li><li>可能是网络延迟无法push</li></ol></blockquote><ol><li>首先查看本地用户名以及邮箱是否是自己的Github（可跳过步骤）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git config user.name  <span class="comment"># 查看本地用户名配置</span></span><br><span class="line">git config user.email <span class="comment"># 查看本地邮箱配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不是重新配置即可</span></span><br><span class="line">git config --<span class="keyword">global</span> user.name <span class="string">&#x27;xxxx&#x27;</span></span><br><span class="line">git config --<span class="keyword">global</span> user.email <span class="string">&#x27;xxxx&#x27;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>生成本地公钥</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;your github 注册邮箱&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>打开 C:\Users.ssh文件夹，将公钥文件中的内容复制到GitHub的SSH中</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">id_rsa<span class="comment"># 私钥</span></span><br><span class="line">id_rsa.pub<span class="comment"># 公钥</span></span><br></pre></td></tr></table></figure><ol start="4"><li>检测SSH Key生效</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github响应超时</title>
      <link href="/2022/03/17/blog/github%E8%B6%85%E6%97%B6%E7%9B%B8%E5%BA%94%E9%97%AE%E9%A2%98/"/>
      <url>/2022/03/17/blog/github%E8%B6%85%E6%97%B6%E7%9B%B8%E5%BA%94%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：github相应超时，经常无法快速连接</p></blockquote><p>解决：更改本地hosts文件，文件位置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:/Windows/system32/divers/etc/hosts</span><br></pre></td></tr></table></figure><h1 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h1><p>打开上述hosts文件增加如下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#           140.82.112.4       github.com</span><br></pre></td></tr></table></figure><h1 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h1><ul><li>进入<a href="https://gitee.com/doshengl/GitHub520">相关网址</a>，复制其中的hosts地址粘贴到本地文件即可——该网站会实时更新地址</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
