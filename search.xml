<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>专利撰写说明</title>
      <link href="/2022/09/06/%E4%B8%93%E5%88%A9%E6%92%B0%E5%86%99%E8%AF%B4%E6%98%8E/"/>
      <url>/2022/09/06/%E4%B8%93%E5%88%A9%E6%92%B0%E5%86%99%E8%AF%B4%E6%98%8E/</url>
      
        <content type="html"><![CDATA[<h1 id="专利文件"><a href="#专利文件" class="headerlink" title="专利文件"></a>专利文件</h1><ol><li>权利要求书；</li><li>说明书；</li><li>说明书附图；</li><li>说明书摘要；</li><li>摘要附图；</li></ol><h1 id="专利要点"><a href="#专利要点" class="headerlink" title="专利要点"></a>专利要点</h1><ol><li>和现有方案的区别点（那个步骤不同）；</li><li>区别点解决的技术问题；</li><li>区别点达到的技术效果；</li></ol><h1 id="1-说明书摘要"><a href="#1-说明书摘要" class="headerlink" title="1.说明书摘要"></a>1.说明书摘要</h1><p>名称+领域+方案+效果</p><h1 id="2-权利要求书"><a href="#2-权利要求书" class="headerlink" title="2. 权利要求书"></a>2. 权利要求书</h1><ol><li><p>权力1有固定的格式，注意描述步骤时不要出现“等、大约、优选、比如。。。”不确定词汇，&#x3D;&#x3D;注意描述自己创新的步骤时可略作详细，非创新直接带过即可，突出自己与其他专利的不同&#x3D;&#x3D;</p><blockquote><p>一种…名称…方法，其特征在于，所述…方法包括&#x2F;包括以下步骤：</p></blockquote></li><li><p>权力2~n，固定格式，详细描述各步骤的细节</p><blockquote><p>根据权利要求n所述的+名称，其特征在于，所述步骤Sn（中的xxx表示为xxx符号）&#x2F;（包括以下步骤）</p></blockquote></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>openmmlab框架</title>
      <link href="/2022/08/23/openmmlab%E6%A1%86%E6%9E%B6/"/>
      <url>/2022/08/23/openmmlab%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="配置文件读取"><a href="#配置文件读取" class="headerlink" title="配置文件读取"></a>配置文件读取</h1><blockquote><p>设置一个临时文件，将源文件内容加载到临时文件中，提取出临时文件的内容，删除临时文件，将所有信息保留在一个类对象中<br>1.得到一个字典类型的返回值<br>2.得到一个带有源文件及其内容的字符串<br>将上述两个参数解析，保留在config类对象中</p></blockquote><p>【其中涉及到类的操作】</p><ol><li>可先调用类的静态函数进行参数的收集（之前没有将该类进行实例化）</li><li>根据类的方法得到的参数再进行类的实例化</li></ol><p>【文件结构】</p><ol><li>选择基类配置文件</li><li>在基类配置文件的基础上进行修改<ul><li>因为存在覆盖的效果，直接设置基类key的修改值value，就能实现修改，但如果该值在基类中在其它处也被调用的话，需要一一修改，否则不能达到修改的效果</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>概率论与数理统计</title>
      <link href="/2022/08/20/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
      <url>/2022/08/20/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="1-连续型随机变量和离散型随机变量有什么区别？"><a href="#1-连续型随机变量和离散型随机变量有什么区别？" class="headerlink" title="1. 连续型随机变量和离散型随机变量有什么区别？"></a>1. 连续型随机变量和离散型随机变量有什么区别？</h1><p>离散型随机变量x：只可能出现有限个或可列个值<br>连续型随机变量x：不可列，通常不研究它去某个特定值得概率，而研究它在某个区间上的概率</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>markdown书写规则</title>
      <link href="/2022/08/18/markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/"/>
      <url>/2022/08/18/markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">markdown书写规范</span><br></pre></td></tr></table></figure><h1 id="公式编写"><a href="#公式编写" class="headerlink" title="公式编写"></a>公式编写</h1><h2 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h2><blockquote><p>格式如下：</p><ol><li>首先对齐声明<code>\begin&#123;aligned&#125;;\end&#123;aligned&#125;</code></li><li>在对齐的&#x3D;号前使用<code>&amp;</code>标记</li><li>换行符&#96;&#96;\\&#96;</li></ol><p>使用align会对多行公式编号，而aligned只编一个号</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x_1 &amp;= x_2 + x_3 \\</span><br><span class="line">    &amp;= .....   </span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure><h2 id="符号代码"><a href="#符号代码" class="headerlink" title="符号代码"></a>符号代码</h2><ol><li>公式省略号——<code>\cdots</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> Rules </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>卡尔曼滤波</title>
      <link href="/2022/08/18/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/"/>
      <url>/2022/08/18/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/</url>
      
        <content type="html"><![CDATA[<h1 id="1-如何预测下一步数据？（如何分析测量值和估计值之间的额关系）"><a href="#1-如何预测下一步数据？（如何分析测量值和估计值之间的额关系）" class="headerlink" title="1. 如何预测下一步数据？（如何分析测量值和估计值之间的额关系）"></a>1. 如何预测下一步数据？（如何分析测量值和估计值之间的额关系）</h1><blockquote><p>当存在k个时序数据，如何估计下一个数据的变化？很自然想到使用已有数据的平均值来估计下一个数据,&#x3D;&#x3D;从平均数下手，分析平均数、当前值测试值、以往估计值之间的关系&#x3D;&#x3D;（假设其中估计值用之前的平均值）</p></blockquote><p>$$<br>\begin{align}<br>\hat{x}<em>{k} &amp;&#x3D;\frac{1}{k}\left(z</em>{1}+z_{2}+\cdots+z_{k}\right) \<br>&amp;&#x3D;\frac{1}{k}\left(z_{1}+z_{2}+\cdots+z_{k-1}\right)+\frac{1}{k} z_{k}\<br>&amp;&#x3D;\frac{1}{k} \frac{k-1}{(k-1)}\left(z_{1}+z_{2}+\cdots+z_{k-1}\right)+\frac{1}{k} z_{k} \<br>&amp;&#x3D;\frac{k-1}{k} \hat{x}<em>{k-1}+\frac{1}{k} z</em>{k} \<br>&amp;&#x3D;\hat{x}<em>{k-1}-\frac{1}{k} \hat{x}</em>{k-1}+\frac{1}{k} z_{k} \<br>\hat x_k &amp;&#x3D; \hat x_{k-1}+ \frac{1}{k}(z_k - \hat x_{k-1})<br>\end{align}<br>$$<br><strong>简式2</strong>：将当前测试值提出，用于分析当前测试值与之前数据值的关系<br><strong>简式3，4</strong>：将以往值化简为估计值<br><strong>简式5，6</strong>：合并化简</p><h1 id="2-Kalman增益"><a href="#2-Kalman增益" class="headerlink" title="2. Kalman增益"></a>2. Kalman增益</h1><p>可以从平均数的分析中发现，式（6）中k较小时（测试初期数据较少），估计值极大依赖当前测试值，但是&#x3D;&#x3D;随着k变大（测试数据增大），测量结果不再重要&#x3D;&#x3D;，因为当前估计值基本与上次估计值相等（后面一项公式接近于0），这和实际情况也符合（当对一件事掌握的信息越多，对其下次变化的估计也就越准确，越稳定），可理解为下式：</p><p>$$<br>\begin{equation}<br>当前估计值 &#x3D; 上一次估计值 + 系数 × （当前测量值-上一次估计值）<br>\end{equation}<br>$$<br>该式中的系数被称为Kerman增益，实际上其计算公式：<br>$$<br>\begin{equation}<br>K_k &#x3D; \frac{e_{est}}{e_{est}+e_{mea}}<br>\end{equation}<br>$$<br>其中est表示估计值，mea表示测量值</p><ul><li>当$e_{est}&gt;&gt;e_{mea}$时，表示估计值误差太大，只能信任测量值；将$K_k&#x3D;1$带入式（6）得$\hat x_k&#x3D;z_k$，即第k次估计值应等于测量值</li><li>反之，测量值误差太大，只能信任估计值，将$K_k&#x3D;0$带入式（6），第k次估计值等于估计值</li></ul><h1 id="Kalman计算步骤"><a href="#Kalman计算步骤" class="headerlink" title="Kalman计算步骤"></a>Kalman计算步骤</h1><p>$$<br>\begin{align}<br>K_k &#x3D; \frac{e_{est}}{e_{est} + e_{mea}} \<br>\hat x_k &#x3D; \hat x_{k-1} + K_k(Z_k - \hat x_{k-1}) \<br>e_{est} &#x3D; (1-K_k)e_{est_{k-1}}<br>\end{align}<br>$$</p><ol><li>式9计算卡尔曼增益$K_k$</li><li>式10根据以往数据计算下一步估计值</li><li>式11更新估计误差</li></ol><h1 id="数据融合"><a href="#数据融合" class="headerlink" title="数据融合"></a>数据融合</h1><blockquote><p>例1：两个称称量同一个物体，两个称存在已知偏差σ，如何通过两个称合理地计算出一个更准确地数值？<br>使用式（10），题目的问法可以理解为：通过两个已知数据的分布，找到一个更为准确、误差更小的数据分布——即计算公式两边的方差，找一个k让期望数值的方差最小</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Work </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络</title>
      <link href="/2022/08/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
      <url>/2022/08/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">学习背景：两个电脑使用不同的鼠标导致频繁换鼠标，寻求两个电脑公用一个鼠标的方法</span><br></pre></td></tr></table></figure><h1 id="1-局域网内使用软件可实现"><a href="#1-局域网内使用软件可实现" class="headerlink" title="1. 局域网内使用软件可实现"></a>1. 局域网内使用软件可实现</h1><blockquote><p>经过搜索发现，该类软件较多，不过要求设备在同一个局域网内，针对局域网等计算机网络名词开始学习</p></blockquote><table><thead><tr><th align="center">名词</th><th align="center">解释</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">网络</td><td align="center">多台电脑可以互相交互数据，就可以简单的称为网络</td><td align="center"></td></tr><tr><td align="center">局域网</td><td align="center">多个网络构成</td><td align="center"></td></tr><tr><td align="center">城域网</td><td align="center">多个局域网构成</td><td align="center">局域网&#x2F;广域网的范围按情况而定，例如家——楼——小区，或学校——城市——国家</td></tr><tr><td align="center">广域网</td><td align="center">多个城域网构成</td><td align="center"></td></tr><tr><td align="center">内网</td><td align="center">局域网</td><td align="center"></td></tr><tr><td align="center">外网</td><td align="center">广域网</td><td align="center">当局域网关闭向广域网的数据传输通道，此时局域网可理解为内网，广域网理解为外网</td></tr><tr><td align="center">因特网&#x2F;internet&#x2F;互联网</td><td align="center">特殊的广域网，最大的计算机网络</td><td align="center"></td></tr><tr><td align="center">以太网</td><td align="center">一种技术，一种实现设备共享的数据传输技术</td><td align="center"></td></tr></tbody></table><h1 id="2-非局域网内如何实现？"><a href="#2-非局域网内如何实现？" class="headerlink" title="2. 非局域网内如何实现？"></a>2. 非局域网内如何实现？</h1><blockquote><p>可使用内网穿透，对于这类名词开始学习</p></blockquote><table><thead><tr><th align="center">名词</th><th align="center">解释</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">IPv4和IPv6</td><td align="center">都用来标识连接到internet的设别的地址，只不过分别是IP协议的第四个版本与第六个版本</td><td align="center"></td></tr><tr><td align="center">公网IP</td><td align="center">唯一地址（如某省某市某小区）</td><td align="center"></td></tr><tr><td align="center">保留地址</td><td align="center">可重复的（可理解为不同小区的第某栋第某号，虽然栋号相同，但小区不同）</td><td align="center">通常在接入路由器&#x2F;交换机后，由其自动分配私有地址（DHCP协议实现，关闭该协议可实现手动设置地址）</td></tr><tr><td align="center">端口</td><td align="center">相当于房子的门，端口对应到某个具体的进程，指明是计算机xx进程需要数据</td><td align="center">地址由IP:端口组成，相同的两个IP传输数据，但是端口不同，这也被认为是新的连接</td></tr><tr><td align="center">NAT技术</td><td align="center">IP转换技术，当私有IP发送数据后，交换机将私有地址改为公有IP+端口，即以自己的名义将数据发送（类似于卧底，只有交换机才知道上下层）</td><td align="center"></td></tr><tr><td align="center">端口映射</td><td align="center">由于交换机存在IP转换，导致端口也被重新随机分配，当私有地址关闭连接时，目标就无法主动发送数据过来，因为关闭连接时交换机将之前的IP及端口释放了，这种情况下，就需要端口映射将端口固定，实现对方也可以主动发数据过来</td><td align="center">理想情况下为端口直连，这样速度直接为两端各自的速度，但是端口绑定只能实现在自己私有地址的上层路由器，实际情况下自己的IP是由多个路由器分配下来，由于无法控制上上层、上上上层路由器进行端口绑定，即它们又开始随机分配</td></tr><tr><td align="center">内网穿透</td><td align="center">一旦两个主机都是存在NAT下，即无法主动向对方发送数据（被交换机丢掉），次数需要中间服务器实现两个主机的连接</td><td align="center">但是数据传输速度由中间服务器控制，导致速度下降</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文排版</title>
      <link href="/2022/08/14/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/"/>
      <url>/2022/08/14/%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/</url>
      
        <content type="html"><![CDATA[<h1 id="双列排版论文"><a href="#双列排版论文" class="headerlink" title="双列排版论文"></a>双列排版论文</h1><blockquote><p>正常单列排版的文章,某些图片过大，需要文字双列、图片单列。</p></blockquote><ol><li>选中正文包括图片，布局——栏——两栏</li><li>光标定位到单列排版的图片之前，分隔符——连续，栏——一栏</li><li>光标定位到图片之后，文字之前，分隔符——连续，栏——两栏</li></ol><h1 id="页脚设置"><a href="#页脚设置" class="headerlink" title="页脚设置"></a>页脚设置</h1><ol><li><p>首页页脚与其他页不同</p><blockquote><p>首页页脚需要标注作者信息，但一设置所有页都存在相同的设置</p></blockquote></li><li><p>设置页脚，填写相关信息</p></li><li><p>在第一页末尾或者第二页起始，分隔符——下一页</p></li><li><p>选择第二页的页脚，取消链接到前一节选项，页脚——删除页脚</p></li></ol><h1 id="参考文献排版"><a href="#参考文献排版" class="headerlink" title="参考文献排版"></a>参考文献排版</h1><ol><li>出现间距过大情况，多出现于英文</li></ol><ul><li>一般来说参考文献要求的字体更小，相对于正文字体会消除很多间距过大的问题</li><li>整体调整好后，仅仅只有个位数的参考文献需要调整，此时手动调整即可——英文可断开换行，手动添加单词换行连接符</li></ul>]]></content>
      
      
      <categories>
          
          <category> Office </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer</title>
      <link href="/2022/08/03/%E5%89%91%E6%8C%87offer/"/>
      <url>/2022/08/03/%E5%89%91%E6%8C%87offer/</url>
      
        <content type="html"><![CDATA[<h1 id="03-数组中重复的数字"><a href="#03-数组中重复的数字" class="headerlink" title="03.数组中重复的数字"></a>03.数组中重复的数字</h1><blockquote><p>在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。</p></blockquote><ul><li><p>【题点】</p><ul><li>长度为n</li><li>元素取值范围[0,n-1]</li><li>元素存在重复</li></ul></li><li><p>【思路】</p><ol><li>排序——将下标与元素对齐</li><li>对比</li></ol></li><li><p>【思考】</p><ol><li>直接对比会使用多个for嵌套，造成复杂度上升</li><li>最终的列前一部分为排序好的列表，后面为重复元素（随机排序）</li></ol></li><li><p>【bug】</p><ol><li>进行排序交换的两个元素有可能相同，此时可直接返回该元素</li></ol></li></ul><hr><h1 id="04-二维数组中的查找"><a href="#04-二维数组中的查找" class="headerlink" title="04.二维数组中的查找"></a>04.二维数组中的查找</h1><blockquote><p>在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个高效的函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p></blockquote><ul><li>【题点】<ol><li>二维数组</li><li>递增</li><li>边界</li></ol></li><li>【思路】<ol><li>考虑从一个定向开始移动，左上和右下角的数字两侧均是增大&#x2F;减小——无法定向；左下和右上一侧增大，一侧减小——可以选择一个作为定向</li><li>无论是小还是大，可以从行或列进行推动，然后再推动列或行，达到定位的目的</li></ol></li><li>【思考】<ul><li>一维数组可直接使用二分法——要么大要么小，已经定向</li><li>二维数组需要考虑定向问题，如果目标小，而数据各方向都为小，此时不知向哪个方向移动——直接二分从中间开始就会出现左上两侧均大或者右下两侧均小，此时无法定向</li><li>选择四个边界为基准，此时靠墙，那么只有两个方向，问题回到一维</li></ul></li><li>【bug】<ol><li>&#x3D;&#x3D;注意使用for循环列表时，注意列表长度为0的情况，此时for会直接报错，可使用while首先就确定边界问题</li><li>注意起始点非原点开始时，首先确定坐标，而不是上来直接使用for<br>##&#x3D;&#x3D;for与while存在什么区别？&#x3D;&#x3D;</li></ol></li></ul><ol><li>while强调条件判断，每次循环都需要判断条件是否满足——猜数游戏中由于while每次都判断，故此时不需要像for语句一样从某侧循环过去</li><li>for在已知循环次数（容器）的情况下，不需要再判断边界——针对已知边界情况（不需要关注边界问题）时，for语句更高效</li></ol><hr><h1 id="05-替换空格"><a href="#05-替换空格" class="headerlink" title="05. 替换空格"></a>05. 替换空格</h1><blockquote><p>请实现一个函数，把字符串 s 中的每个空格替换成”%20”。——本题需要使用C++完成，python中的字符串类型无法被修改</p></blockquote><ul><li>【思路】<ul><li>首先一个空格对应三个位，先增加字符串长度</li><li>设置两个指针，前面的判断是不是空格，后面的起复制加添加字符串的功能</li></ul></li><li>【bug】<ul><li>for语句后面是()不是{}</li><li>多个字符串使用<code>&quot;&quot;</code>而不是<code> &#39;&#39;</code></li><li>C++中考虑到字符串之类的要考虑到位的概念</li></ul></li></ul><hr><h1 id="06-从尾到头打印链表"><a href="#06-从尾到头打印链表" class="headerlink" title="06. 从尾到头打印链表"></a>06. 从尾到头打印链表</h1><blockquote><p>输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。</p></blockquote><ul><li>【补充】<ul><li>在python中的链表，ListNode存有两个值，ListNode.val为当前阶段结点的数值，ListNode.next指向下一个结点</li><li>链表作为输入，输入的是某个结点</li></ul></li><li>【思路】<ul><li>可以顺序取出在反转</li><li>可以使用insert永远插在第一个位置</li></ul></li><li>【bug】<ul><li>实际场景下使用的是链表某节点的值，不要把阶段传入使用</li></ul></li></ul><hr><h1 id="07-重建二叉树"><a href="#07-重建二叉树" class="headerlink" title="07. 重建二叉树"></a>07. 重建二叉树</h1><blockquote><p>输入某二叉树的前序遍历和中序遍历的结果，请构建该二叉树并返回其根节点。<br>假设输入的前序遍历和中序遍历的结果中都不含重复的数字。</p></blockquote><ul><li>【补充】<ul><li>树<ol><li>深度优先——先确定根节点的顺序，接下来都是先左后右<ol><li>先序遍历——【根，左，右】从根节点开始输出，再输出所有左侧结点，最后输出右侧（右左，右右）结点</li><li>中序遍历——【左下叶子树，结点，右子树】所有结点放在中间，找到最左下的叶节点开始输出，最后输出右侧结点</li><li>后序遍历——【左下叶子树，右子树，结点】</li></ol></li><li>广度优先</li></ol></li></ul></li><li>【思路】</li><li>【思考】</li><li>【bug】</li></ul><hr><h1 id="09-用两个栈实现队列"><a href="#09-用两个栈实现队列" class="headerlink" title="09. 用两个栈实现队列"></a>09. 用两个栈实现队列</h1><blockquote><p>用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deleteHead 操作返回 -1 )</p></blockquote><ul><li>【题点】<ul><li>栈、队列</li></ul></li><li>【思路】<ul><li>用两个栈模拟队列先进先出，不用考虑实际存储空间是什么样子的，只需要模拟先进先出的特性即可</li></ul></li><li>【思考】<ul><li>注意两个栈模拟队尾和队头</li></ul></li><li>【bug】<ul><li>注意删除函数的判断<ol><li>出栈stack为空——将进栈stack的数据元素弹出并压入</li><li>出栈不为空——弹出并返回；否则，返回-1</li></ol></li></ul></li></ul><hr><h1 id="10-1-斐波那契数列"><a href="#10-1-斐波那契数列" class="headerlink" title="10.1 斐波那契数列"></a>10.1 斐波那契数列</h1><blockquote><p>写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项（即 F(N)）。斐波那契数列的定义如下：<br>F(0) &#x3D; 0,   F(1) &#x3D; 1<br>F(N) &#x3D; F(N - 1) + F(N - 2), 其中 N &gt; 1.<br>斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。<br>答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。</p></blockquote><ul><li>【题点】<ul><li>斐波那契——递归函数</li></ul></li><li>【思路】<ul><li>两种算法<ol><li>简单的正向计算</li><li>递归函数</li><li>直接查表</li></ol></li></ul></li><li>【思考】<ul><li>很多情况下正向计算即可，因为这样的时间复杂度较低</li></ul></li><li>【bug】<ul><li>注意返回值返回的是x，还是y</li></ul></li></ul><hr><h1 id="10-2-青蛙跳台阶问题"><a href="#10-2-青蛙跳台阶问题" class="headerlink" title="10.2 青蛙跳台阶问题"></a>10.2 青蛙跳台阶问题</h1><blockquote><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。<br>答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。</p></blockquote><ul><li><p>【思路】</p><ol><li>如果第一次跳的是1级台阶，那么剩下n-1级台阶，跳法是f(n-1)</li><li>如果第一次跳的是2级台阶，那么剩下n-2级台阶，跳法是f(n-2)</li><li>可以得出总跳法为: f(n) &#x3D; f(n-1) + f(n-2)</li><li>由题意可得：没有台阶的时候f(0) &#x3D; 1，只有一级台阶的时候 f(1) &#x3D; 1</li></ol></li><li><p>【思考】</p><ul><li>注意，为什么需要0级台阶？当x指向0级台阶，那么i级台阶和x就对应起来了</li><li>同时在题目的测试中也提示了f(0)&#x3D;1</li><li>递归问题首先考虑正向解</li></ul></li><li><p>【bug】</p></li></ul><hr><h1 id="11-旋转数组的最小数字"><a href="#11-旋转数组的最小数字" class="headerlink" title="11.旋转数组的最小数字"></a>11.旋转数组的最小数字</h1><blockquote><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。给你一个可能存在 重复 元素值的数组 numbers ，它原来是一个升序排列的数组，并按上述情形进行了一次旋转。请返回旋转数组的最小元素。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一次旋转，该数组的最小值为 1。  注意，数组 [a[0], a[1], a[2], …, a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], …, a[n-2]] 。</p></blockquote><ul><li>【题点】<ul><li>本质上是求取一个数组的最小值，为什么说这么多？——直接遍历整个数组，复杂度较高，本体考验的是二分法</li></ul></li><li>【思路】<ul><li>使用二分法分析左右两侧数值大小来确定最小值位置</li></ul></li><li>【思考】<ul><li>只旋转一次，而且原列表升序（左一定小于右）<ol><li>如果中间大于右侧，说明旋转侧在右边，即最小值也在右侧，但是&#x3D;&#x3D;left&#x3D;mid+1&#x3D;&#x3D;，为什么要加1？因为已经确定mid这个位置必定不是最小值</li><li>如果中间小于右侧，那么旋转侧在左边，那么最小值在左侧，此时&#x3D;&#x3D;right&#x3D;mid&#x3D;&#x3D;，此处没有-1，那是因为mid处的值有可能就是最小值，无法直接排除</li></ol></li></ul></li><li>【bug】<ul><li>如果没有+1，时间复杂度超出程序要求</li></ul></li></ul><hr><h1 id="12-矩阵中的路径——没有做，需要后续知识"><a href="#12-矩阵中的路径——没有做，需要后续知识" class="headerlink" title="12. 矩阵中的路径——没有做，需要后续知识"></a>12. 矩阵中的路径——没有做，需要后续知识</h1><blockquote><p>给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。<br>单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。</p></blockquote><ul><li>【题点】<ul><li>二维矩阵，涉及dfs深度优先搜索算法</li><li>回溯法，递归寻找目标，否则回退</li></ul></li><li>【思路】</li><li>【思考】<ul><li>所谓的回退是将所有路径都列举出来，选择符合条件的那条路，在矩阵中，加入递归首先是向左移动，那么便会一直左移（如果符合条件），相当于一直尝试</li></ul></li><li>【bug】</li></ul><hr><h1 id=""><a href="#" class="headerlink" title=""></a></h1><blockquote></blockquote><ul><li>【题点】</li><li>【思路】</li><li>【思考】</li><li>【bug】</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> interview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩的概念</title>
      <link href="/2022/08/02/%E7%9F%A9%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
      <url>/2022/08/02/%E7%9F%A9%E7%9A%84%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：不规则图形的中心、重心、质点如何求取？</span><br><span class="line">牵扯概念：</span><br><span class="line">矩</span><br><span class="line">    1. 物理上的矩——力矩</span><br><span class="line">    2. 数学上的矩——期望、方差、原点矩、中心矩</span><br></pre></td></tr></table></figure><h1 id="矩（Moment）"><a href="#矩（Moment）" class="headerlink" title="矩（Moment）"></a>矩（Moment）</h1><p>什么是矩？<br>矩是观察与描述随机变量的工具，不同矩就是不同的维度，即从不同的角度（维度）去观察随机变量分布情况。</p><ol><li>是计算在第k个维度，移动到原点&#x2F;中心的问题，也可以理解为偏离原点&#x2F;中心</li><li>偏离某一值的惩罚</li></ol><hr><h1 id="1-物理上的矩"><a href="#1-物理上的矩" class="headerlink" title="1. 物理上的矩"></a>1. 物理上的矩</h1><h2 id="1-1力矩"><a href="#1-1力矩" class="headerlink" title="1.1力矩"></a>1.1力矩</h2><p>输入变量（自变量）：力矩<br>力矩 &#x3D; 力臂 X 力</p><blockquote><p>例1：一个船舵，3个船员同向转动船舵，如何每个船员用了多少能量？<br>例2：一个船舵，3个船员中有一个反方向转动船舵，如何计算每个船员平均使用多少能量？</p></blockquote><p> $a_1 &#x3D; \frac{F_1L_1 + F_2L_2+F_3L_3}{3}$  &lt;1&gt;<br> $a_2 &#x3D; \frac{(F_1L_1)^2 + (-F_2L_2)^2+(F_3L_3)^2}{3}$  &lt;2&gt;</p><p>计算公式是没问题的，都是平均力矩，但是为什么第二个式子却需要加平方？——如果直接计算平均力矩，例2中会出现力矩被抵消，最终计算比实际小。</p><ul><li>例1可看为计算一阶原点矩——衡量力矩的平均水平</li><li>例2可看为计算二阶原点矩——衡量力矩的总能量</li></ul><h2 id="1-2中心矩和原点矩"><a href="#1-2中心矩和原点矩" class="headerlink" title="1.2中心矩和原点矩"></a>1.2中心矩和原点矩</h2><p>船舵问题中，力矩均指向船舵原点，上述问题计算的就是原点矩；当指向其他位置，可理解为“中心矩”。<br>但是中心是一个定点，针对中心的具体位置，在如下中解释说明。</p><table><thead><tr><th align="center">名称</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">一阶原点矩</td><td align="center">数据的平均水平&#x2F;程度</td></tr><tr><td align="center">二阶原点矩</td><td align="center">数据移动到平均位置的平均能量</td></tr><tr><td align="center">一阶中心矩</td><td align="center">0</td></tr><tr><td align="center">二阶中心矩</td><td align="center">数据离散&#x2F;集中程度，或数据偏离平均水平的程度</td></tr><tr><td align="center">三阶中心矩</td><td align="center">偏度——数据偏离平均位置的平均水平</td></tr><tr><td align="center">四阶中心矩</td><td align="center">峭度——数据偏离中心的密集程度</td></tr></tbody></table><hr><h1 id="2-数学上的矩——概率论中"><a href="#2-数学上的矩——概率论中" class="headerlink" title="2. 数学上的矩——概率论中"></a>2. 数学上的矩——概率论中</h1><h2 id="2-1矩的概念"><a href="#2-1矩的概念" class="headerlink" title="2.1矩的概念"></a>2.1矩的概念</h2><p>输入变量（自变量）：离散随机变量——离散数值</p><blockquote><p>定义：设X和Y是离散随机变量，c为常数，k为正整数，如果$E(|X-c|^k)$存在，则称$E(|X-c|^k)$为X关于点c的k阶矩。</p><ul><li>c&#x3D;0时，称为k阶原点矩；</li><li>c&#x3D;$E(x)$时，称为k阶中心距</li></ul></blockquote><table><thead><tr><th align="center">名称</th><th align="center">公式</th><th align="center"></th><th align="center"></th><th align="center">常用</th><th align="center">名称</th></tr></thead><tbody><tr><td align="center">k阶原点矩</td><td align="center">$E(X^k)$</td><td align="center"></td><td align="center"></td><td align="center">1阶原点矩</td><td align="center">期望$E(X)$</td></tr><tr><td align="center">k阶中心矩</td><td align="center">$E[(X-E(X))^k]$</td><td align="center"></td><td align="center"></td><td align="center">2阶中心矩</td><td align="center">方差$D(X)$</td></tr><tr><td align="center">k+l阶混合矩</td><td align="center">$E(X^kY^l)$</td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center">k+l阶混合中心矩</td><td align="center">$E[(X-E(X))^k(X-E(X))^l]$</td><td align="center"></td><td align="center"></td><td align="center">2阶混合中心矩</td><td align="center">协方差$Conv(X,Y)$</td></tr></tbody></table><h2 id="2-2期望"><a href="#2-2期望" class="headerlink" title="2.2期望"></a>2.2期望</h2><ul><li>期望，又称均值，是对将来某种情况发生的估计值</li><li>平均值，已经发生&#x2F;存在的事件&#x2F;数据的平均<h2 id="2-3理解说明"><a href="#2-3理解说明" class="headerlink" title="2.3理解说明"></a>2.3理解说明</h2></li><li>原点矩指向0</li><li>中心指向离散随机变量的期望</li></ul><hr><h1 id="图像矩"><a href="#图像矩" class="headerlink" title="图像矩"></a>图像矩</h1><p>在求取图像的矩时，通常对图像进行二值化，其前景为1，背景为0</p><table><thead><tr><th align="center">名称</th><th align="center">公式</th><th align="left">意义</th></tr></thead><tbody><tr><td align="center">零阶矩</td><td align="center">$\mathrm{M}<em>{00}&#x3D;\sum</em>{\mathrm{I}} \sum_{\mathrm{J}} \mathrm{V}(\mathrm{i}, \mathrm{j})$</td><td align="left">面积</td></tr><tr><td align="center">一阶矩</td><td align="center">$\mathrm{M}<em>{10}&#x3D;\sum</em>{\mathrm{I}} \sum_{\mathrm{J}} \mathrm{i} \cdot \mathrm{V}(\mathrm{i}, \mathrm{j})$</td><td align="left">横坐标和对应像素值的乘积</td></tr><tr><td align="center">一阶矩</td><td align="center">$\mathrm{M}<em>{01}&#x3D;\sum</em>{\mathrm{I}} \sum_{\mathrm{J}} \mathrm{j} \cdot \mathrm{V}(\mathrm{i}, \mathrm{j})$</td><td align="left">纵坐标和对应像素值的乘积</td></tr><tr><td align="center">一阶矩</td><td align="center">$\mathrm{x}<em>{\mathrm{c}}&#x3D;\frac{\mathrm{M}</em>{10}}{\mathrm{M}<em>{00}}, \mathrm{y}</em>{\mathrm{c}}&#x3D;\frac{\mathrm{M}<em>{01}}{\mathrm{M}</em>{00}}$</td><td align="left">随机图形的质心</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>图像金字塔1</title>
      <link href="/2022/06/24/paper/%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%941/"/>
      <url>/2022/06/24/paper/%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%941/</url>
      
        <content type="html"><![CDATA[<h1 id="Feature-Pyramid-Transformer"><a href="#Feature-Pyramid-Transformer" class="headerlink" title=" Feature Pyramid Transformer "></a><center> Feature Pyramid Transformer </center></h1><blockquote><p>图像在空间和尺度上存在丰富的视觉语义信息，融合不同尺度的特征，有助于获取全局信息，特别是目标检测任务中小目标检测性能差，本文任务江不同尺度的信息进行结合，可解决此问题</p></blockquote><ol><li>self-trans:进行自注意力特征提取，使用Mixture of Softmaxes代替原来的softmax函数</li><li>Grounding Transformer (GT)：<br>原文描述：将高层的语义概念与低层特征图的像素特征联系起来，注意此处只是连接起来<br>目的在于使用high-level的语义信息，增强低层特征<ul><li>代码具体操作：Q为低层特征，KV是高层特征，利用transformer建立两者的关系</li></ul></li><li>Rending Transformer (RT)：<br>原文描述：使用low-level像素特征来渲染higher-level的内容特征。<br>目的在于使用low-level的细粒度信息，增强高层的粗粒度语义信息<ul><li>代码中的具体操作：使用低层进行低维嵌入提取低层的信息直接应用在高层特征上，同时将低层下采样卷积成高层尺寸，两个相加</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解数据维度</title>
      <link href="/2022/06/08/work/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6/"/>
      <url>/2022/06/08/work/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE%E7%BB%B4%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="高维数据理解"><a href="#高维数据理解" class="headerlink" title="高维数据理解"></a>高维数据理解</h1><p>初级理解：数据维度的上升，遵循简单三维的排列</p><blockquote><ol><li>单个数据理解为点</li><li>一维数据理解为向量，或者直线</li><li>二维数据理解为平面，一副图像</li><li>三维数据理解为空间，从二维向垂直方向延申数据</li><li>四维数据理解：将三维数据沿水平方向延申</li><li>五维数据理解：将四维数据沿竖直方向延申</li><li>六维数据理解：将五维数据沿垂直方向延申</li></ol></blockquote><h1 id="高维数据维度交换"><a href="#高维数据维度交换" class="headerlink" title="高维数据维度交换"></a>高维数据维度交换</h1><p>数据维度：<code>[C, H, W]</code></p><blockquote><ol><li>将H维度提前<code>[H, C, W]</code>：数据上的直观反应为在原数据上提取图像的行重新排列</li><li>将C维度交换到最后<code>[H, W, C]</code>：可理解为两步操作（将H、W提前，即将C后退两次），第一次提取图像的行重新排列，第二次有两种理解（第一种理解以通道为基准，再提取列数据——由行列定位到单个像素数据，即提取同一位置的所有通道上的像素特征。第二种理解为C与W的转置，第一步提取了行数据，转置之后为某位置上的通道特征），但是需要注意的是每个二维矩阵代表的是每个通道上某行的排列，也就是说最终的结果第一个二维矩阵中的数据是原数据中所有通道的单个像素按照行进行排列</li></ol></blockquote><h1 id="维度交换的理解"><a href="#维度交换的理解" class="headerlink" title="维度交换的理解"></a>维度交换的理解</h1><ol><li>首先确定最后一个维度代表的是通道还是排列的像素值</li><li>三个维度之间的交换理解为提取行和提取列</li><li>确定第一个数据矩阵代表什么特征</li></ol><h1 id="计算时的维度理解"><a href="#计算时的维度理解" class="headerlink" title="计算时的维度理解"></a>计算时的维度理解</h1><ul><li><p>生成一个[m,n]的矩阵，如何理解？</p><ol><li>这个矩阵有m行，n列数据</li><li>这个矩阵m维，每维n个数据</li></ol></li><li><p>torch.max(x, dim&#x3D;0)如何理解这个维度？</p><ol><li>沿着第0维计算最大值</li><li>沿着行的方向计算最大值——行的方向即使列，行的方向；每个维度、对象、目标，而不是直接在行中计算</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN与transformer</title>
      <link href="/2022/05/13/work/CNN%E4%B8%8Etransformer/"/>
      <url>/2022/05/13/work/CNN%E4%B8%8Etransformer/</url>
      
        <content type="html"><![CDATA[<h1 id="直观理解CNN与Transformer"><a href="#直观理解CNN与Transformer" class="headerlink" title="直观理解CNN与Transformer"></a>直观理解CNN与Transformer</h1><ul><li>CNN：从局部开始，扩散获得全局视野（或理解为CNN逐个像素地识别图像，在这个过程中通过从局部到全局的方式识别角&#x2F;线等特征）</li><li>Transformer:从开始就能直接建立距离较远像素之间的关系<blockquote><p>CNN相当于从单个像素开始，用变焦镜头缩小远处物体的像的放大倍数<br>Transformer相当于将整个模糊图像聚焦</p></blockquote></li></ul><h1 id="blog"><a href="#blog" class="headerlink" title="blog"></a>blog</h1><p>想象一下你走进一家本地的五金店，在货架上看到一种新型的锤子。你听说过这种锤子：它比其他锤子敲得更快、更准确，而且在过去的几年里，在大多数用途中，它已经淘汰了许多其他锤子。</p><p>此外，通过一些调整，比如这里加一个附件，那里拧一个螺丝，这种锤子还能变成一把锯，其切割速度能媲美其他任何替代品。一些处于工具开发前沿的专家表示，这把锤子可能预示着所有工具将融合到一个设备中。</p><p>类似的故事正在人工智能领域上演。这种多功能的新锤子是一种人工神经网络——一种在现有数据上进行训练以「学习」如何完成某些任务的节点网络——称为 Transformer。它最初用于处理语言任务，但最近已经开始影响其他 AI 领域。</p><p>Transformer 最初出现在 2017 年的一篇论文中：《Attention Is All You Need》。在其他人工智能方法中，系统会首先关注输入数据的局部 patch，然后构建整体。例如，在语言模型中，邻近的单词首先会被组合在一起。相比之下，Transformer 运行程序以便输入数据中的每个元素都连接或关注其他元素。研究人员将此称为「自注意力」。这意味着一旦开始训练，Transformer 就可以看到整个数据集的迹。</p><p>在 Transformer 出现之前，人工智能在语言任务上的进展一直落后于其他领域的发展。「在过去 10 年发生的这场深度学习革命中，自然语言处理在某种程度上是后来者，」马萨诸塞大学洛厄尔分校的计算机科学家 Anna Rumshisky 说，「从某种意义上说，NLP 曾落后于计算机视觉，而 Transformer 改变了这一点。」</p><p>Transformer 很快成为专注于分析和预测文本的单词识别等应用程序的引领者。它引发了一波工具浪潮，比如 OpenAI 的 GPT-3 可以在数千亿个单词上进行训练并生成连贯的新文本。</p><p>Transformer 的成功促使人工智能领域的研究者思考：这个模型还能做些什么？</p><p>答卷正在徐徐展开——Transformer 被证明具有惊人的丰富功能。在某些视觉任务中，例如图像分类，使用 Transformer 的神经网络比不使用 Transformer 的神经网络更快、更准确。对于其他人工智能领域的新兴研究，例如一次处理多种输入或完成规划任务，Transformer 也可以处理得更多、更好。</p><p>「Transformer 似乎在机器学习领域的许多问题上具有相当大的变革性，包括计算机视觉，」在慕尼黑宝马公司从事与自动驾驶汽车计算机视觉工作的 Vladimir Haltakov 说。</p><p>就在十年前，AI 的不同子领域之间还几乎是互不相通的，但 Transformer 的到来表明了融合的可能性。「我认为 Transformer 之所以如此受欢迎，是因为它展示出了通用的潜力，」德克萨斯大学奥斯汀分校的计算机科学家 Atlas Wang 说：「我们有充分的理由尝试在整个 AI 任务范围内尝试使用 Transformer。」</p><p>Dosovitskiy 正在研究该领域最大的挑战之一，即在不增加处理时间的前提下，将 CNN 放大：在更大的数据集上训练，表示更高分辨率的图像。但随后他看到，Transformer 已经取代了以前几乎所有与语言相关的 AI 任务的首选工具。「我们显然从正在发生的事情中受到了启发，」他说，「我们想知道，是否可以在视觉上做类似的事情？」 这个想法某种程度上说得通——毕竟，如果 Transformer 可以处理大数据集的单词，为什么不能处理图片呢？</p><p>最终的结果是：在 2021 年 5 月的一次会议上，一个名为 Vision Transformer（ViT）的网络出现了。该模型的架构与 2017 年提出的第一个 Transformer 的架构几乎相同，只有微小的变化，这让它能够做到分析图像，而不只是文字。「语言往往是离散的，」Rumshisky 说：「所以必须使图像离散化。」</p><p>ViT 团队知道，语言的方法无法完全模仿，因为每个像素的自注意力在计算时间上会非常昂贵。所以，他们将较大的图像划分为正方形单元或 token。大小是任意的，因为 token 可以根据原始图像的分辨率变大或变小（默认为一条边 16 像素），但通过分组处理像素，并对每个像素应用自注意力，ViT 可以快速处理大型训练数据集，从而产生越来越准确的分类。</p><p>Transformer 能够以超过 90% 的准确率对图像进行分类，这比 Dosovitskiy 预期的结果要好得多，并在 ImageNet 图像数据集上实现了新的 SOTA Top-1 准确率。ViT 的成功表明，卷积可能不像研究人员认为的那样对计算机视觉至关重要。</p><p>与 Dosovitskiy 合作开发 ViT 的谷歌大脑苏黎世办公室的 Neil Houlsby 说：「我认为 CNN 很可能在中期被视觉 Transformer 或其衍生品所取代。」他认为，未来的模型可能是纯粹的 Transformer，或者是为现有模型增加自注意力的方法。</p><p>一些其他结果验证了这些预测。研究人员定期在 ImageNet 数据库上测试他们的图像分类模型，在 2022 年初，ViT 的更新版本仅次于将 CNN 与 Transformer 相结合的新方法。而此前长期的冠军——没有 Transformer 的 CNN，目前只能勉强进入前 10 名。</p><ol><li>Transformer的工作原理<br>Transformer 的力量来自于它处理图像编码数据的方式。「在 CNN 中，你是从非常局部的地方开始，然后慢慢获得全局视野，」Raghu 说。CNN 逐个像素地识别图像，通过从局部到全局的方式来识别角或线等特征。但是在带有自注意力的 Transformer 中，即使是信息处理的第一层也会在相距很远的图像位置之间建立联系（就像语言一样）。如果说 CNN 的方法就像从单个像素开始并用变焦镜头缩小远处物体的像的放大倍数，那么 Transformer 就是慢慢地将整个模糊图像聚焦。</li></ol><p>这种差异在 Transformer 最初专注的语言领域更容易理解，思考一下这些句子：「猫头鹰发现了一只松鼠。它试图用爪子抓住它，但只抓住了尾巴的末端。」第二句的结构令人困惑：「它」指的是什么？只关注「它」邻近的单词的 CNN 会遇到困难，但是将每个单词与其他单词连接起来的 Transformer 可以识别出猫头鹰在抓松鼠，而松鼠失去了部分尾巴。</p><p>显然，Transformer 处理图像的方式与卷积网络有着本质上的不同，研究人员变得更加兴奋。Transformer 在将数据从一维字符串（如句子）转换为二维数组（如图像）方面的多功能性表明，这样的模型可以处理许多其他类型的数据。例如，Wang 认为，Transformer 可能是朝着实现神经网络架构的融合迈出的一大步，从而产生了一种通用的计算机视觉方法——也许也适用于其他 AI 任务。「当然，要让它真正发生是有局限性的，但如果有一种可以通用的模型，让你可以将各种数据放在一台机器上，那肯定是非常棒的。」</p><ol start="2"><li>VIT展望<br>现在研究人员希望将 Transformer 应用于一项更艰巨的任务：创造新图像。GPT-3 等语言工具可以根据其训练数据生成新文本。在去年发表的一篇论文《TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up》中，Wang 组合了两个 Transformer 模型，试图对图像做同样的事情，但这是一个困难得多的问题。当双 Transformer 网络在超过 200000 个名人的人脸上进行训练时，它以中等分辨率合成了新的人脸图像。根据初始分数（一种评估神经网络生成的图像的标准方法），生成的名人面孔令人印象深刻，并且至少与 CNN 创建的名人一样令人信以为真。</li></ol><p>Wang 认为，Transformer 在生成图像方面的成功比 ViT 在图像分类方面的能力更令人惊讶。「生成模型需要综合能力，需要能够添加信息以使其看起来合理，」他说。与分类领域一样，Transformer 方法正在生成领域取代卷积网络。</p><p>Raghu 和 Wang 还看到了 Transformer 在多模态处理中的新用途。「以前做起来比较棘手，」Raghu 说，因为每种类型的数据都有自己的专门模型，方法之间是孤立的。但是 Transformer 提出了一种组合多个输入源的方法。</p><p>「有很多有趣的应用程序可以结合其中一些不同类型的数据和图像。」例如，多模态网络可能会为一个系统提供支持，让系统除了听一个人的声音外，还可以读取一个人的唇语。「你可以拥有丰富的语言和图像信息表征，」Raghu 说，「而且比以前更深入。」</p><p>新的一系列研究表明了 Transformer 在其他人工智能领域的一系列新用途，包括教机器人识别人体运动、训练机器识别语音中的情绪以及检测心电图中的压力水平。另一个带有 Transformer 组件的程序是 AlphaFold，它以快速预测蛋白质结构的能力，解决了五十年来蛋白质分子折叠问题，成为了名噪一时的头条新闻。</p><ol start="3"><li>transformer同时也不一定必须<br>即使 Transformer 有助于整合和改进 AI 工具，但和其他新兴技术一样，Transformer 也存在代价高昂的特点。一个 Transformer 模型需要在预训练阶段消耗大量的计算能力，才能击败之前的竞争对手。</li></ol><p>这可能是个问题。「人们对高分辨率的图像越来越感兴趣，」Wang 表示。训练费用可能是阻碍 Transformer 推广开来的一个不利因素。然而，Raghu 认为，训练障碍可以借助复杂的滤波器和其他工具来克服。</p><p>Wang 还指出，尽管视觉 transformer 已经在推动 AI 领域的进步，但许多新模型仍然包含了卷积的最佳部分。他说，这意味着未来的模型更有可能同时使用这两种模式，而不是完全放弃 CNN。</p><p>同时，这也表明，一些混合架构拥有诱人的前景，它们以一种当前研究者无法预测的方式利用 transformer 的优势。「也许我们不应该急于得出结论，认为 transformer 就是最完美的那个模型，」Wang 说。但越来越明显的是，transformer 至少会是 AI shop 里所有新型超级工具的一部分。</p><p><a href="https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/">原文</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instance Enhancement Batch Normalization:An Adaptive Regulator of Batch Noise</title>
      <link href="/2022/05/12/paper/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B62/"/>
      <url>/2022/05/12/paper/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B62/</url>
      
        <content type="html"><![CDATA[<h1 id="Instance-Enhancement-Batch-Normalization-An-Adaptive-Regulator-of-Batch-Noise"><a href="#Instance-Enhancement-Batch-Normalization-An-Adaptive-Regulator-of-Batch-Noise" class="headerlink" title=" Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise "></a><center> Instance Enhancement Batch Normalization: An Adaptive Regulator of Batch Noise </center></h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ul><li><p>BN层可通过输入的batch对数据进行归一化，但同时也引入了噪声，适当的噪声有益于模型的优化和泛化，但是BN层对于风格迁移这种任务会引入过多噪声</p></li><li><p>IEBN通过简单的线性变换重新校准通道间的信息，对数据起到了正则化并加固的网络的训练过程</p></li></ul><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><ul><li>对于SE-Block，是对特征层[H,W,C]进行最大池化或者平均池化，得到[1,1,C]，再通过FC进行线性变化处理</li><li>IEBN也是是对特征层[H,W,C]进行最大池化或者平均池化得到[1,1,C]，但对每个channel只设置两个系数做线性变换，参数量大大减少，同时性能良好</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perceiver:General Perception with Iterative Attention</title>
      <link href="/2022/05/11/paper/transformer1/"/>
      <url>/2022/05/11/paper/transformer1/</url>
      
        <content type="html"><![CDATA[<center> Perceiver: General Perception with Iterative Attention </center><h1 id="code"><a href="#code" class="headerlink" title="code"></a>code</h1><ul><li>傅里叶位置编码<ol><li>每个像素拥有一个轴向和纵向的傅里叶位置编码向量——其中向量维度为13，两个为26，加上输入通道则为29，即image上的一个像素点使用一个29维度的向量表示</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EdgeFormer</title>
      <link href="/2022/05/10/paper/Former/"/>
      <url>/2022/05/10/paper/Former/</url>
      
        <content type="html"><![CDATA[<center> EDGEFORMER: IMPROVING LIGHT-WEIGHT CONVNETS BY LEARNING FROM VISION TRANSFORMERS </center><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ul><li><p>存在问题：</p><ol><li>VIT优于大型卷积模型，提取全局信息，但存在计算复杂</li><li>卷积网络建立模型局部关系，存在硬件支持，小模型训练相对容易</li></ol></li><li><p>改进：</p><ul><li>将VIT柔和到卷积网络中</li></ul></li><li><p>模型结构，使用新提出全局循环卷积代替self-attention，再使用一个简单的channel wise attention实现通道注意力——使用空间、通道注意力代替ViT中的注意力结构</p></li></ul><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><ul><li>从图像的行与列来构造空间注意力相关模块<ol><li>GCC-V——位置编码pe[CxBx1]通过双线性插值后[CxHx1],再将其沿水平方向复制得到[CxHxW],加和到输入X上，将X沿V垂直方向堆叠（得到两块），使用一个[CxBx1]通过双线性插值后[CxHx1]的向量作为卷积核，卷积后得到[CxHxW]</li><li>GCC-H——位置编码pe[Cx1xB]通过双线性插值后[Cx1xW],再将其沿垂直方向复制得到[CxHxW]，加和到输入上，将X沿H水平方向堆叠（得到两块），使用一个[Cx1xB]通过双线性插值后[Cx1xW]的向量作为卷积核，卷积后得到[CxHxW]<blockquote><p>其中垂直方向则按照垂直方向堆叠，且卷积核也构造为垂直向量;水平方向按照水平方向堆叠，且卷积核构造为水平方向</p></blockquote></li></ol></li></ul><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ul><li>位置编码对于目标检测以及分割这种对位置敏感的任务十分重要</li><li>当前层的全局核覆盖的像素实际上只有一半来自源输入，其他均为零，因为卷积会改变特征尺寸（否则使用padding，常用padding就是0，这就是其他为零的原因）</li><li>token mixer用于在不同空间位置的令牌之间交换信息，channel mixer用于在不同通道之间混合信息。</li></ul><h1 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h1><p>动态改变位置编码、kernel来应对不同尺寸的输入</p><h1 id="感受"><a href="#感受" class="headerlink" title="感受"></a>感受</h1><p>换汤不换药，使用新的空间、通道注意力结构对transformer进行重建，结合卷积和VIT的优点，提高了模型的实现能力</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Beyond Self-attention:External Attention using Two Linear Layers for Visual Tasks</title>
      <link href="/2022/05/09/paper/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B61/"/>
      <url>/2022/05/09/paper/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B61/</url>
      
        <content type="html"><![CDATA[<h1 id="Beyond-Self-attention-External-Attention-using"><a href="#Beyond-Self-attention-External-Attention-using" class="headerlink" title=" Beyond Self-attention: External Attention using"></a><center> Beyond Self-attention: External Attention using</h1><p>Two Linear Layers for Visual Tasks </center></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>自注意力通过计算所有位置上成对亲和力进行特征加权来更新每个位置上的特征——这个特征能够捕捉单个样本中的远距离依赖关系，但其计算都复杂且忽略了不同样本之间的潜在关系。<br>本文设计了两个外部的、小的、可学习的、存储共享的结构，仅由两个级联线性层以及两个归一化层——只是线性复杂度，且考虑了的所有数据样本之间的相关性。</p><h1 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h1><ul><li>使用可学习矩阵作为整个数据集的先验知识memory<ol><li>使用可学习矩阵$ M_k $作为整个训练中的key</li><li>使用可学习矩阵$ M_v $作为整个训练中的value</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JOJO</title>
      <link href="/2022/05/08/life/JOJO/"/>
      <url>/2022/05/08/life/JOJO/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="46620823b00ff4d2d273340504405608e4955a57827d0d78802c75c8d5b09f64">d6183a03306b1dea4b48207219cc7eefef1b01d79e8c1be979c192e16a1e9484cb883b8478251a8586ec5d9548e2765f1648484d7e3f1d7a34895fc0900ecd797c9a2726c349ef68d51c8f7bb5901975e3ea14d8d0198cbe2509715950a6eee5afcc9552cdb45178a05735a06ee11069c53df22075c3638d8db1c55b56e28eb7ed559ee18aaf28da840673f05bab0470f7444bfc9363eebe570b49a7a85e2e7cc3c49df36128ffe2bd9b8244fb713ad74f8c5bb7fa22aacbcca6e9051e3e861527d5e7cad2d18e3d82298f350e24ba2aff69d4b36de18cb050132b6cb924ff14c8627d07d492f5160aa1bc5daf5993b2e7427e9163c7cdc89b7682ef34557cbb65c230aa54ff7c2ef40159f5579dd980ac8f87f2a2d692a7602c007534c34c73c8d1ab4324481df6ceceb1cde647effbcd1eb6224e7c587929d9184de84d8cf4ba5e96e62cec38a1fa8c19ee1c590d66612059a7ff7456c7405245edfc2f00fb609eb5d517d4a3d984f393e7a65d4ed8aca632423e2ecf45e7a3217a35670389ada48935e6b681ee33943aa7bb31eb649dccfc5809ad9086f4aecc54c41d8d3a1a63cb1cbc190ebdec80d95062801ceabd45a8b455dac371d22bd0947dfe9488d9b3996b870a67a2cf952dd523df22941bc7d22a99991a42949f2eb42a71491c54eb20b915922d99aef2bed22974aa0eef0181bb7ecaa6d2f1bb9297536141364f0eeeac727514081b66aee384573da31c82453e59d565ae27c1f4e1360719f8b686832691fa262d3d85e6fac530a36209ac2c96b587b96b2797e6dbb2147761</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DeepFusion:Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</title>
      <link href="/2022/05/06/paper/%E5%A4%9A%E6%A8%A1%E6%80%811/"/>
      <url>/2022/05/06/paper/%E5%A4%9A%E6%A8%A1%E6%80%811/</url>
      
        <content type="html"><![CDATA[<h1 id="DeepFusion-Lidar-Camera-Deep-Fusion-for-Multi-Modal-3D-Object-Detection"><a href="#DeepFusion-Lidar-Camera-Deep-Fusion-for-Multi-Modal-3D-Object-Detection" class="headerlink" title=" DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection "></a><center> DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection </center></h1><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>多模态融合问题中的关键挑战是如何有效对齐不同模态的深度特征</p><ol><li>InverseAug（逆几何相关增强）——进行精确的几何对齐</li><li>LearnableAlign——使用交叉注意力动态捕捉图像和激光雷达的相关性</li></ol><h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><blockquote><p>previous work</p><ol><li>将相机图像通过分割网络per-pixel分割得到相机特征</li><li>将相机特征与原始激光雷达点cat进行特征提取</li><li>进行目标检测</li></ol><p>ours</p><ol><li>不直接将相机特征与激光雷达cat在一起，将其从透视图转换为鸟瞰图</li><li>使用端到端训练，不再先分割再检测</li></ol><p>思路<br>提取各自特征-&gt;融合-&gt;backbone-&gt;head</p></blockquote><h1 id="涉及问题"><a href="#涉及问题" class="headerlink" title="涉及问题"></a>涉及问题</h1><ol><li>不同模态融合问题<br>相机图像是2D信息，激光雷达是3D信息，如使直接使用点云作为3D检测架构，就需要将相机特征与原始点云一起进行cat处理，但是相机特征与点云明显不是一类特征</li></ol><h1 id="InverseAug"><a href="#InverseAug" class="headerlink" title="InverseAug"></a>InverseAug</h1><p>使用逆几何对齐原因，数据增强中会使用翻转、旋转，原本3D和2D是对齐的，但增强后就无法对齐，为了保持3D与2D的对齐,使用参数将增强后的数据反转会原始坐标，这样就可一找到对应2D坐标</p><h1 id="LearnableAlign"><a href="#LearnableAlign" class="headerlink" title="LearnableAlign"></a>LearnableAlign</h1><p>将input直接作为特征进行融合那么多模态是一对一映射，但是激光雷达中层特征与多个摄像头的中层特征形成了一对多映射，简单方法是均值，但是均值会导致信息损失</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络模型架构</title>
      <link href="/2022/05/05/work/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"/>
      <url>/2022/05/05/work/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="Stage重复次数"><a href="#Stage重复次数" class="headerlink" title="Stage重复次数"></a>Stage重复次数</h1><blockquote><p>重复比例大概保持[1:1:2:1]</p></blockquote><ul><li>Resnet50:[3, 4, 6, 3]</li></ul><h1 id="残差结构"><a href="#残差结构" class="headerlink" title="残差结构"></a>残差结构</h1><blockquote><p>在主残差中加入组卷积</p></blockquote><h1 id="MLP结构"><a href="#MLP结构" class="headerlink" title="MLP结构"></a>MLP结构</h1><blockquote><p>在类似于MLP这种结构中将其调整成两头细，中间粗的结构</p></blockquote><ul><li>[96, 384, 96]</li></ul><h1 id="每个Stage设计"><a href="#每个Stage设计" class="headerlink" title="每个Stage设计"></a>每个Stage设计</h1><blockquote><p>每个卷积后面跟随一个1x1卷积，将通道加深再恢复，注意第一个尺寸上的卷积使用depthwise卷积<br>|7 x 7, 96 |<br>|1 x 1, 384|<br>|1 x 1, 96 |</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多传感器融合任务</title>
      <link href="/2022/05/03/work/%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E4%BB%BB%E5%8A%A1/"/>
      <url>/2022/05/03/work/%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="网络模型一"><a href="#网络模型一" class="headerlink" title="网络模型一"></a>网络模型一</h1><blockquote><p>选择一个backbone，多个head进行训练（其中head预测颜色、纹理等等）</p></blockquote><p>存在问题：</p><ul><li>多个head说明损失计算时会有多个反向传播，但是每个head倾向不同的任务，其更新权重时可能存在不同任务之间参数的互相破坏——即一个任务效果还行，但是多个任务训练的效果就很差</li></ul><h1 id="网络模型二"><a href="#网络模型二" class="headerlink" title="网络模型二"></a>网络模型二</h1><blockquote><p>针对模型一出现的问题：防止不同任务之间参数在更新时相互破坏</p></blockquote><ul><li>使用注意力机制，对backbone特定的层进行软注意掩膜，即学习不同任务的注意力，训练一个特征选择器，选择性的提取backbone某层中的特定的特征。</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DETR深度理解</title>
      <link href="/2022/04/28/paper/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
      <url>/2022/04/28/paper/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="End-to-End-Object-Detection-with-Transformers"><a href="#End-to-End-Object-Detection-with-Transformers" class="headerlink" title=" End-to-End Object Detection with Transformers "></a><center> End-to-End Object Detection with Transformers </center></h1><blockquote><ol><li>因为预测是模型预测，而标记是人为标记，得到的预测第一个不一定就直接与认为标记好的目标直接对应，如何将预测到的[class, box]和GT对应？</li></ol></blockquote><p>首先用二分图的方法进行信息匹配，匹配预测和标记之间相似程度，每一个预测目标只能与一个GT对应。对应好后进行损失计算</p><blockquote><ol start="2"><li>decoder输入的object queries如何理解？</li></ol></blockquote><ol><li>可理解成起控制decoder输出各式的作用，因为其本来就是随机初始化，自学习的token</li><li>本文经过可视化后，可以理解为每个token以不同的角度去分析encoder编码后的token信息</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像修改检测</title>
      <link href="/2022/04/28/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E6%94%B9%E6%A3%80%E6%B5%8B/"/>
      <url>/2022/04/28/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E6%94%B9%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="ObjectFormer-for-Image-Manipulation-Detection-and-Localization"><a href="#ObjectFormer-for-Image-Manipulation-Detection-and-Localization" class="headerlink" title=" ObjectFormer for Image Manipulation Detection and Localization "></a><center> ObjectFormer for Image Manipulation Detection and Localization </center></h1><h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>现有的图像篡改检测方法通常使用CNN (卷积神经网络) 提取图像内的篡改信息，它们往往关注图像层面（高层）或者像素层面（底层）的视觉一致信息，而没有明确地对物体层面（中层）的表示进行建模。相比之下，我们认为，图像操纵检测不仅要检查某些像素是否具有异常，还要考虑物体之间是否一致。</p><h1 id="技术基础"><a href="#技术基础" class="headerlink" title="技术基础"></a>技术基础</h1><ol><li>transformer能探索不同patch之间的相关性——用于明确地建模物体层面地视觉一致性信息。</li><li>使用可学习的参数作为query，用于发现对象一致性作为中层表征，这能够表征两个特征中特定区域地一致性，通过这种一致性来改进patch-level的一致性重建。——使用transformer应用在目标检测中的方法，使用object query学习并控制输出格式，以这种对象层面的信息辅助发现图像修改</li><li>BCIM模块考虑到全局检测是粗粒度的，当引入对象层面的检测后，再进行对象层面的细粒度建模，使用一个窗口控制局部区域的大小，计算这个区域中token的相似度，保持每个位置和周围区域的一致性。</li><li>交叉注意力，将混合token分别作为query或者kv计算注意力矩阵进行权重加和，这样能在本文任务中保证被篡改的物体层面的一致性信息。</li></ol><h1 id="问题解决思路"><a href="#问题解决思路" class="headerlink" title="问题解决思路"></a>问题解决思路</h1><blockquote><p>目标：判断一张图片是否被修改，并指出修改目标<br>本文思路：表征出图像中信息的一致性特征</p></blockquote><ol><li>Object Encoder：使用一个learnbale的query学习修改图片与其高频图像的一致性特征——将其作为<mark>中层一致性特征</mark>。（学习多模态向量得到一个相似性矩阵，可理解为关于信息一致性的中层特征，也可以理解为对原图和其高频图像之间的依赖关系建模，这个Q只是为了约束输出的格式）</li><li>Patch Decoder：中层一致性信息还不够，再使用较差注意力加强一致性信息</li><li>仅从一致性上创新度不够，考虑到修改存在尺寸上、颜色像素等层面的差异（不是类似于直接平移再粘贴，因为直接粘贴那么向量不变），此时再加入局部窗口内的特征信息相似性——也就是所谓的细粒度。</li></ol><h1 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h1><p>从计算图像中的一致性信息入手来区分图片是否被修改，然而transformer正好用来计算图像的一致性信息</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像的高低频</title>
      <link href="/2022/04/28/work/%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E4%BD%8E%E9%A2%91/"/>
      <url>/2022/04/28/work/%E5%9B%BE%E5%83%8F%E7%9A%84%E9%AB%98%E4%BD%8E%E9%A2%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>不同频率信息在图像结构中有不同的作用。图像的主要成分是低频信息，它形成了图像的基本灰度等级，对图像结构的决定作用较小；中频信息决定了图像的基本结构，形成了图像的主要边缘结构；高频信息形成了图像的边缘和细节，是在中频信息上对图像内容的进一步强化。</p></blockquote><p>图像的频率：灰度值变化剧烈程度的指标，是灰度在平面空间上的梯度。</p><h1 id="什么是低频"><a href="#什么是低频" class="headerlink" title="什么是低频?"></a>什么是低频?</h1><p>低频就是颜色缓慢地变化,也就是灰度缓慢地变化,就代表着那是连续渐变的一块区域,这部分就是低频. 对于一幅图像来说，除去高频的就是低频了，也就是边缘以内的内容为低频，而边缘内的内容就是图像的大部分信息，即图像的大致概貌和轮廓，是图像的近似信息。</p><h1 id="什么是高频"><a href="#什么是高频" class="headerlink" title="什么是高频?"></a>什么是高频?</h1><p>反过来, 高频就是频率变化快.图像中什么时候灰度变化快?就是相邻区域之间灰度相差很大,这就是变化得快.图像中,一个影像与背景的边缘部位,通常会有明显的差别,也就是说变化那条边线那里,灰度变化很快,也即是变化频率高的部位.因此，图像边缘的灰度值变化快，就对应着频率高，即高频显示图像边缘。图像的细节处也是属于灰度值急剧变化的区域，正是因为灰度值的急剧变化，才会出现细节。</p><p>另外噪声（即噪点）也是这样,在一个像素所在的位置,之所以是噪点,就是因为它与正常的点颜色不一样了，也就是说该像素点灰度值明显不一样了,,也就是灰度有快速地变化了,所以是高频部分，因此有噪声在高频这么一说。</p><p>其实归根到底,是因为我们人眼识别物体就是这样的.假如你穿一个红衣服在红色背景布前拍照,你能很好地识别么?不能,因为衣服与背景融为一体了,没有变化,所以看不出来,除非有灯光从某解度照在人物身上,这样边缘处会出现高亮和阴影,这样我们就能看到一些轮廓线,这些线就是颜色（即灰度）很不一样的地方.</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>self-attention意义分析</title>
      <link href="/2022/04/27/work/self-attention%E6%84%8F%E4%B9%89%E5%88%86%E6%9E%90/"/>
      <url>/2022/04/27/work/self-attention%E6%84%8F%E4%B9%89%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Transformer-中的-Attention机制"><a href="#Transformer-中的-Attention机制" class="headerlink" title=" Transformer 中的 Attention机制 "></a><center> Transformer 中的 Attention机制 </center></h1><p>Attention Function函数如下：<br>$$ \operatorname{Attention}(Q, K, V)&#x3D;\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V$$</p><h1 id="分母上scale的原因"><a href="#分母上scale的原因" class="headerlink" title="分母上scale的原因"></a>分母上scale的原因</h1><p>对于较大的$ d_k $来说在完成$ QK^k $后将会得到很大的值，而这将导致在经过sofrmax操作后产生非常小的梯度，不利于网络的训练</p><h1 id="使用softmax的原因"><a href="#使用softmax的原因" class="headerlink" title="使用softmax的原因"></a>使用softmax的原因</h1><ol><li>首先以softmax函数本身来说，它能将输出的值按照数值的大小占总数值的比率计算，最后表示成概率</li><li>在计算相似度时，会得到负数——这说明两个向量反向，使用softmax后，把原数值中的负数按照概率显示，这样做消除了方向的影响。即如果不做softmax，方向也会成为一个影响因素，原值为负数时，乘需要调整的矩阵会出现额外的问题</li><li>使用softmax还有归一化的作用，</li></ol><h1 id="使用多头的原因"><a href="#使用多头的原因" class="headerlink" title="使用多头的原因"></a>使用多头的原因</h1><p>每次计算完attention-map之后，map上对角线上的分数相对较高——相同的向量相似度肯定高，即过度的将注意力集中于自身的位置。<br>分成多头之后可理解为映射到不同的子空间，使用多个视角去关注像素之间的关系（在单头中对角线上的分数高，但是到了子空间中，其每个头关注的map就不是沿着对角线分数较高了）</p><h1 id="self-attention后面加上MLP的原因？"><a href="#self-attention后面加上MLP的原因？" class="headerlink" title="self-attention后面加上MLP的原因？"></a>self-attention后面加上MLP的原因？</h1><p>经过计算注意力图之后，还并不能称为注意力机制，因为得到的图是相对于单个channel来说的，只有经过MLP之后才能进行通道上的信息交互，同时某些时候通道会存在像素，位置信息，经过注意力图之后像素和位置信息并没有进行交互。</p><h1 id="attention机制中使用add-amp-Norm的原因"><a href="#attention机制中使用add-amp-Norm的原因" class="headerlink" title="attention机制中使用add &amp; Norm的原因"></a>attention机制中使用add &amp; Norm的原因</h1><ol><li>在使用softmax之前需要注意，要进行layernorm缩小数值之间的差距，因为假设k1和k2为两个向量且比较相关，但是由于k1和k2数值上比较大，计算相似度出出来的向量模长也很大，最后的相似度也很大，但相似度很大说明不相关，此时就需要提前进行归一化。</li><li>由于计算的核心思想是使用QK的相似度调整V，那么会不会出现一种问题：网络随便训练QK，而着重于V（答案）的优化？当然不会，因为有一个add的加入，即使用skip connect将Q加入到最后的输出进行统一。</li></ol><h1 id="1-QK-T-是计算相似度没错，但是为什么这样相乘能计算相似度？由此引出以下问题："><a href="#1-QK-T-是计算相似度没错，但是为什么这样相乘能计算相似度？由此引出以下问题：" class="headerlink" title="1. $ QK^{T} $是计算相似度没错，但是为什么这样相乘能计算相似度？由此引出以下问题："></a>1. $ QK^{T} $是计算相似度没错，但是为什么这样相乘能计算相似度？由此引出以下问题：</h1><blockquote><ul><li>向量的内积是什么？如何计算，几何意义是什么？</li></ul></blockquote><p>向量内积表征两个向量的夹角，表征一个向量在另一个向量上的投影，而这个投影值大，意味着两个向量的相关度高；如果两个向量的夹角是90°，那么这两个向量线性无关。</p><blockquote><ul><li>矩阵与自身转置相乘$ XX^{T} $，代表什么意义？</li></ul></blockquote><p><strong>计算上理解</strong>：第一行与其它行内积、第二行与其它行内积…<br><strong>意义上理解</strong>：计算每行与其他行的相似度，相当于遍历每行与其他行的相似度，得到的是一个相似度矩阵，其第一行第i个元素对应着原矩阵第一行与第i行相似度。<br><strong>应用上理解</strong>：当图片切成patch后，矩阵中的每一行代表每个patch中的所有像素值，计算时相当于计算每个patch与其他patch之间的相似度</p><blockquote><ul><li>相似度矩阵再乘以原矩阵如何理解？</li></ul></blockquote><p><strong>计算上理解</strong>：相似度矩阵第一行是原矩阵第一行与其它行的相似度，原矩阵第一列是每行的第一个数值，全部乘积完后的第一行，相当于以第一行为基准，用第一行和其它行相似度对应着调整其它行的占比。<br><strong>应用上理解</strong>：用第一个patch与其他patch的相似度调整原矩阵中的第一行（第一个patch），而调整后的第一行是以相似度为基准，对所有patch的加权。</p><h1 id="2-关于Q、K、V维度上的理解"><a href="#2-关于Q、K、V维度上的理解" class="headerlink" title="2.关于Q、K、V维度上的理解"></a>2.关于Q、K、V维度上的理解</h1><p>为了保证矩阵维度上的正确，前提条件：</p><ol><li><p>Q和K中向量的dim&#x2F;深度相同</p></li><li><p>K和V中向量的个数&#x2F;行相同</p><blockquote><ul><li>当Q、K、V自定义为可学习参数时：</li></ul></blockquote></li><li><p>可以将Q理解为输入（n个苹果），K可以理解为模板（m种水果），经过QK相乘之后得到的是一个n行m列的score注意力分数矩阵，作为被调整的矩阵V，V的行必须与K的个数（可理解为行，或m的个数）保持一致。</p></li></ol><blockquote><ul><li>当Q、K、V全都来自输入X时：</li></ul></blockquote><ol><li>例如计算图像的注意力机制相当于计算当前像素与所有像素的相似度并对原图像进行调整。调整时可理解为以K（水果模板）种类作为参考，重新审视或分类Q（苹果）</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>张量的广播性质</title>
      <link href="/2022/04/26/work/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E6%80%A7%E8%B4%A8/"/>
      <url>/2022/04/26/work/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B9%BF%E6%92%AD%E6%80%A7%E8%B4%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="维度解读"><a href="#维度解读" class="headerlink" title="维度解读"></a>维度解读</h1><ul><li>[3, 4, 5]<blockquote><p>该张量包含3个二维张量，每个二维张量都包含4个一维张量，每个一维张量都包含5个标量&#x2F;零维张量</p></blockquote><h1 id="相同维度但不同形状"><a href="#相同维度但不同形状" class="headerlink" title="相同维度但不同形状"></a>相同维度但不同形状</h1><h2 id="1、某个维度的形状不同且其中有一个维度为1"><a href="#1、某个维度的形状不同且其中有一个维度为1" class="headerlink" title="1、某个维度的形状不同且其中有一个维度为1"></a>1、某个维度的形状不同且其中<mark>有一个维度为1</mark></h2><blockquote><p>如形状为[1,4]和[3,4]的两个张量相加相当于分别将a与b的三个一维张量相加（实际上广播的操作是将a的维度扩展，复制其二维数据成三份，此时其尺寸和b相同）</p></blockquote></li></ul><ol><li>[1,4]和[3,4]、[4,1]和[4,3]</li><li>[1,4]和[3,1]也可以相加，相加前相当于分别扩展为[3,4]进行加和操作</li><li>[2,3]和[3,2,1]</li></ol><ul><li>注意：形状不相同的维度只要某个维度为1就能广播，单纯的形状不同且其中没有任何一个维度为1则不能广播</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers</title>
      <link href="/2022/04/25/paper/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B21/"/>
      <url>/2022/04/25/paper/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B21/</url>
      
        <content type="html"><![CDATA[<h1 id="Learning-Affinity-from-Attention-End-to-End-Weakly-Supervised-Semantic-Segmentation-with-Transformers"><a href="#Learning-Affinity-from-Attention-End-to-End-Weakly-Supervised-Semantic-Segmentation-with-Transformers" class="headerlink" title=" Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers "></a><center> Learning Affinity from Attention:End-to-End Weakly-Supervised Semantic Segmentation with Transformers </center></h1><blockquote><p>将transformer引入图像分割，使用MHSA得到的激活图作为预处伪标签，其作为标签还需要细化预伪标签。</p><ol><li>AFA模块来学习transformer中MHSA中的语义亲和力</li><li>自适应细化模块PAR用来有效地控制AFA生成亲和力标签的可靠性并确保伪标签的局部一致性</li></ol></blockquote><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><ol><li>语义分割通常需要大量标记labels，故提出使用弱&#x2F;廉价标签，常用的方法是：首先使用一个分类模型生成类激活图作为伪标签，再经过细化后才能被用来作为语义分割网络的训练标签——这意味着需要训练多个模型</li><li>AFA能学习MHSA中的语义亲和力用于改进初始伪标签，但为了确保这种改进的可靠和局部一致性，提出自适应细化模块</li></ol><blockquote><p>模型思想：</p><ol><li>Transformer作为backbone，输入图像</li><li>分支一：提取由transformer得到的语义亲和力信息，通过MLP进行语义亲和力预测得到亲和力特征信息A，</li><li>分支二：对transformer提取到的特征图直接进行分类层，其一将其通过类激活得到初始伪标签B，该标签再由PAR细化处理并得到亲和力特征信息C，[A,C]计算亲和力信息损失；其二经由类别预测直接输出类别损失</li><li>分支三：将由transformer得到的特征图经由decoder重建得到分割特征的预测信息D,将[A, B]进行Random wolk后再PAR细化得到分割模型能用来训练的最终伪标签E，计算[D,E]的预测损失</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimMIM：a Simple Framework for Masked Image Modeling</title>
      <link href="/2022/04/23/paper/%E5%9B%BE%E5%83%8F%E6%8E%A9%E8%86%9C/"/>
      <url>/2022/04/23/paper/%E5%9B%BE%E5%83%8F%E6%8E%A9%E8%86%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="SimMIM-a-Simple-Framework-for-Masked-Image-Modeling"><a href="#SimMIM-a-Simple-Framework-for-Masked-Image-Modeling" class="headerlink" title=" SimMIM:a Simple Framework for Masked Image Modeling "></a><center> SimMIM:a Simple Framework for Masked Image Modeling </center></h1><blockquote><p>本文模型不需要特殊设计，使用简单的离散VAE或聚类实现masking和Tokenization就行<br>探究图像掩膜模型表征好的原因:</p><ol><li>使用中等大小（32）的掩膜对图像随机掩码</li><li>直接对原始像素进行回归预测</li><li>预测头架构简单，且性能不比复杂层差</li></ol></blockquote><h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p>[1]——使用附加网络进行patch tokenization，通过掩膜来破坏短距离连接</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>1.1 掩码的自监督学习范式广泛应用于NLP领域却在计算机视觉表现的不温不火的原因：</p><ol><li>图像具有更强的局部关系，即相互靠近的像素往往是高度相关的。复制靠近的像素可以很好地完成任务，显然通过语义推理并不容易办到。</li><li>视觉信号是原始的、低层次的，而文本分词是由人类产生的高级概念。那么，对低层次信号的预测是否对高层次的视觉识别任务有用？ </li><li>视觉信号是连续的，而文本分词是离散的。那么，如何利用基于分类的掩码语言建模方法处理连续的视觉信号？</li></ol><p>【注意】：掩膜大小为32在很宽的掩膜率范围内性能都能表现的富有竞争力，但是掩膜大小为8就需要掩膜率高达80%才表现良好</p><p>模型共有四个部分：masking strategy——encoder architecture——prediction head</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformer的意义在哪里</title>
      <link href="/2022/04/21/work/transformer%E7%9A%84%E6%84%8F%E4%B9%89%E5%9C%A8%E5%93%AA%E9%87%8C/"/>
      <url>/2022/04/21/work/transformer%E7%9A%84%E6%84%8F%E4%B9%89%E5%9C%A8%E5%93%AA%E9%87%8C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>1.transformer是如何出现的<br>2.transformer如何使用？</p></blockquote><h1 id="transformer的背景"><a href="#transformer的背景" class="headerlink" title="transformer的背景"></a>transformer的背景</h1><ul><li>transformer基于self-attention<ul><li>self-attention比CNN看的更宽更远(感受野大，同时是全局远距离关联)</li><li>self-attention比lstm训练更快</li></ul></li></ul><ol><li>卷积具有位移不变性和局部性，卷积核是针对某个像素及其周围8个像素块之间的关系，它的归纳偏置可带来更好的可迁移性和泛化能力。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>卷积</title>
      <link href="/2022/04/19/work/%E5%8D%B7%E7%A7%AF/"/>
      <url>/2022/04/19/work/%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="转置卷积Transposed-Convolution"><a href="#转置卷积Transposed-Convolution" class="headerlink" title="转置卷积Transposed Convolution"></a>转置卷积Transposed Convolution</h1><blockquote><p>用来对图像进行上采样，扩大图像尺寸</p></blockquote><h2 id="运算步骤："><a href="#运算步骤：" class="headerlink" title="运算步骤："></a>运算步骤：</h2><ol><li>在输入特征图元素间填充s-1行、列0</li><li>在输入特征图四周填充k-p-1行、列0</li><li>将卷积核参数上下、左右翻转</li><li>做正常卷积运算（填充0，步距1）</li></ol><h1 id="膨胀卷积Dilated-convolution"><a href="#膨胀卷积Dilated-convolution" class="headerlink" title="膨胀卷积Dilated convolution"></a>膨胀卷积Dilated convolution</h1><blockquote><p>保证图像尺寸不变，但感受野扩大的卷积</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数以零为中心的意义</title>
      <link href="/2022/04/19/work/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%A5%E9%9B%B6%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E6%84%8F%E4%B9%89/"/>
      <url>/2022/04/19/work/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%A5%E9%9B%B6%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在很多介绍激活函数的文章中看到一条：该函数不以零为中心。并将其列为该激活函数的缺点</p></blockquote><p>要说明激活函数以零为中心的意义，需要做以下证明：</p><ul><li>某次反向传播的方向实则仅与输入的值相关。</li></ul><h1 id="证明："><a href="#证明：" class="headerlink" title="证明："></a>证明：</h1><h2 id="1-神经元单元中输入输出步骤说明"><a href="#1-神经元单元中输入输出步骤说明" class="headerlink" title="1. 神经元单元中输入输出步骤说明"></a>1. 神经元单元中输入输出步骤说明</h2><p>神经网络完全仿照人类大脑的神经元，在某个神经元 $ i $ ：上个神经元的输出作为本次输入 $ X_i $ ，经过神经元 $ i $ 整合之后： $ z(\vec{x};\vec{w},b) &#x3D; \sum_i w_ix_i + b $ ,再通过激活 $ f(z) $得到输出。 </p><h2 id="2-sigmoid和tanh"><a href="#2-sigmoid和tanh" class="headerlink" title="2. sigmoid和tanh"></a>2. sigmoid和tanh</h2><p>本文为说明激活函数以零为中心的意义，选用sigmoid和tanh对比说明</p><ul><li>区别</li></ul><ol><li>sigmoid区间为[0,1]，但tanh以零为中心</li></ol><ul><li>相同（在本文中无关，但是当时困扰我很多）</li></ul><ol><li>其导数均大于0</li></ol><h2 id="3-收敛速度"><a href="#3-收敛速度" class="headerlink" title="3. 收敛速度"></a>3. 收敛速度</h2><p>为得到最优解，迭代轮数多即收敛速度慢；反之</p><h2 id="4-深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。"><a href="#4-深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。" class="headerlink" title="4. 深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。"></a>4. 深度学习的基本思路是靠反向传播，即通过链式法则将梯度回传并更新参数，重点就是参数更新。</h2><p>此处为公式，后期补充</p><h2 id="5-通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关"><a href="#5-通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关" class="headerlink" title="5. 通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关"></a>5. 通过链式法则证明梯度在反向传播算法，在某次反向传播中，对于所有的输入其梯度均是固定的，即意味着本次更新方向仅与输入值（上一个神经元的输出）有关</h2><h2 id="6-以零为中心的实际影响"><a href="#6-以零为中心的实际影响" class="headerlink" title="6. 以零为中心的实际影响"></a>6. 以零为中心的实际影响</h2><p>如果输入值像sigmoid一样为正数，那么某次反向传播中整体只能向某个方向变化，现在假设：</p><p>即我们希望 $ w_1 $ 适当增大， $ w_2 $ 适当减小，这时就出现一个问题，像上述一样，在某个神经元中梯度实际上是确定的，整体上数值的方向仅取决于输入值的方向，一旦输入值方向都相同（因为输入值是上个神经元的输出，而这个输出的最后一步通过了激活函数）——sigmoid，那么此次更新只能整体朝一个方向更新，那么就无法做到增大 $ w_1 $ 的同时减小 $ w_2 $ ,因为这样需要输入值可正可负（但sigmoid只大于零，这里引出一个问题，下节说明）<br>使用sigmoid这样不以零为中心的激活函数（只有正值或负值），神经网络在某次更新中无法同时满足所有的权重向最优解移动（即方向不是直指最优解），而是在最优解那个方向上左右摇动，这样导致收敛速度缓慢。</p><h2 id="7-当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？"><a href="#7-当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？" class="headerlink" title="7. 当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？"></a>7. 当输入值确定为sigmoid输出，即为正数，那么全都向正向了，还怎么保证收敛？</h2><p>所谓的方向全由输入 $ X_i $ 决定，前提是将其定义在某个神经元单元中考虑输入 $ X_i $ 对方向影响的问题来说，因为本文就是说明激活函数以零为中心的意义，刚好神经元的输入就是上层神经元输出，最重要的是该输出的最后一步是通过了激活函数，我们要用的就是经过激活函数的输出。</p><p>但是脱离了这个限制，实则某次反向传播的最终方向是由下方公式共同决定的，这说明了其中$\frac{\partial L}{\partial f}\frac{\partial f}{\partial z}$是可正可负的，它在整个更新的过程中是起作用的。<br>$$\left{\begin{array}{l}<br>\boldsymbol{x}<em>{i} \cdot \frac{\partial L}{\partial f} \frac{\partial f}{\partial z} \<br>\boldsymbol{x}</em>{j} \cdot \frac{\partial L}{\partial f} \frac{\partial f}{\partial z}<br>\end{array}\right.$$</p><h2 id="8-在sigmoid前提中对于梯度更新方向的进一步解析"><a href="#8-在sigmoid前提中对于梯度更新方向的进一步解析" class="headerlink" title="8. 在sigmoid前提中对于梯度更新方向的进一步解析"></a>8. 在sigmoid前提中对于梯度更新方向的进一步解析</h2><p>上一节中说明，方向是由下方公式共同决定:</p><p>$$x_i\cdot \frac{\partial L}{\partial f}\frac{\partial f}{\partial z}$$</p><p>而在sigmoid函数中,该函数的导数实际上是大于零：</p><p>$$\frac{\partial f}{\partial z}$$</p><p>那么此时更新方向只与输入和loss的导数相关，此时将问题再次限制到某个神经元单元中，loss导数固定，只与输入有关，对于ReLU这样的激活函数也存在同时只更新一个方向的问题，由此便可说明其变种leakyReLU、PReLU等出现的原因——它们的出现让输入可正可负。</p><p>本文实际上已经说明问题，公式补充请参考<a href="https://liam.page/2018/04/17/zero-centered-active-function/">博客</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>心累</title>
      <link href="/2022/04/18/life/%E5%BF%83%E7%B4%AF/"/>
      <url>/2022/04/18/life/%E5%BF%83%E7%B4%AF/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="a72e971753ffe09e9919f440d6efbafb5f7df4d2f9d8db6dcec075954f932315">556f064327254578d4f5db008d23104a723911764892c06eb641010e272d20c4769f539efe1b1ba283d209c7ef1f8915e678596f418475c686cb249b78694ff887608c5e4bb77a759c0fb2c2c27dd8a217770879e419299122c5debc336b851b1b795155cf4762452f3620174b7ae8a146a86dc774184be5c9ef67af8e14691c832741b27f91dd5bbf1f85982083e9d0155c94fa767b1a86dc055fce100737ad262629a42ca3f51670575a55e7d7fb8146ae0307b4627f8fab073effdb671f90bad8e67e36a6af3881953ff8a0242db5c35aa7299df560431fd809aac760d20f2a36263c98eab2564fccb987c29e9da71a42172c05890eec8e4ffeb6d8f5dd572f4b6db17ceacedd8f2f9f2a421f498add01ade417a36f5e4c4022d7d9bbf6f26d01fbae3947448f6a61585960a439f2d51a6f7c05d3e8e8e7c1e9136bbb1b452af2681537482f87c67a15fd5d3b2d7546ac4f14eeb68590c40cb00a5cc65bb7986fd4b21fa24e94c54456e88d1001e954c8b89a6ecfc75db42e69c87d89dcd48c270f8c5f9636ff4a1d4c756b6502dac1ece7eef18b2aaf7def504d443f8505f1919eb501a05ba4860ba3579c127192f91614a7da79c324ee5941d646d1f2c6145ac2f17f2f01560bbfbed8d6bf61308d2ed998a149e41c55f580296640b6fd7255a72704df3fec026a874de05bd480cd2bb3ca0598ae64b7b81c951eceef425f8e2dbedd58410bee6e0ffce8feb1fae4b0c3f855306a770b9a5f2d840ed65646a826541b06a907701f46369de62c84ebd32515a552d33a51b2b71c7c16509aa6e1921dba695b1e1c3bfc99a4dcdd230dd30e782ec8cf7516118f9e5a8b9a6e98f6d9e34fea532f614973096d2f0f4a0ae733d0c9a094482465ea4ed06ab0e57afed0a1b00b8b2180a2a3fbc64eae3c8794545fbb3b599dc7169d3e88efc503c4ead50b4e7330ddf2a450020a3181834bfc45e8c774e9b9d4de30e5a9acab15ddf2400ff7b1ed68e4200c931b3e5f9463453d9acf6a942138117d91f6524b4929e35fe8cb5f821ade622869ea69303f2475367e23908fbcdc69e78c25ae634595351d0fc112fbdb198a80ef9a16d332c76f2aad032830ba528c3c84d5ee11b4a20c8cc9ece8017126ecc2c9da1650c7a73b5eab2c86ddc88e24504e7a7b9f3b85d07dad7fb7a155d843017fcc7fa63a40a6a88512fa5a1d5e7084b47c96e7088ecb725bb68dfd7018cdd5504b304a65db867fa896ae051f9425a247cf28eeeabc8fb988696ad34fbf8620da388d8eb93d3256249d2635a5421b2813285dbdcb877b2d593de5c274839bffed8ac7b68f30f871068624b003d212d2d192f0dbfef6c85d1187e8ea74e890afc1dc31c734ec6c95fba2415c02c658f6fb604a1dc938639525861d26f6344370abb33204e76699912f4b0465b6d3954d280d2f2574057471d4fd6b1346bbad9018866dfab64e5a85017e1fb5aa615f728705e972b84bb890c54189e923cdc943f6c44c03bf8d4ccc7047ce7bbd7a1814ce823118425a6393cf7377fb36183d7a7989ebc8ec7da00e30c680c67c0cef09d1e171b38b25566785d2c6ac22254643e518f91f0aabf78b4ad6f1ce0a212250acb72fdf2df3efcd11f91adcf88a00350590cd640788a8d1ab3a6d6dd90f21fee13a63944808b26018fb6aa70493b156947efbcf837cbad54389a7ab470523ddd699c3318617ec44c0aed7423dc1d6e27b19ebe8499d0ddb4f95f196eb360651d02456172d7f60cc26219b60b6b88e45191ec9d21cbcaeebfc8c5299e66a3da9d634944a3b19cdc418fc2f73d0bbe2db406ee152c7a4d5d5f410e2eba376f9d900d790d4b74fff0c8fcae0aaa25f56c763e5bd3eea5d479976394657afea0d6bd66c375279c3cb33e8230a1a3fa54d15219264728c3652855a3882155ffc4120ca3fb5a97db25e0be388b17c896a24264dd6b4ca3e19bd1f7fe862eb803a35a75657e511bbe1b83ea2004b0bf00dacdf03b0ec0d460bf27001f668140936c7b89679bc583d5bc2cef2c788933f1d542bbe1d2ce1c3ccda59bf961369bf67aea8d864d949948906fd2f96893fa5fd1535ef6f81afca9b1486f3d8d8983695e72a812dc3a63c350cb05e844a45e168907d198c680d1586f2fa7684eb828383187e6ae745b6044225365b634a837d4ae0e4817c93556cef239206ee6e01c7c4097247240029af4fd167466713740ec6c3aa1a459e6c80036a2e9835aa51e0ea4abe4358c47fda415a7536e9914b6227954327fcf9deaf6dfdfaee9cf30a125edd16505a146816c442a697009739c90bf6162831c2504272d7eaeca325e38fd3d4e928e1ddc43399976c38fdc5d52aa43c69b406e272f078eda241c742206514db5dfc654b70d72c2fb6bca4c4ef7448aaf2167b872e6ed16a767906d7253535c0ea2c7f59514a48770b71cfc605b2471b1b8b2f69fbb7c2d3f0e9bc812518b23a6485f84641b2fd39ac5d2e9fed7b635a196c2413a8224722e8e2a5ad2af34cde162136dd2265d9b66100267f6fd679ffe3d130eaf678d63f81dad997b589fdc48d331c376a3bc052574773c8e74b1550287d79e23a6aeb4a72de1237d5f7ca415176ab3936e7d3ab0ae6ce957ec6725c92c45654b956df927d75b41696969654dc14ce819d98c8481b35333326cfb5485c1381ff2608f318eb09bcdab9d499381ea3489ab339174669dfe101e6f0de4b4cf7833bda6328eee1d42e9356474c17f3e4c6fadca6d8d70325c6841d2a795c01be7caf29138c2ae6b432c5f276b570398481da8dfcdc1280b5a176082ea47195bb2f2cef817140f63d0f2cd53b2384ed7759104da277e790e515e152c7472e5c7426c6348858f6a209fec411ec7ac066ae57d606aec5f9566baf3565f20bf9f3e765e65ecb76557c2f42c111cbe7df9028a7f4efc45183e9cc6a3ead67516188bfba37a5b0e2ee93060c7c1f427f3e99317013710a49a74d9e10cb0b84e58a298076a4ad8c8da3b6906704b8af6a342f95df74fd9eaf2c9588bdf85f36328255079848d4813bd432bed0854f45ce0cfd9c9ecd5a3957fcbed15b566a64dc2090d31b281e875107b23d0664edcc392d89a674fd0d14daabfd042da8323c99a6dbab434667c1ae96b05f5bacd23d0e2cad748f7c1bda290e523246171287933f2f10cb5866856aeeec912bec4943dcd036c992bf0b2c37671812a47f606dde6fb9d0f8f4c79ba37c40fb861786fcc3c8a8f100c32e70826975474ffb7a05b5b2584f36fd65278a7be49385e1f9bead70e6c0df943a25a41119f51e8994631aec9c395a198045453d8d4138e88b56064a2a97d45594fa997fe4a4797b5a495ed9bfac3ab28c5f5032c16556d5aef1c882aca9a5848b774dfdb67a875190a39a092d371fc66331c9f1922b68ee3ab99f078a53130e823e5e351db71b307a1bf7f3fb11107a4f7396fc56a906ce6d8bed83324075a04c35653cf0e46fc7ac1b2d6fcd6c18931a7c68dbae08b9f631c5c911057018ecc45528d86dab50d9190cc717c11ef5a804c680e30c6ddf9451dd0a9ae437ca21842f3c90a276c0c0eebe89e23477d0bc639d7c4576ada8c24a85dfe3797d7ebad2e134315efff2f98ddb876834800c32a30fdb1c2a5b6be3a832220dae96a8aa3d8947ece63393b125781fc816c7d5a5d82b5f236c4dcc900b86d54a1d0090f9070dd980ff026bdd3f2ca375149dd1fbc7eb29e2141fc209f16a64ba1c154fa716f5bcb10b6a09256daf2e22c64fc3109fc7c8725d81c20eadaa746d4ba27c01e2e9f89cb5ff5386c06818ef640c245bd201d20ad162dcdb057081cda16e79bc865a8a66596bb5995ff283fdc8791374fd22d0976e46809f50fbec7fb79767d1fe8612e80e3cf1a4da808854823a32e51c8e4803dac9ef274a12d60929459be2b4c7d466ba8ba247c93ee5440fd7f9c5681230b873f7a8ed495770288e364660c298fb99626c5e8f5101df8376ea81ce374ac3bffd31015c662f06a7252b2b906672dc6ba17694a782922e456fb76dddb72180a9357c42ff776c97a3a1fa878551b1fea79748434e42565df096123469f0518eff8ac3caf718f3d69db60dc368ca5cbad063c3e98dedaeaadb60642154596074b70503e41868c4216230ccf4cb2212d79ba1c7346c7ad29a1387f663c20261c4ca91b42d93bbf8b84c3c055d0473126208b4f59668b289715ed00f8f8c2a2099e75275e717e5c4faa04864b8c7593403fedeb70fcd3d450b224e458c09454e364bb462cb3b52d6d0317dd13efed6a9</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Visdom教程</title>
      <link href="/2022/04/17/work/Visdom%E6%95%99%E7%A8%8B/"/>
      <url>/2022/04/17/work/Visdom%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">pip install visdom</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开端口</span></span><br><span class="line">python -m visdom.server</span><br></pre></td></tr></table></figure><h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><h2 id="vis-test-vis-image"><a href="#vis-test-vis-image" class="headerlink" title="vis.test(),vis.image()"></a>vis.test(),vis.image()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom   <span class="comment"># 添加visdom库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np   <span class="comment"># 添加numpy库</span></span><br><span class="line">vis = visdom.Visdom(env= <span class="string">&#x27;test&#x27;</span>)   <span class="comment"># 设置环境窗口的名称,如果不设置名称就默认为main</span></span><br><span class="line">vis.text( <span class="keyword">class</span>=<span class="string">&#x27;test&#x27;</span>, win=<span class="string">&#x27;main&#x27;</span>)   <span class="comment"># 使用文本输出</span></span><br><span class="line">vis.image(np.ones(( <span class="number">3</span>,  <span class="number">100</span>,  <span class="number">100</span>)))   <span class="comment"># 绘制一幅尺寸为3 * 100 * 100的图片，图片的像素值全部为1</span></span><br></pre></td></tr></table></figure><h2 id="vis-images"><a href="#vis-images" class="headerlink" title="vis.images()"></a>vis.images()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 新建一个连接客户端</span></span><br><span class="line"><span class="comment"># 指定env = &#x27;test1&#x27;，默认是&#x27;main&#x27;,注意在浏览器界面做环境的切换</span></span><br><span class="line">vis = visdom.Visdom(env=<span class="string">&#x27;test1&#x27;</span>)</span><br><span class="line"><span class="comment"># 绘制正弦函数</span></span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">100</span>, <span class="number">0.01</span>)</span><br><span class="line">y = torch.sin(x)</span><br><span class="line">vis.line(X=x,Y=y, win=<span class="string">&#x27;sinx&#x27;</span>,opts=&#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;y=sin(x)&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># 绘制36张图片随机的彩色图片</span></span><br><span class="line">vis.images(torch.randn(<span class="number">36</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>).numpy(),nrow=<span class="number">6</span>, win=<span class="string">&#x27;imgs&#x27;</span>,opts=&#123;<span class="string">&#x27;title&#x27;</span>:<span class="string">&#x27;imgs&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure><h2 id="绘制loss函数"><a href="#绘制loss函数" class="headerlink" title="绘制loss函数"></a>绘制loss函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制loss变化趋势，参数一为Y轴的值，参数二为X轴的值，参数三为窗体名称，参数四为表格名称，参数五为更新选项，从第二个点开始可以更新</span></span><br><span class="line">vis.line(Y=np.array([totalloss.item()]), X=np.array([traintime]),</span><br><span class="line">                win=(<span class="string">&#x27;train_loss&#x27;</span>),</span><br><span class="line">                opts=<span class="built_in">dict</span>(title=<span class="string">&#x27;train_loss&#x27;</span>),</span><br><span class="line">                update=<span class="literal">None</span> <span class="keyword">if</span> traintime == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&#x27;append&#x27;</span></span><br><span class="line">                )</span><br></pre></td></tr></table></figure><h2 id="在一张图上绘制多条loss曲线"><a href="#在一张图上绘制多条loss曲线" class="headerlink" title="在一张图上绘制多条loss曲线"></a>在一张图上绘制多条loss曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实例化窗口</span></span><br><span class="line">wind = Visdom()</span><br><span class="line"><span class="comment"># 初始化窗口参数</span></span><br><span class="line">wind.line([&#123;<span class="number">0.</span>,<span class="number">0.</span>]],[<span class="number">0.</span>],win = <span class="string">&#x27;train&#x27;</span>,opts = <span class="built_in">dict</span>(title = <span class="string">&#x27;loss&amp;acc&#x27;</span>,legend = [<span class="string">&#x27;loss&#x27;</span>,<span class="string">&#x27;acc&#x27;</span>]))</span><br><span class="line"><span class="comment"># 更新窗口数据</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">loss = <span class="number">0.2</span> * np.random.randn() + <span class="number">1</span></span><br><span class="line">acc = <span class="number">0.1</span> * np.random.randn() + <span class="number">0.5</span></span><br><span class="line">wind.line([[loss, acc]],[step],win = <span class="string">&#x27;train&#x27;</span>,update = <span class="string">&#x27;append&#x27;</span>)</span><br><span class="line">time.sleep(<span class="number">0.5</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="实际代码"><a href="#实际代码" class="headerlink" title="实际代码"></a>实际代码</h2><blockquote><p>类定义：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 记录训练日志，显示生成图，画loss曲线 的类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Logger</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_epochs, batches_epoch</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param n_epochs:  跑多少个epochs</span></span><br><span class="line"><span class="string">        :param batches_epoch:  一个epoch有几个batches</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.viz = Visdom() <span class="comment"># 默认env是main函数</span></span><br><span class="line">        self.n_epochs = n_epochs</span><br><span class="line">        self.batches_epoch = batches_epoch</span><br><span class="line">        self.epoch = <span class="number">1</span> <span class="comment"># 当前epoch数</span></span><br><span class="line">        self.batch = <span class="number">1</span> <span class="comment"># 当前batch数</span></span><br><span class="line">        self.prev_time = time.time()</span><br><span class="line">        self.mean_period = <span class="number">0</span></span><br><span class="line">        self.losses = &#123;&#125;</span><br><span class="line">        self.loss_windows = &#123;&#125; <span class="comment"># 保存loss图的字典集合</span></span><br><span class="line">        self.image_windows = &#123;&#125; <span class="comment"># 保存生成图的字典集合</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, losses=<span class="literal">None</span>, images=<span class="literal">None</span></span>):</span><br><span class="line">        self.mean_period += (time.time() - self.prev_time)</span><br><span class="line">        self.prev_time = time.time()</span><br><span class="line"></span><br><span class="line">        sys.stdout.write(<span class="string">&#x27;Epoch %03d/%03d [%04d/%04d] -- &#x27;</span> % (self.epoch, self.n_epochs, self.batch, self.batches_epoch))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, loss_name <span class="keyword">in</span> <span class="built_in">enumerate</span>(losses.keys()):</span><br><span class="line">            <span class="keyword">if</span> loss_name <span class="keyword">not</span> <span class="keyword">in</span> self.losses:</span><br><span class="line">                self.losses[loss_name] = losses[loss_name].data.item() <span class="comment">#这里losses[loss_name].data是个tensor（包在值外面的数据结构），要用item方法取值</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.losses[loss_name] = losses[loss_name].data.item()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) == <span class="built_in">len</span>(losses.keys()):</span><br><span class="line">                sys.stdout.write(<span class="string">&#x27;%s: %.4f -- &#x27;</span> % (loss_name, self.losses[loss_name]/self.batch))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sys.stdout.write(<span class="string">&#x27;%s: %.4f | &#x27;</span> % (loss_name, self.losses[loss_name]/self.batch))</span><br><span class="line"></span><br><span class="line">        batches_done = self.batches_epoch * (self.epoch - <span class="number">1</span>) + self.batch</span><br><span class="line">        batches_left = self.batches_epoch * (self.n_epochs - self.epoch) + self.batches_epoch - self.batch</span><br><span class="line">        sys.stdout.write(<span class="string">&#x27;ETA: %s&#x27;</span> % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 显示生成图</span></span><br><span class="line">        <span class="keyword">for</span> image_name, tensor <span class="keyword">in</span> images.items(): <span class="comment"># 字典.items()是以list形式返回键值对</span></span><br><span class="line">            <span class="keyword">if</span> image_name <span class="keyword">not</span> <span class="keyword">in</span> self.image_windows:</span><br><span class="line">                self.image_windows[image_name] = self.viz.image(tensor2image(tensor.data), opts=&#123;<span class="string">&#x27;title&#x27;</span>:image_name&#125;)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.viz.image(tensor2image(tensor.data), win=self.image_windows[image_name], opts=&#123;<span class="string">&#x27;title&#x27;</span>:image_name&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># End of each epoch</span></span><br><span class="line">        <span class="keyword">if</span> (self.batch % self.batches_epoch) == <span class="number">0</span>: <span class="comment"># 一个epoch结束时</span></span><br><span class="line">            <span class="comment"># 绘制loss曲线图</span></span><br><span class="line">            <span class="keyword">for</span> loss_name, loss <span class="keyword">in</span> self.losses.items():</span><br><span class="line">                <span class="keyword">if</span> loss_name <span class="keyword">not</span> <span class="keyword">in</span> self.loss_windows:</span><br><span class="line">                    self.loss_windows[loss_name] = self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]),</span><br><span class="line">                                                                 opts=&#123;<span class="string">&#x27;xlabel&#x27;</span>:<span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;ylabel&#x27;</span>:loss_name, <span class="string">&#x27;title&#x27;</span>:loss_name&#125;)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.viz.line(X=np.array([self.epoch]), Y=np.array([loss/self.batch]), win=self.loss_windows[loss_name], update=<span class="string">&#x27;append&#x27;</span>) <span class="comment">#update=&#x27;append&#x27;可以使loss图不断更新</span></span><br><span class="line">                <span class="comment"># 每个epoch重置一次loss</span></span><br><span class="line">                self.losses[loss_name] = <span class="number">0.0</span></span><br><span class="line">            <span class="comment"># 跑完一个epoch，更新一下下面参数</span></span><br><span class="line">            self.epoch += <span class="number">1</span></span><br><span class="line">            self.batch = <span class="number">1</span></span><br><span class="line">            sys.stdout.write(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batch += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>调用代码</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">logger = Logger(opt.n_epochs, <span class="built_in">len</span>(dataloader))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.epoch, opt.n_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    </span><br><span class="line">    ......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 记录训练日志</span></span><br><span class="line">            <span class="comment"># Progress report (http://localhost:8097) 显示visdom画图的网址</span></span><br><span class="line">            logger.log(&#123;<span class="string">&#x27;loss_G&#x27;</span>: loss_G, <span class="string">&#x27;loss_G_identity&#x27;</span>: (loss_identity_A + loss_identity_B),</span><br><span class="line">                        <span class="string">&#x27;loss_G_GAN&#x27;</span>: (loss_GAN_A2B + loss_GAN_B2A),</span><br><span class="line">                        <span class="string">&#x27;loss_G_cycle&#x27;</span>: (loss_cycle_ABA + loss_cycle_BAB), <span class="string">&#x27;loss_D&#x27;</span>: (loss_D_A + loss_D_B)&#125;,</span><br><span class="line">                       images=&#123;<span class="string">&#x27;real_A&#x27;</span>: real_A, <span class="string">&#x27;real_B&#x27;</span>: real_B, <span class="string">&#x27;fake_A&#x27;</span>: fake_A, <span class="string">&#x27;fake_B&#x27;</span>: fake_B&#125;)</span><br></pre></td></tr></table></figure><h1 id="使用注意："><a href="#使用注意：" class="headerlink" title="使用注意："></a>使用注意：</h1><ul><li>画loss曲线时，输入的X和y因为是整数或者tensor，需要将其包裹在[]中</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数值稳定性引出模型初始化</title>
      <link href="/2022/04/16/work/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BC%95%E5%87%BA%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
      <url>/2022/04/16/work/%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BC%95%E5%87%BA%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>当数值过大或过小时都会导致数值问题（这里的数值指的是梯度数值）</p><h1 id="梯度爆炸的问题："><a href="#梯度爆炸的问题：" class="headerlink" title="梯度爆炸的问题："></a>梯度爆炸的问题：</h1><ol><li><p>值超出值域</p><ul><li>对于16位浮点数尤为严重</li></ul></li><li><p>对学习率敏感</p><ul><li>如果学习率太大-&gt;大参数值-&gt;更大的梯度</li><li>如果学习率太小-&gt;训练无进展</li><li>导致在训练过程中不断调整学习率</li></ul></li></ol><h1 id="梯度消失的问题："><a href="#梯度消失的问题：" class="headerlink" title="梯度消失的问题："></a>梯度消失的问题：</h1><ol><li><p>梯度值变为0</p><ul><li>对16位浮点数尤为严重</li></ul></li><li><p>训练没有进展</p><ul><li>不管如何选择学习率——因为梯度太小，学习率已经起不到作用了</li></ul></li><li><p>对于低层尤为严重</p><ul><li>仅仅顶部层训练的好——因为梯度是反向传播，从顶层开始。这意味着接近输入数据的层梯度太小，前几层没有起到作用，那么这样再深的神经网络本质上和浅层神经网络差不多</li><li>无法让神经网络更深</li></ul></li></ol><h1 id="如何让训练更稳定"><a href="#如何让训练更稳定" class="headerlink" title="如何让训练更稳定"></a>如何让训练更稳定</h1><ul><li>让梯度值在合理的范围内<ol><li>将乘法变成加法——如ResNet，LSTM就使用了加法（因为加法相对乘法，其数值变化不会太大）</li><li>归一化</li></ol><ul><li>梯度归一化，梯度裁剪</li></ul><ol start="3"><li>合理的权重初始和激活函数</li></ol></li></ul><h1 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h1><p>所谓的权重初始化，目的是希望将权重在初始化时就尽量将其取值在最优解的附近（将其在合理值区间里进行随机初始化），这样网络就能更快收敛</p><p>对于权重可使用Xviar进行初始化（根据每层的尺寸来变化，不是简单的满足均值和方差为(0,0.01)），对于激活函数可使用tanh和relu，但是sigmoid可调整为<code>4*sigmoid - 2</code></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch的Hook机制</title>
      <link href="/2022/04/16/work/Pytorch%E7%9A%84Hook%E6%9C%BA%E5%88%B6/"/>
      <url>/2022/04/16/work/Pytorch%E7%9A%84Hook%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="简单例子说明前向与反向传播过程"><a href="#简单例子说明前向与反向传播过程" class="headerlink" title="简单例子说明前向与反向传播过程"></a>简单例子说明前向与反向传播过程</h1><blockquote><p>例1：创建一个简单的计算图</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">c.backward()</span><br></pre></td></tr></table></figure><ol><li>使用默认值创建一个张量，其requires_grad默认为False；设置为True时，梯度初始化为1</li><li>当使用mul创建得到一个张量c时，同时会创建一个对应c的backward结点图（用于表明梯度会传递到哪些叶子），其由a和b共两个梯度图构成（该梯度图记录对应叶子的梯度值）。<ul><li>计算梯度流向：c-&gt;a,b</li></ul></li></ol><blockquote><p>例2：在1的基础上增加</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">e = c * d</span><br><span class="line">e.backward()</span><br></pre></td></tr></table></figure><ol><li>在1的基础上创建d，计算e：通过mul得到e，创建对应e反向节点图，其由两个方向：d和c</li><li><mark> 由计算得到的数值对应的反向计算图为节点图（其内表明该操作所包含哪些叶子），自定义创建的变量，其对应的反向计算图为梯度图（用于存储反向传播的grad值）</mark></li></ol><blockquote><p>例3：在2的基础上使用HOOK技术</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c = a * b</span><br><span class="line">c.register_hook(c_hook)</span><br><span class="line">c.register_hook(<span class="keyword">lambda</span> grad: <span class="built_in">print</span>(grad))</span><br><span class="line">c.retain_grad()</span><br><span class="line"></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">d.register_hook(<span class="keyword">lambda</span> grad: grad + <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line">e.retain_grad()</span><br><span class="line">e.register_hook(<span class="keyword">lambda</span> grad: grad * <span class="number">2</span>)</span><br><span class="line">e.retain_grad() <span class="comment"># 添加这个会因为上面已经存在而无效，但多添加对没有任何影响</span></span><br><span class="line"></span><br><span class="line">e.backward()</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol><li>在中间节点添加hook需要注意添加hook的顺序，在反向传播中会首先执行最先添加的hook。</li><li>在中间节点和叶节点添加hook有所不同<ol><li>在中间节点添加hook，同时其对应的反向计算图节点也会添加一个backword_hooks，这个hook是pre_hooks的（意味着是在该节点反向传播之前先进行hook操作）</li><li>在叶节点添加的hook不会关联到反向计算图的叶节点图中</li><li>在反向传播过程中，中间节点计算图只需要查看自己的hook即可；但叶节点由于并没有关联，需要先查看叶节点是由注册有hook机制并执行。</li></ol></li></ol><h1 id="为什么要使用HOOK？"><a href="#为什么要使用HOOK？" class="headerlink" title="为什么要使用HOOK？"></a>为什么要使用HOOK？</h1><ul><li>由上述可发现，可使创建变量或使用操作得到节点来更改&#x2F;变动前向传播，但是反向传播在整个过程中只是一行代码loss.backward()，这意味着我们只能查看反向传播结束后叶子节点上的梯度，无法查看甚至修改中间梯度，此时就需要使用HOOK机制，这能让我们访问甚至修改反向传播中的任何梯度</li></ul><h1 id="HOOK的使用注意"><a href="#HOOK的使用注意" class="headerlink" title="HOOK的使用注意"></a>HOOK的使用注意</h1><ul><li>不要使用inplace操作，即不要将梯度覆盖，仅仅运算即可；否则这种操作不仅会影响当前分支，还会导致当前分支上所有子节点梯度有所变化。（除非确定其不会对子节点有任何影响）<blockquote><p>例：在上述代码上展示</p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad + <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># inplace代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">c_hook</span>(<span class="params">grad</span>):</span><br><span class="line">    <span class="built_in">print</span>(grad)</span><br><span class="line">    <span class="keyword">return</span> grad += <span class="number">2</span></span><br></pre></td></tr></table></figure><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><h2 id="匿名函数lambda"><a href="#匿名函数lambda" class="headerlink" title="匿名函数lambda"></a>匿名函数lambda</h2><ul><li>匿名函数lambda如何使用？其又是在sort中的key里实现作用的？<blockquote><ol><li>匿名函数实际上是将简单的函数简化的一种写法，只用一行就能实现原本函数三行（首先是def，再是功能，最后是return）才能完成的事。</li><li>sort中的key和dict中的key不是同一个意思，sort中的key表示一种自定义的规则（即在函数中使用什么样的规则实现排序）</li><li>需要注意的是lambda函数可直接在声明之后立刻赋值：<code> list = [lambda x：x**2(i) for i range(0,10)]</code>，在该式中可看到直接将i复制到了lambda中。</li></ol></blockquote></li></ul><h2 id="反向传播中存在的问题"><a href="#反向传播中存在的问题" class="headerlink" title="反向传播中存在的问题"></a>反向传播中存在的问题</h2><h3 id="混淆概念"><a href="#混淆概念" class="headerlink" title="混淆概念"></a>混淆概念</h3><ol><li>反向传播后释放的是计算图的缓冲区，但是梯度并没有释放，故才需要在代码中进行梯度清零。</li><li>反向传播后，只有叶子节点的梯度值保留，中间变量的梯度值默认不保留，除非使用retain_grad()的方法</li></ol><ul><li>反向传播在更新什么，更新的是权重还是输入值？（即在本文的代码中对a，b在backward后有变化么，如果没有变化，在本文中所谓的权重又是什么？）</li></ul><ol><li>反向传播中更新的是权重，并不会改变输入值的大小。</li></ol><h1 id="遗留"><a href="#遗留" class="headerlink" title="遗留"></a>遗留</h1><p>由于HOOK对于当前阶段并不适用，关于<a href="https://www.bilibili.com/video/BV1MV411t7td?spm_id_from=333.999.0.0">视频</a>中后面在pycharm中的讲解那部分没有学习</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022-4-12</title>
      <link href="/2022/04/12/life/2022-4-12/"/>
      <url>/2022/04/12/life/2022-4-12/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="01518544ca6add0616e307616dee4d65e988996498208f5bdf3206651fe5d9dd">b3cf69ad95289fed13e724d57178b1a51fb4439f19aa62b82935113c005d7df75cbaaf50c19978e36a2e6b09ccd7ded776f8679f2354c924be14aab5439f6f567334ab6d711219a82bb6e69bb388f12e4866a7232ed819eb22f557edd84a36434584d49fe81597180f03e5000df7ff3206d26463b588773ac7b6b8bf6a176672056493ca080183a6422e208a2d11d8e2b27eb45ae6002b261ac31a91bcc2b17816d0e61e5153da90372b61aabbe0f7286be5fae26d33298554e4522b5daaeeb6595962bb02905c4601d19be8f41b595fd0c54b274cb2adf7254262ebd02d0218f80d702c713ada2fb317c8253efe3fa49a0a3914361e1395bd96b9774e789ed5f14f413228c899a70be205819126a58c2a64d4616024a447ef17ed05134440233e635039df0c5da3ad11acecebae0a4265cafdfa5ae473d98c258f8b7b638435d621c650e16897650bd39e6435075069ff25eafcad69cb7e2600e2b70d7c5c23cfda01d27aed7abe0a3be68a3e91261dbd631be31911bac030b60e57f8b81b72db5cd1a8b47479bff34ce50dc93622567a2c2746dd2672bf7ed672746ca363ac969e6b143c678ac13a24225096370ae403e5cbd0a10baa9d25d4c23c43df0a872c0082314371a00bc434c193988d2f624301f1296c22bf5b057a89874ebd6d22e9f3aa1bf02842a273c0fc97e5cd89c89f6254f9c8c1d82c31e42accb50cb7fd355db346b42b0a17f3b3bdca5fc5dbe34f73cb18e6bb0900ab47f9ae3a95cac944df8896211fbbd917802a3aad511f51caa1f99bbd653e4a12381819fc5a7a5ac76366a754a6e8cc05d7863a50b34e8fa1a402609d39432b5d2ed9333e06c2581c09cac5d47d5019b10382aeb1b891fc3fc2ddeca3320ca28d2b9a4e57c5890418b5a29f689c0484029a5ff0ac9888ffc0e16863dee83b003cdd09b6097e095aa25d26fcc4763021f352a8a94b4c06672cd5b8d13f4c7e3fdb8430cc48fe43094f8f0b0875b854eed3eaccee678f42397c994865ab5bcd63645c39089c0e1f413aa8da34529a181a832c6fb8cf67997ef01e54ce5650086e699bbfb20bdc895c7242b743b351bb46987f4b0eba7325fe8ab50d99b65bc2eb80a2949b1c7e92848bdcf0bb30aa35efce2e635eeb72a5fab2ebb2a319bfadabe2203b638410662fada75b07a9595d22bd3e9c5abe714f87342774a424a9ceaee6b453333e31e23a</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>邪恶</title>
      <link href="/2022/04/12/life/%E9%82%AA%E6%81%B6/"/>
      <url>/2022/04/12/life/%E9%82%AA%E6%81%B6/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="530805f297517d48ec28992eedde9233284db6ba0b6bec2e0eb6bb04052767eb">365f6828a6d9432dd36f78bf1ae632b0d3d14b9bca3110ec97aee6034e8ebaf640c1d3cf2e3457a7480ec63857b578391ef0a9a8a4b194702c107265b0aeee8b3bed3a38137295bb2fa736a76dae1f50a3f3026e34abe9b4a8be79b622e6d40072a8c48ab1d60dcebe716b4dd603421570332435cef19678c5440fa1bf434d222b8a59f8048d4db4f69dcb51d0bc603fcef0e0f2dc04fca44d022c93ad9ef6dc4659d33d56687dc6b4cb38f5770435ac6b63d7fbbf29066ba9b98c0395d96110d361b463d8ec9660de5e13354fb53e17cfcbd6868389302e3d2b277195e39149fcd6a9009a67df6ea4aa362db672dbe80a82bd7780a2d9c6f9debfece114456e5aa045b16c77b75b510f226ea1784da9</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows</title>
      <link href="/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/"/>
      <url>/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/</url>
      
        <content type="html"><![CDATA[<h1 id="ArtFlow：Unbiased-Image-Style-Transfer-via-Reversible-Neural-Flows"><a href="#ArtFlow：Unbiased-Image-Style-Transfer-via-Reversible-Neural-Flows" class="headerlink" title=" ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows "></a><center> ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows </center></h1><h2 id="文章概述"><a href="#文章概述" class="headerlink" title="文章概述"></a>文章概述</h2><ul><li>图像迁移过程中出现内容泄露，主要原因是decoder不是无损重建，故文中提出projection-reversion结构，该模块核心思想是projection和reversion是对称的，其各小模块共享，reversion在结构上完全是反过来的projection，这样的结构有效解决了内容泄露</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StyTr2:Image Style Transfer with Transformers</title>
      <link href="/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/"/>
      <url>/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/</url>
      
        <content type="html"><![CDATA[<h1 id="StyTr2-Image-Style-Transfer-with-Transformers"><a href="#StyTr2-Image-Style-Transfer-with-Transformers" class="headerlink" title=" StyTr2:Image Style Transfer with Transformers "></a><center> StyTr2:Image Style Transfer with Transformers </center></h1><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>在传统的风格迁移网络中加入transformer模块，让效果更好</li></ul><h3 id="思路概述："><a href="#思路概述：" class="headerlink" title="思路概述："></a>思路概述：</h3><ol><li>加入transformer模块</li><li>引入内容感知位置编码</li></ol><h2 id="文章提到的关键点以及需要注意的文献"><a href="#文章提到的关键点以及需要注意的文献" class="headerlink" title="文章提到的关键点以及需要注意的文献"></a>文章提到的关键点以及需要注意的文献</h2><h3 id="信息点"><a href="#信息点" class="headerlink" title="信息点"></a>信息点</h3><ol><li>风格转换的思路是将内容图像的二姐统计信息与风格图像对齐来进行优化——[6]-[14]</li><li>可以使用transformer提取不同层但提取相似的结构信息——[24]</li><li>identity loss——[15]</li></ol><h3 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h3><ol><li>内容损失：计算某特征层与输入层的L2Loss</li><li>风格损失：一般与某特征层的均值、方差有关</li></ol><h3 id="具体内容"><a href="#具体内容" class="headerlink" title="具体内容"></a>具体内容</h3><h4 id="内容感知的位置编码"><a href="#内容感知的位置编码" class="headerlink" title="内容感知的位置编码"></a>内容感知的位置编码</h4><blockquote><p>对图片不再使用物理意义上的位置编码，根据内容来自适应编码，代码操作如下</p></blockquote><ol><li>首先对原图进行AdaptiveAvgPool2d自适应平均池化<ul><li>注意直接使用原图的像素数据</li><li>指定固定大小的输出（AdaptiveAvgPool2d(18)是将原图池化层成[b,c,18,18]大小）</li></ul></li><li>进行1*1的一个卷积，不进行尺寸与通道的上的变化，用于提取特征信息</li><li>将该特征信息的尺寸采样到与风格图像相同尺寸<br>最终的使用跟transformer一样，直接与图片的patch_embedding相加</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>文章是为了加入transformer而引入内容感知的位置编码，在解释上是一种知道答案硬扯问题的感觉，编码思路值得借鉴，技术内容没有新意</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Inpainting with External-internal Learning and Monochromic Bottleneck</title>
      <link href="/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/"/>
      <url>/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/</url>
      
        <content type="html"><![CDATA[<h1 id="Image-Inpainting-with-External-internal-Learning-and-Monochromic-Bottleneck"><a href="#Image-Inpainting-with-External-internal-Learning-and-Monochromic-Bottleneck" class="headerlink" title=" Image Inpainting with External-internal Learning and Monochromic Bottleneck "></a><center> Image Inpainting with External-internal Learning and Monochromic Bottleneck </center></h1><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>解决图像修复时存在的blunt structures和伪影，以他人模型输出的修复图作为输入进行再调整</li></ul><h3 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h3><ol><li>external learning stage：输入单通道重建缺失的结构和细节，即提取结构和细节特征</li><li>internal learning stage：在上述的特征上进行颜色恢复</li></ol><h2 id="文章解决我个人的疑惑以及需要注意的文献"><a href="#文章解决我个人的疑惑以及需要注意的文献" class="headerlink" title="文章解决我个人的疑惑以及需要注意的文献"></a>文章解决我个人的疑惑以及需要注意的文献</h2><ol><li>最常见的伪影是渗色</li></ol><p>【文献】:[边缘]——20，17； [结构信息]——24； [分割图]——29； [模糊图像]——37，38，36</p><h2 id="模型代码思路"><a href="#模型代码思路" class="headerlink" title="模型代码思路"></a>模型代码思路</h2><ol><li>取得其他图像修复模型输出图像（本文以其他模型输出存在的缺陷为出发点），取其灰度图（单通道）</li><li>对灰度图下采样得到三个尺度的图像（仅resize得到）<ol><li>对最小尺度的灰度图进行颜色重构：输入单通道下采样灰度图，注意是全图重构，不是对masked图像重构，loss计算对象：Loss1（颜色重构图 X mask， 原图像 X mask）。<mark> 需要注意的是颜色重构了全图，包括缺陷处的颜色，但是计算loss只计算非缺陷处的loss，这样做避免了对缺陷处直接计算，但是通过非缺陷处的优化来重构缺陷处的颜色重构）</mark></li><li>对中间尺度的灰度图进行颜色重构：输入为cat（第一步output，中间尺寸灰度图）——四通道，输出三通道的颜色重构图，loss计算同1</li><li>对原图尺度的灰度图进行颜色重构：相同于2</li></ol></li></ol><p>【注意】：每个尺寸的颜色重构网络不共享</p><h2 id="文章关键点"><a href="#文章关键点" class="headerlink" title="文章关键点"></a>文章关键点</h2><p>虽然只计算非缺陷处的图像损失，用来优化网络模型，但重构网络中仍然对缺陷处进行颜色重构，即意味着使用非缺陷优化网络间接重构了缺陷处颜色。（也可以理解为学习非缺失区域的颜色映射，直接应用于确实区域进行着色）</p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>loss和反向传播的关系</title>
      <link href="/2022/04/08/work/loss%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
      <url>/2022/04/08/work/loss%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E5%85%B3%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：</p><ul><li>文献中总会出现求f的最小值、最大值，疑惑在于模型怎么知道你最大最小的？模型怎么按照最大最小去变化？</li></ul><ol><li>模型参数的调整和loss输出的值没有直接关系，loss输出可理解为期望与实际值之间直观的数值上的可视化，所谓的损失越小越好就是这里的loss数值越小越好。</li><li>模型权重的调整是以label作为基准线，使用loss计算target和label之间的差异，该差异体现在反向传播时的梯度，该梯度作为把控不断调整模型数据分布向label数据分布靠拢，即模型在label附近徘徊。</li><li>模型会将梯度的反方向作为调整方向，此时loss就会变大变小，方向是模型更新时自适应变化的。</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[gan]High-Fidelity GAN Inversion for Image Attribute Editing</title>
      <link href="/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/"/>
      <url>/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/</url>
      
        <content type="html"><![CDATA[<h1 id="High-Fidelity-GAN-Inversion-for-Image-Attribute-Editing"><a href="#High-Fidelity-GAN-Inversion-for-Image-Attribute-Editing" class="headerlink" title=" High-Fidelity GAN Inversion for Image Attribute Editing "></a><center> High-Fidelity GAN Inversion for Image Attribute Editing </center></h1><h2 id="文章生成思路"><a href="#文章生成思路" class="headerlink" title="文章生成思路"></a>文章生成思路</h2><h3 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h3><ul><li>让图像能够编辑，生成自己想要的图像风格</li></ul><h3 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h3><ol><li>确定生成模型<br>使用GAN，但生成图像的中间步骤不可控，且生成效果较差。使用基于编码器的GAN，特点在于encoder得到的latent能够进行更多操作（如本文中的编辑，其需要某种特征，而完整的GAN无法生成中间latent）</li><li>编码器中low-latent存在保真度低的问题，重建效果较差；而high-latent可编辑性差，此时考虑在低层进行细节补偿，然后投影到高层，由此进行互补</li><li>对low-latent进行细节保留处理（该细节信息使用重建图像与原图的失真图），将失真图和原图cat后提取low-latent，将其作为系数调整原图重建的low-latent，再将其重建回high-latent，由此得到目标图像</li><li>以上方法可正常应用于在原图上的所有操作，但是中间输入的是编辑图像，不可回避的问题是编辑图像对于原图有所错位，即第二步骤使用的失真图和编辑图像同样存在错位，故设计一个ADA模块先将两者进行对齐，再进行第二步骤</li></ol><h2 id="文章解决我个人的疑惑以及需要注意的文献"><a href="#文章解决我个人的疑惑以及需要注意的文献" class="headerlink" title="文章解决我个人的疑惑以及需要注意的文献"></a>文章解决我个人的疑惑以及需要注意的文献</h2><ol><li>简单的GAN由于是直接重建图像，而基于编码器可生成latent，这个latent可进行更多操作</li><li>在特征提取的过程中，丢失的信息主要是图像特定的细节，倾向于保留公共信息3</li></ol><p>【文献】:1、17、45、26、34、40、5、7、29、32</p><h2 id="关键词理解"><a href="#关键词理解" class="headerlink" title="关键词理解"></a>关键词理解</h2><ul><li>low-rate：GAN inversion时无法实现高保真（因为细节丢失，信息不完整，无法达到高保真的效果），可理解为低层特征</li><li>high-rate：因信息丰富重建效果好，但正是因为信息保留完整导致图片的编辑性变差，可理解为高层特征</li><li>ADA：使用编辑后，所谓的对齐是因为重建图像的latent不只是原图latent，还存在了其他编辑属性（如年龄），而失真图是重建原图和原图之差，此时失真图和编辑图还有编辑属性这个差集，不处理会导致最终重建效果图存在伪影，ADA可以将失真图和编辑属性自定义对齐</li></ul><h2 id="指出问题："><a href="#指出问题：" class="headerlink" title="指出问题："></a>指出问题：</h2><ol><li>low-rate的latent code在重建和编辑图像时难以保证高保真。</li><li>增大latent code大小虽然能提高GAN反演的准确性，但是导致图像可编辑行变差</li></ol><h2 id="解决方法："><a href="#解决方法：" class="headerlink" title="解决方法："></a>解决方法：</h2><ol><li>设计一个结构能得到丰富细节的hig-rate的latent code</li><li>设计一个结构去保证从原图生成的图像和编辑图像存在联系</li></ol><h2 id="文中要点"><a href="#文中要点" class="headerlink" title="文中要点"></a>文中要点</h2><p>原文摘要：To enable real-world image editing, GAN inversion techniques [40] have been recently explored, which aim at projecting images to the latent space of a pre-trained GAN generator.</p><ul><li><pre><code>为了实现真实世界的图像编辑，最近探索了 GAN 反演技术 [40]，其旨在将图像投影到预训练的 GAN 生成器的潜在空间。</code></pre><blockquote><p>问题：</p><ol><li>如何实现真实世界的图像编辑？<blockquote><ul><li>使用GAN version技术将其投影到可编辑的空间（latent space）</li></ul></blockquote></li><li>为什么需要latent space？<blockquote><ul><li>在特征图上进行编辑，而不是直接在原图上进行编辑，反映在文章中是选择图像的某一种属性进行编辑（如年龄，该抽象特征无法直接在原图上进行编辑，故需要提取出原图相对于该属性的特征才能编辑）</li></ul></blockquote></li><li>将其投影到什么空间？<blockquote><ul><li>文中选择将图像投影到与预训练G能够生成的latent space相同的空间中，此时再进行编辑。（个人猜测是为了将两者进行空间对齐，方便两个模块的信息之间进行传递）</li></ul></blockquote></li></ol></blockquote></li></ul><h2 id="总体流程"><a href="#总体流程" class="headerlink" title="总体流程"></a>总体流程</h2><ol><li>W &#x3D; encoder(X)<blockquote><p>将输入图像经过encoder得到latent向量W</p></blockquote></li><li>X^ &#x3D; decoder(W)<blockquote><p>将latent W经过decoder进行重建</p></blockquote></li><li>res_gt &#x3D; X^ - X</li><li>res_gt &#x3D; transformer(res_gt)</li><li>res_align &#x3D; align(cat(res_gt, X^))<blockquote><p>进行图像对齐</p></blockquote></li><li>delta &#x3D; res_align - res_gt</li><li>F &#x3D; consulation(delta)<blockquote><p>得到相应位置的权重系数</p></blockquote></li><li>img &#x3D; decoder(W,F)<blockquote><p>调整decoder重建时相同通道数的每个像素值的权重</p></blockquote></li></ol><h2 id="Ec模块"><a href="#Ec模块" class="headerlink" title="Ec模块"></a>Ec模块</h2><ol><li>将经过对齐后的delta输入encoder提取特征</li><li>使用自定义的卷积核对特征进行卷积，提取出两层特征<ul><li>关于这两层的使用是在decoder中，相加后直接将其作为权重系数，然后调整decoder重建时相同通道数的每个像素值的权重</li></ul></li></ol><h2 id="ADA模块"><a href="#ADA模块" class="headerlink" title="ADA模块"></a>ADA模块</h2><p>input[b, 6, h, w]</p><blockquote><p>第一部分：conv，其中stride&#x3D;2，故导致图片尺寸下降</p></blockquote><ol><li>conv_layer1：图像尺寸不变<ul><li>Conv + BatchNorm + PReLU</li></ul></li><li>conv_layer2：图像尺寸减半<ol><li><code>[b, 6, h, w] -&gt; [b, 16, h/2, w/2]：</code>Conv + BatchNorm</li><li><code>[b, 6, h, w] -&gt; [b, 16, h/2, w/2]：</code>BatchNorm + Conv + Conv</li><li><code>[b, 6, h/2, w/2] -&gt; [b, 16, h/2, w/2]：</code> 1 + 2</li></ol></li><li>conv_layer3：图像尺寸减半</li><li>conv_layer4：图像尺寸减半</li></ol><blockquote><p>第二部分：dconv，主要分为两部分</p><ol><li>图像插值上采样</li><li>和conv部分完全相同的卷积层，只不过stride&#x3D;1，不对图片进行下采样</li></ol></blockquote><ol><li>dconv1<ol><li>插值上采样</li><li>上次输出和采样值通道cat输入到_block</li></ol></li><li>dconv2<ol><li>插值上采样</li><li>上次输出和采样值通道cat输入到_block</li></ol></li><li>dconv3<ol><li>上次输出和采样值通道cat输入到_block<br>【输出与原图大小相同的对其重建图像】</li></ol></li></ol><h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><blockquote><p>问题描述：报错很明显为数据类型错误，将float-&gt;long即可，但是无论是将类型变成float64或者long仍然报同样的错误</p><ul><li>经过尝试后发现：将将类型直接转换成float即可</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">600</span>,<span class="number">1</span>,dtype=torch.float64).view(<span class="number">1</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错如下</span></span><br><span class="line">expected scalar <span class="built_in">type</span> Long but found Float</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更改如下</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>,<span class="number">600</span>,<span class="number">1</span>,dtype=torch.<span class="built_in">float</span>).view(<span class="number">1</span>, <span class="number">6</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perceptual Loss</title>
      <link href="/2022/04/06/work/Perceptual%20Loss/"/>
      <url>/2022/04/06/work/Perceptual%20Loss/</url>
      
        <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>生成任务中损失函数分为分为两个阶段：</p><ol><li>计算生成图像与输入图像之间的损失，被称为<mark> Per-Pixel Loss </mark>，经研究发现，如果将原图向任意方向偏移一个像素，这样做实际上本身分辨率和风格均未发生太大变换，但是Per-Pixel Loss却会因这一个像素的偏移出现显著上升，可推论Per-Pixel Loss并未反映&#x2F;约束图像高级特征信息</li><li>基于Per-Pixel Loss的缺陷，提出将约束角度从出入与输出转向约束feature，即Pixel-&gt;feature，故生成了<mark> Perceptual Loss </mark>（意为能感知到高层语义特征）</li></ol><p><a href="https://blog.csdn.net/WhaleAndAnt/article/details/107116360?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164920726816782248548021%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164920726816782248548021&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-107116360.142%5Ev5%5Epc_search_result_control_group,157%5Ev4%5Econtrol&utm_term=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%85%AC%E5%BC%8F&spm=1018.2226.3001.4187">参考博文</a></p><p><a href="https://blog.csdn.net/studyeboy/article/details/118724526?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164920726816782248586959%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=164920726816782248586959&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-6-118724526.142%5Ev5%5Epc_search_result_control_group,157%5Ev4%5Econtrol&utm_term=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%85%AC%E5%BC%8F&spm=1018.2226.3001.4187">参考博文</a><br>关于感知损失的使用方法参考文献：<a href="https://arxiv.org/abs/2103.10571">Generic Perceptual Loss for Modeling Structured Output Deppendencies</a></p><h1 id="2-Loss"><a href="#2-Loss" class="headerlink" title="2. Loss"></a>2. Loss</h1><blockquote><p>感知损失目的是约束高层语义信息，其形式大体可总结为两个部分</p></blockquote><ol><li>feature损失<ol><li>高层feature内容损失</li><li>高层feature风格损失</li></ol></li><li>Per-Pixel损失</li></ol><p>$$\text{Perceptual Loss} &#x3D; \text{Loss_feature} + \text{Loss_style} + \text{Loss_Per-Pixel}$$</p><p>为了让输出图像的风格和内容均保持输入的原图与风格图像的特征信息，只约束输入与输出会导致中间层收敛方向不明确（个人猜测），故在中间特征层也使用感知损失来约束，从而确定在整个过程中整个模型均向同一个方向优化</p><ol><li>内容损失<code>Loss_feature</code>作用在较低特征层上即可——保留一些纹理细节</li><li>风格损失<code>Loss_style</code>可以作用在从低到高所有特征层上——保留一些语义上的信息</li></ol><h1 id="3-常见的生成模型损失函数"><a href="#3-常见的生成模型损失函数" class="headerlink" title="3. 常见的生成模型损失函数"></a>3. 常见的生成模型损失函数</h1><p>所谓的感知损失并没有固定的公式，只要是在特征级上的损失即可视为感知损失，下方的损失函数可自行组合</p><h2 id="3-1-Feature-Reconstruction-Loss"><a href="#3-1-Feature-Reconstruction-Loss" class="headerlink" title="3.1 Feature Reconstruction Loss"></a>3.1 Feature Reconstruction Loss</h2><p>$$l_{\text {feat }}^{\phi, j}(\hat{y}, y)&#x3D;\frac{1}{C_{j} H_{j} W_{j}}\left|\phi_{j}(\hat{y})-\phi_{j}(y)\right|_{2}^{2}$$</p><p>计算第j层的特征重建损失，CHW是第j层feature_map的size</p><h2 id="3-2-Style-Reconstruction-Loss"><a href="#3-2-Style-Reconstruction-Loss" class="headerlink" title="3.2 Style Reconstruction Loss"></a>3.2 Style Reconstruction Loss</h2><p>对于风格重建的损失函数，首先要先计算 Gram 矩阵:</p><p>$$G_{j}^{\phi}(x)<em>{c, c^{\prime}}&#x3D;\frac{1}{C</em>{j} H_{j} W_{j}} \sum_{h&#x3D;1}^{H_{j}} \sum_{w&#x3D;1}^{W_{j}} \phi_{j}(x)<em>{h, w, c} \phi</em>{j}(x)_{h, w, c^{\prime}}$$</p><p>产生的 feature_map 的大小为 CjHjWjCjHjWj，可以看成是 CjCj 个特征，这些特征两两之间的内积的计算方式如上。</p><p>$$l_{\text {style }}^{\phi, j}(\hat{y}, y)&#x3D;\left|G_{j}^{\phi}(\hat{y})-G_{j}^{\phi}(y)\right|_{F}^{2}$$</p><p>两张图片，在 loss 网络的每一层都求出 Gram 矩阵，然后对应层之间计算欧式距离，最后将不同层的欧氏距离相加，得到最后的风格损失。</p><h2 id="3-3-Simple-Loss-Function"><a href="#3-3-Simple-Loss-Function" class="headerlink" title="3.3 Simple Loss Function"></a>3.3 Simple Loss Function</h2><h3 id="3-3-1-Pixel-Loss"><a href="#3-3-1-Pixel-Loss" class="headerlink" title="3.3.1 Pixel Loss"></a>3.3.1 Pixel Loss</h3><p>pixel loss 是输出 y^ 和目标 y 之间的欧几里得距离</p><p>$$l_{\text {pixel }}(\hat{y}, y)&#x3D;|\hat{y}-y|_{2}^{2}$$</p><h3 id="3-3-1-Total-Variation-Regularization"><a href="#3-3-1-Total-Variation-Regularization" class="headerlink" title="3.3.1 Total Variation Regularization"></a>3.3.1 Total Variation Regularization</h3><p>Total Variation Loss，实际上是一个平滑项（一个正则化项），目的是使生成的图像在局部上尽可能平滑，而它的定义和马尔科夫随机场（MRF）中使用的平滑项非常相似。</p><p>$$ l_{T V}(\hat{y})&#x3D;\sum_{n}\left|\hat{y}<em>{n+1}-\hat{y}</em>{n}\right|_{2}^{2} $$</p><p>其中 yn+1 和 yn+1 是相邻像素</p><h2 id="3-4-计算判别器之间的损失"><a href="#3-4-计算判别器之间的损失" class="headerlink" title="3.4 计算判别器之间的损失"></a>3.4 计算判别器之间的损失</h2><p>$$ l_{\text {feat }}^{D, l}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)&#x3D;\frac{1}{2 C_{l} H_{l} W_{l}}\left|D_{l}(\mathbf{x})-D_{l}\left(\mathbf{x}^{\prime}\right)\right|_{F}^{2} $$</p><p>其中x , x’，x分别表示源和生成图像，D 表示判别器网络，l 表示判别器的第l 层，CHW为特征图size</p><ul><li><input checked="" disabled="" type="checkbox"> 注意这里计算判别器之间的损失：其作用是什么，放在融合网络中可以进行特征补偿么，查看其原文论文为什么这么做</li></ul><ol><li>Li M, Zuo W, Zhang D, et al. Deep Identity-aware Transfer of Facial Attributes.[J]. arXiv: Computer Vision and Pattern Recognition, 2016</li><li>Wang C, Xu C, Wang C, et al. Perceptual Adversarial Networks for Image-to-Image Transformation[J]. IEEE Transactions on Image Processing, 2018, 27(8): 4066-4079</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>匹配低层特征可以保留几何、纹理等低级语义信息，匹配深层特征可以保留内容、风格等高级语义信息。</li><li>预训练网络提取的特征附带预训练时的任务属性，会对生成模型产生影响，所以预训练模型尽量选择与生成任务相关的模型参数。</li><li>选择特征匹配层不必局限于激活层，选择激活层之前的特征进行匹配，可以为生成模型提供更强的监督信息。</li><li>对于具有Ground Truth的任务，深度特征匹配可以设置多层且较强的约束；对于没有Ground Truth仅使用感知损失做重构的任务，深度特征匹配应设置单层且较弱的约束。</li></ul><h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ol><li>使用项目两个判别器中的feature计算loss有什么作用，请查看原文献<ul><li>已经解决：因为原项目分类器预训练使用的其他数据集，之后固定了参数，故转向判别器进行约束，其中判别器使用的是自己的数据集</li></ul></li><li>在这些损失函数中使用L1loss和L2loss有什么区别</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>抱歉</title>
      <link href="/2022/04/03/life/%E6%8A%B1%E6%AD%89/"/>
      <url>/2022/04/03/life/%E6%8A%B1%E6%AD%89/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="d97aff4b256e06de5f6b745002518349373595d52c7adb064578799d8eaaac62">7ab9d85c8eda3888a25d4b7fa9c695bd44f6ad7c704f97ef3b670a2230628f08</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>将</title>
      <link href="/2022/04/03/life/%E5%B0%86/"/>
      <url>/2022/04/03/life/%E5%B0%86/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="0b4927115b1be92104c2d0bc157641c0936b0d61c7baf31e7d81bea4cd6e6a31">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Me </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Idea</title>
      <link href="/2022/04/02/Idea/"/>
      <url>/2022/04/02/Idea/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="73fc77f843c6aeacab1f0cdc3579a71bddd0bb62b040e0e7bbc7e8b2a1826e77">b37e1c5f21f5af8a2d0feaf56fcb9ee1b85e5698fc43d4497edf61ce25fb459a8ab63d148a137dd47c5985e2afd45fa6fd605ef6c7198f5da845bc1e151e1e9d4207ad8b96f7a00bcf6385a3aa611fefbe63c640a45fc904f5f50aaa90eb46a89c4355d4b4037c51ccc3b1374d987c5c1d01d739bab095b55ae7964a58c1b3d419ef6cc6156e2434121cbba8b66c88849d328bba7deff6d2ffe0cacf756a0fd1d7f1c8a7061dd7c1dbafad0e31487fdf55493e72feb3cfa0a1fe3eb6bb9fc83956a05b5efb0428e938410c950a3db6a37dedc557dc10ca73b354de0b4c935527aeedba1d14d00f1658ab182ef2054149e09bada79d61d6127d44a6752ae7457932b3ad609052a0733b59b2a7518d44296912fba943d9abfc500259c21c7f176c1faf9c55332e699767c31581700bac6700f56ecaac83e6e89b8d92ef2a25abfa6fa6dab599208ba9e716a6995f233912e08aaff891f68aeedb1df9800b423c00e2ec753ac2d8c666ca8b018dbe0651e686b3aba08fda3ac2fe61e1f28e6fa1f53c858f45f7d6945d97b9d1698dc48434e048d0086162eb05e953c9d16cfcc38e22eb7187a904abe2d73df0164568b16869610cd2fa1f31522ae1cb7de957a4314d1050ae024132951a0f2af363ff2f5441986fcb99ab3ffe4044e9b81ead3384822cecd37aab4f515853414847af006d0adad4e21456a8d5b3b05f712207cb2a08c4a93bb4444bbf29e6fdcc1b175324e565a9eef07c1787ca39627c35702de5b26ee9fe9360e80b3b7f9820c5d22ef4ff59775514e0b6cc701138c3a66dd28d32d46091afd7b0fe02156bee9b8e779d3c7ff29da187c84bd1e9a75f4add4249a52cc4f6aadeae8dbcb62defe7ebf32b</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo博客中渲染数学公式</title>
      <link href="/2022/04/01/blog/hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
      <url>/2022/04/01/blog/hexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="一、更换渲染引擎"><a href="#一、更换渲染引擎" class="headerlink" title="一、更换渲染引擎"></a>一、更换渲染引擎</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p><mark>注意此处有坑，更换渲染引擎之后会导致hexo某些markdown语法无法正常显示，以及hexo博客中图片无法显示</mark></p><ul><li>由于无法正常显示，我重新将卸载的引擎重新安装后异常消失了，更换的引擎也就没有卸载了</li></ul><h1 id="二、解决语义冲突"><a href="#二、解决语义冲突" class="headerlink" title="二、解决语义冲突"></a>二、解决语义冲突</h1><ol><li>进入博客根目录：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node_modules\kramed\lib\rules\inline.js</span><br></pre></td></tr></table></figure><ol><li>修改该文件中两处代码</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一处位于11行</span></span><br><span class="line">//  escape: /^\\([\\`*&#123;&#125;\[\]()<span class="comment">#$+\-.!_&gt;])/,</span></span><br><span class="line">  escape: /^\\([`*\[\]()<span class="comment">#$+\-.!_&gt;])/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二处位于20行</span></span><br><span class="line">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">  em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span><br></pre></td></tr></table></figure><ol start="3"><li>开启主题配置文件中mathjax引擎渲染</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意其中per_page可以不开启，但是enable必须开启使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MathJax</span></span><br><span class="line">mathjax:</span><br><span class="line">  enable: true</span><br><span class="line">  per_page: false</span><br></pre></td></tr></table></figure><ol start="4"><li>没有使用per_page时则在每篇文章开头写</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mathjax: true</span><br></pre></td></tr></table></figure><p>三、公式书写注意</p><ol><li>得到的LaTeX公式需要加上<mark>单$</mark>才能成功渲染</li><li>对LaTeX公式加<mark>双$包裹</mark>可将公式居中显示</li><li>注意公式上下的回车空行</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA安装后出现的两种报错</title>
      <link href="/2022/04/01/work/CUDA%E5%AE%89%E8%A3%85%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%8A%A5%E9%94%99/"/>
      <url>/2022/04/01/work/CUDA%E5%AE%89%E8%A3%85%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<blockquote><p>安装CUDA之后，运行代码，会出现两种报错<br>1.cuda安装的版本过高，NVIDIA显卡驱动无法驱动<br>2.cuda安装的是pytorch，其版本和cuda不太适配</p></blockquote><hr><h1 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The NVIDIA driver on your system <span class="keyword">is</span> too old (found version <span class="number">10010</span>).</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 解决：安装版本较低的cuda——查看GPU最高版本要求</li></ul><h1 id="问题二："><a href="#问题二：" class="headerlink" title="问题二："></a>问题二：</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Torch <span class="keyword">not</span> compiled <span class="keyword">with</span> CUDA enabled</span><br></pre></td></tr></table></figure><ul><li><input checked="" disabled="" type="checkbox"> 解决：不安装官网给出的pytorch-cudatoolkit那种conda安装命令，使用torch-cu那种pip安装命令</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSIM</title>
      <link href="/2022/03/31/work/SSIM/"/>
      <url>/2022/03/31/work/SSIM/</url>
      
        <content type="html"><![CDATA[<blockquote><p>结构相似性指数SSIM(论文：Image Quality Assessment: From Error Visibility to Structural Similarity)用于度量两幅图像间的结构相似性，结构被定义为三个关键特征：</p><ol><li>亮度</li><li>对比度</li><li>结构</li></ol><p>其中SSIM值越大，图像越相似，两幅图片完全相同时，SSIM&#x3D;1，<mark>故作为损失函数时，应该取负号（即越相似loss越小）</mark>，如：loss &#x3D; 1-SSIM</p></blockquote><p>实际loss使用：</p><p>$$\text{loss}(x, y) &#x3D; \frac{1 - \text{SSIM}(x, y)}{2}$$</p><h1 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h1><p>这个系统计算2幅给定图像之间的结构相似度指数，取值范围[0,1]，其中极端值具有相同的含义。用均值作为亮度的估计，标准差作为对比度的估计，协方差作为结构相似程度的估计</p><h2 id="1-1-亮度"><a href="#1-1-亮度" class="headerlink" title="1.1 亮度"></a>1.1 亮度</h2><p>将图像的平均灰度作为测量估计。用μ表示为：</p><p>$$\mu_{x}&#x3D;\frac{1}{N} \sum_{i&#x3D;1}^{N} x_{i}$$</p><p>对应到图像中：</p><p>$$\mu_{X}&#x3D;\frac{1}{H \times M} \sum_{i&#x3D;1}^{H} \sum_{j&#x3D;1}^{M} X(i, j)$$</p><h2 id="1-2-对比度"><a href="#1-2-对比度" class="headerlink" title="1.2 对比度"></a>1.2 对比度</h2><p>将图像的标准差(方差的平方根)作为测量估计。用σ表示为：</p><p>$$\sigma_{x}&#x3D;\left(\frac{1}{N-1} \sum_{i&#x3D;1}^{N}\left(x_{i}-\mu_{x}\right)^{2}\right)^{\frac{1}{2}}$$</p><p>对应到图像中：</p><p>$$\sigma_{X}&#x3D;\left(\frac{1}{H+W-1} \sum_{i&#x3D;1}^{H} \sum_{j&#x3D;1}^{M}\left(X(i, j)-\mu_{X}\right)^{2}\right)^{\frac{1}{2}}$$</p><h2 id="1-3-结构"><a href="#1-3-结构" class="headerlink" title="1.3 结构"></a>1.3 结构</h2><p>结构比较是通过使用一个合并公式来完成的(后面会详细介绍)，但在本质上，我们用输入信号的标准差来除以它，因此结果有单位标准差，这可以得到一个更稳健的比较。</p><p>$$\left(\mathbf{x}-\mu_{x}\right) &#x2F; \sigma_{x}$$</p><p>其中x是输入图像。</p><h1 id="2-比较函数"><a href="#2-比较函数" class="headerlink" title="2. 比较函数"></a>2. 比较函数</h1><ol><li>亮度比较函数：由函数定义，l(x, y)，如下图所示。μ表示给定图像的平均值。x和y是被比较的两个图像。</li></ol><p>$$l(\mathbf{x}, \mathbf{y})&#x3D;\frac{2 \mu_{x} \mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$$</p><p>其中C1为常数，保证分母为0时的稳定性。C1这样给出：</p><p>$$C_{1}&#x3D;\left(K_{1} L\right)^{2}$$</p><p>其中k1&#x3D;0.01,L代表输入数据的最大值（没有归一化处理的图像取值255，归一化取值1）</p><ol start="2"><li>对比度函数：由函数c(x, y)定义，如下图所示。σ表示给定图像的标准差。x和y是被比较的两个图像。</li></ol><p>$$c(\mathbf{x}, \mathbf{y})&#x3D;\frac{2 \sigma_{x} \sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}}$$</p><p>其中C2这样给出：</p><p>$$C_{2}&#x3D;\left(K_{2} L\right)^{2}$$</p><p>其中k2&#x3D;0.03</p><p>3.结构比较函数：由函数s(x, y)定义，如下图所示。σ表示给定图像的标准差。x和y是被比较的两个图像。</p><p>$$s(\mathbf{x}, \mathbf{y})&#x3D;\frac{\sigma_{x y}+C_{3}}{\sigma_{x} \sigma_{y}+C_{3}}$$</p><p>其中c3&#x3D;c2&#x2F;2</p><h1 id="3-SSIM测量函数"><a href="#3-SSIM测量函数" class="headerlink" title="3. SSIM测量函数"></a>3. SSIM测量函数</h1><h2 id="3-1-SSIM的定义式"><a href="#3-1-SSIM的定义式" class="headerlink" title="3.1 SSIM的定义式"></a>3.1 SSIM的定义式</h2><p>$$\begin{aligned}<br>\operatorname{SSIM}(x, y) &amp;&#x3D;f(l(x, y), c(x, y), s(x, y)) \<br>&amp;&#x3D;[l(x, y)]^{\alpha}[c(x, y)]^{\beta}[s(x, y)]^{\gamma}<br>\end{aligned}$$</p><p>其中， α、β、γ &gt; 0，用来调整这三个模块的重要性<br>假设α、β、γ都为1，C3 &#x3D; C2 &#x2F; 2 ,则</p><p>$$S S I M(x, y)&#x3D;\frac{\left(2 \mu_{x} \mu_{y}+C_{1}\right)\left(2 \sigma_{x y}+C_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+C_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}\right)}$$</p><p>SSIM函数的值域为[0, 1], 值越大说明图像失真越小，两幅图像越相似。</p><h2 id="3-2-SSIM函数满足的三个条件"><a href="#3-2-SSIM函数满足的三个条件" class="headerlink" title="3.2 SSIM函数满足的三个条件"></a>3.2 SSIM函数满足的三个条件</h2><ol><li>对称性：S(x,y)&#x3D;S(y,x)</li><li>有界性：S(x,y)≤1</li><li>最大值唯一性：当且仅当x&#x3D;y时，S(x,y)&#x3D;1</li></ol><h1 id="4-实际应用"><a href="#4-实际应用" class="headerlink" title="4. 实际应用"></a>4. 实际应用</h1><p>使用在全局范围会导致局部信息受损，故一般使用一个固定大小的滑窗，计算局部SSIM，故此，公式进行局部约束：</p><p>$$\begin{aligned}<br>\mu_{x} &amp;&#x3D;\sum_{i&#x3D;1}^{N} w_{i} x_{i} \<br>\sigma_{x} &amp;&#x3D;\left(\sum_{i&#x3D;1}^{N} w_{i}\left(x_{i}-\mu_{x}\right)^{2}\right)^{\frac{1}{2}} \<br>\sigma_{x y} &amp;&#x3D;\sum_{i&#x3D;1}^{N} w_{i}\left(x_{i}-\mu_{x}\right)\left(y_{i}-\mu_{y}\right)<br>\end{aligned}$$</p><p>其中wi是高斯加权函数。</p><h1 id="5-遗留"><a href="#5-遗留" class="headerlink" title="5. 遗留"></a>5. 遗留</h1><ol><li>均值、标准差、归一化为什么可以分别作为亮度、对比度、结构特征的计算，这三种数据计算方法的几何意义、物理意义是什么？</li><li>SSIM论文中应该有相关信息没有注意到</li><li>为什么2ab&#x2F;(a+b)的形式能够作为对比？</li><li>损失函数后面调整为负值，进行加减等操作会对反向传播造成什么影响？</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> img_fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCGAN训练思考</title>
      <link href="/2022/03/30/work/DCGAN%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83/"/>
      <url>/2022/03/30/work/DCGAN%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83/</url>
      
        <content type="html"><![CDATA[<h1 id="工作说明"><a href="#工作说明" class="headerlink" title="工作说明"></a>工作说明</h1><blockquote><p>用了两天的时间使用MNIST数据集训练DCGAN</p><ol><li>第一天怎么训练，loss D都在两个batch_size内快速下降到0.0x，之后直接趋向于0</li><li>第二天训练的各项数据正常</li></ol></blockquote><h1 id="问题回顾"><a href="#问题回顾" class="headerlink" title="问题回顾"></a>问题回顾</h1><ol><li><p>第一天的训练：</p><ol><li>错误的认为loss D应该保持在0.5左右——保持在0.5左右的是D(x)和D(G(Z))，而不是loss函数</li><li>将real_label设置为0，fake_label设置为1——对训练有一定的影响，必要时可以对调，更好的方法是使用软标签</li></ol></li><li><p>第二天的训练：</p><ol><li>添加D(x)和D(G(Z))可视化曲线</li><li>将标签对调为正常情况</li></ol></li></ol><h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><ul><li>INFO：数据</li></ul><center> D、G[1:1]更新 </center><pre><code class="python">Epoch [0/5] Batch 0/469                 D(x):0.5047，D(G(z)): 0.5004，output: 0.4535，Loss D: 0.6889,loss G: 0.7908Epoch [0/5] Batch 100/469               D(x): 0.9880，D(G(z)): 0.0171，output: 0.0165，Loss D:0.0147， loss G: 4.1140Epoch [e/5] Batch 200/469               D(x):0.8712，D(G(z)): 0.8090，output: 0.6145，Loss D: 0.9081，loss G: 0.4901Epoch [0/5] Batch 300/469               D(x):0.6222，D(G(z)): 0.3736，output: 0.3109，Loss D: 0.4783，loss G: 1.1826Epoch [0/5] Batch 400/469               D(x): 0.6995，D(G(z)): 0.4954，output: 0.1875，Loss D: 0.5354,loss G: 1.7224Epoch [1/5] Batch 0/469                 D(x):0.5371，D(G(z)):0.3516，output: 0.4096，Loss D: 0.5372，loss G: 0.9151Epoch [1/5] Batch 100/469               D(x): 0.5416，D(G(z)):0.4679，output: 0.3907，Loss D: 0.6339,loss G: 0.9563Epoch [1/5] Batch 200/469               D(x):0.5662，D(G(z)):0.4320，output: 0.4054,Loss D: 0.5761， loss G: 0.9195Epoch [1/5] Batch 300/469               D(x): 0.5569，D(G(z)): 0.4211，output:0.3418,Loss D: 0.5769, loss G: 1.0925Epoch [1/5] Batch 400/469               D(x):0.4531，D(G(z)): 0.2992，output: 0.5755，Loss D: 0.5908， loss G: 0.5740Epoch [2/5] Batch 0/469                 D(x): 0.5905，D(G(z)): 0.4527，output: 0.3409，Loss D: 0.5796，loss G: 1.1067Epoch [2/5] Batch 100/469               D(x):0.6021，D(G(z)):0.4192，output: 0.3489,Loss D: 0.5394，loss G: 1.0880Epoch [2/5] Batch 200/469               D(x): 0.5366，D(G(z)): 0.4305，output: 0.4065,Loss D: 0.6069，loss G: 0.9206Epoch [2/5] Batch 300/469               D(x):0.5416，D(G(z)):0.2691，output: 0.4179，Loss D: 0.4877， loss G: 0.9194Epoch [2/5] Batch 400/469               D(x):0.7101，D(G(z)):0.4133，output: 0.1640，Loss D:0.4520，loss G: 1.8622Epoch [3/5] Batch 0/469                 D(x):0.7748，D(G(z)): 0.3042，output: 0.0808，Loss D: 0.3200，loss G: 2.6203Epoch [3/5] Batch 100/469               D(x): 0.6514,D(G(z)):0.1118，output: 0.1721，Loss D: 0.2863，loss G: 1.9265Epoch [3/5] Batch 200/469               D(x):0.8176，D(G(z)):0.3861，output: 0.0538，Loss D: 0.3626,loss G: 3.1119Epoch [3/5] Batch 300/469               D(x): 0.7629，D(G(z)):0.3083，output: 0.0762，Loss D: 0.3354，loss G: 2.7599Epoch [3/5] Batch 400/469               D(x): 0.8042，D(G(z)):0.2689，output: 0.1024，Loss D: 0.2779,loss G: 2.4286Epoch [4/5] Batch 0/469                 D(x):0.8273，D(G(z)): 0.3855，output: 0.0283，Loss D: 0.3615，loss G: 3.8461Epoch [4/5] Batch 100/469               D(x):0.8260，D(G(z)):0.1448,output: 0.1007,Loss D: 0.1850， loss G: 2.5335Epoch [4/5] Batch 200/469               D(x):0.9277，D(G(z)):0.4498,output: 0.0191，Loss D: 0.3639，loss G: 4.2212Epoch [4/5] Batch 300/469               D(x): 0.5809，D(G(z)):0.0402，output: 0.4912，Loss D: 0.3201，loss G: 0.8003Epoch [4/5] Batch 400/469               D(x): 0.9238，D(G(z)):0.5808,output: 0.1350,Loss D: 0.5133, loss G: 2.1491</code></pre><ol><li>从图中可以发现，第一、第二个batch_size明显虚高，是因为GAN的训练特性——在G尚未学习的情况下首先更新D，由此导致前几个batch_size中D的分数较高，不过个人猜测是正常情况</li><li>随着训练epoch增加，D(x)的性能平均保持在0.8左右，而G的分数一直保持在0.3左右，个人猜测是G的学习能力较低——应降低D的学习频率，增加G的学习频率<ul><li>经过测试后发现，减少D的训练频率会导致G偷懒，从噪声生成的所有图片都是一个样子来欺骗D,其中设置G的训练频率是D的[4,14,32]倍——从4倍起就开始偷懒，<mark>故调整训练频率保持在[2,3]倍左右，或者不调整训练频率，改为调整学习率。</mark></li></ul></li></ol><center> D、G[1:2]更新 </center><pre><code class="python">Epoch [0/5] Batch 0/469                     D(x): 0.4946, D(G(z)): 0.4892, output: 0.4453, Loss D: 0.6879, loss G: 0.8089Epoch [0/5] Batch 100/469                   D(x): 0.9524, D(G(z)): 0.0601, output: 0.0536, Loss D: 0.0558, loss G: 2.9602Epoch [0/5] Batch 200/469                   D(x): 0.9416, D(G(z)): 0.1034, output: 0.0751, Loss D: 0.0856, loss G: 2.6562Epoch [0/5] Batch 300/469                   D(x): 0.4917, D(G(z)): 0.4844, output: 0.4827, Loss D: 0.6864, loss G: 0.7284Epoch [0/5] Batch 400/469                   D(x): 0.4993, D(G(z)): 0.4931, output: 0.4904, Loss D: 0.6872, loss G: 0.7127Epoch [1/5] Batch 0/469                     D(x): 0.4806, D(G(z)): 0.3470, output: 0.7397, Loss D: 0.5817, loss G: 0.3027Epoch [1/5] Batch 100/469                   D(x): 0.4667, D(G(z)): 0.4295, output: 0.4518, Loss D: 0.6636, loss G: 0.7955Epoch [1/5] Batch 200/469                   D(x): 0.4734, D(G(z)): 0.4435, output: 0.4776, Loss D: 0.6698, loss G: 0.7406Epoch [1/5] Batch 300/469                   D(x): 0.5026, D(G(z)): 0.4947, output: 0.4927, Loss D: 0.6868, loss G: 0.7092Epoch [1/5] Batch 400/469                   D(x): 0.5637, D(G(z)): 0.5503, output: 0.4108, Loss D: 0.6870, loss G: 0.8902Epoch [2/5] Batch 0/469                     D(x): 0.5009, D(G(z)): 0.4761, output: 0.5022, Loss D: 0.6697, loss G: 0.6893Epoch [2/5] Batch 100/469                   D(x): 0.5442, D(G(z)): 0.5315, output: 0.4591, Loss D: 0.6839, loss G: 0.7789Epoch [2/5] Batch 200/469                   D(x): 0.5498, D(G(z)): 0.5392, output: 0.4371, Loss D: 0.6871, loss G: 0.8279Epoch [2/5] Batch 300/469                   D(x): 0.4320, D(G(z)): 0.4212, output: 0.5299, Loss D: 0.6934, loss G: 0.6352Epoch [2/5] Batch 400/469                   D(x): 0.3814, D(G(z)): 0.3730, output: 0.5444, Loss D: 0.7159, loss G: 0.6087Epoch [3/5] Batch 0/469                     D(x): 0.5367, D(G(z)): 0.5260, output: 0.4762, Loss D: 0.6847, loss G: 0.7421Epoch [3/5] Batch 100/469                   D(x): 0.5263, D(G(z)): 0.5135, output: 0.5199, Loss D: 0.6817, loss G: 0.6546Epoch [3/5] Batch 200/469                   D(x): 0.3974, D(G(z)): 0.3897, output: 0.5477, Loss D: 0.7087, loss G: 0.6024Epoch [3/5] Batch 300/469                   D(x): 0.5390, D(G(z)): 0.5214, output: 0.4688, Loss D: 0.6780, loss G: 0.7582Epoch [3/5] Batch 400/469                   D(x): 0.5181, D(G(z)): 0.5077, output: 0.4879, Loss D: 0.6836, loss G: 0.7180Epoch [4/5] Batch 0/469                     D(x): 0.4994, D(G(z)): 0.4818, output: 0.5079, Loss D: 0.6764, loss G: 0.6781Epoch [4/5] Batch 100/469                   D(x): 0.5411, D(G(z)): 0.5297, output: 0.4642, Loss D: 0.6854, loss G: 0.7683Epoch [4/5] Batch 200/469                   D(x): 0.5262, D(G(z)): 0.5221, output: 0.4577, Loss D: 0.6912, loss G: 0.7824Epoch [4/5] Batch 300/469                   D(x): 0.4609, D(G(z)): 0.4493, output: 0.5305, Loss D: 0.6862, loss G: 0.6345Epoch [4/5] Batch 400/469                   D(x): 0.5205, D(G(z)): 0.5072, output: 0.4688, Loss D: 0.6810, loss G: 0.7585   </code></pre><ul><li>观察发现数值按照预期一样，但是图像可视化中显示生成人眼可辨识的数字在epoch慢于[1:1]速率——故在此不推荐更改训练速率，更改学习率即可</li></ul><h1 id="DCGAN生成手写数字集项目总结"><a href="#DCGAN生成手写数字集项目总结" class="headerlink" title="DCGAN生成手写数字集项目总结"></a>DCGAN生成手写数字集项目总结</h1><ul><li><input checked="" disabled="" type="checkbox"> 拟解决问题</li></ul><blockquote><ol><li>本项目中查看了torchvision.utils.transformers.Normalize函数，在GAN中需要将输入图片经过ToTensor转化到[0,1]之间，在经过Normalize图像矩阵中的数据从[0,1]转化到[-1,1]之间，同时将生成器最后一层生成的数据经过tanh函数映射到[-1,1]之间</li><li>为什么需要将数据归一化，且为什么归一化到[0,1]或[-1,1]之间？<ol><li>加快梯度下降求最优解的速度将数据约束在已知范围内——统一输入数据分布<mark>（深度学习的本质就是在学习数据分布）</mark>，否则网络需要适应不同分布的数据，降低网络训练速度</li><li>有可能提高精度</li></ol></li></ol></blockquote><ul><li><input disabled="" type="checkbox"> 拟生成问题</li></ul><blockquote><p>模型如何通过loss的约束来调整模型，如何确定约束时最大还是最小</p></blockquote><ul><li><input disabled="" type="checkbox"> 项目训练总结</li></ul><blockquote><ol><li>训练直接崩塌可对调label</li><li>可视化数据中D(x)在后期保持高分，D(G)一直保持低分，这种情况是G的学习能力差，调整学习率即可</li><li>中途怀疑batch_size对训练有影响，但是尝试[32,64,128]，经过不仔细观察影响效果不大</li></ol></blockquote><h1 id="遗漏"><a href="#遗漏" class="headerlink" title="遗漏"></a>遗漏</h1><ul><li>本次模型调试中没有可视化曲线图、直方图</li><li>没有可视化特征层</li><li>没有可视化卷积核</li></ul><p>【预计在后面调试融合模型出现问题时会回头查看该模型的各层权重直方图】</p><h1 id="code"><a href="#code" class="headerlink" title="code"></a>code</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hyperparameters etc.</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">LEARNING_RATE = <span class="number">2e-4</span>  <span class="comment"># could also use two lrs, one for gen and one for disc</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">IMAGE_SIZE = <span class="number">64</span></span><br><span class="line">CHANNELS_IMG = <span class="number">1</span></span><br><span class="line">NOISE_DIM = <span class="number">100</span></span><br><span class="line">NUM_EPOCHS = <span class="number">5</span></span><br><span class="line">FEATURES_DISC = <span class="number">64</span></span><br><span class="line">FEATURES_GEN = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">transforms = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.Resize(IMAGE_SIZE),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(</span><br><span class="line">            [<span class="number">0.5</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(CHANNELS_IMG)], [<span class="number">0.5</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(CHANNELS_IMG)]</span><br><span class="line">        ),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you train on MNIST, remember to set channels_img to 1</span></span><br><span class="line">dataset = datasets.MNIST(root=<span class="string">&quot;dataset/&quot;</span>, train=<span class="literal">True</span>, transform=transforms,</span><br><span class="line">                       download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># comment mnist above and uncomment below if train on CelebA</span></span><br><span class="line"><span class="comment">#dataset = datasets.ImageFolder(root=&quot;celeb_dataset&quot;, transform=transforms)</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels_img, features_d</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, self).__init__()</span><br><span class="line">        self.disc = nn.Sequential(</span><br><span class="line">            <span class="comment"># input: N x channels_img x 64 x 64</span></span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                channels_img, features_d, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span></span><br><span class="line">            ),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            <span class="comment"># _block(in_channels, out_channels, kernel_size, stride, padding)</span></span><br><span class="line">            self._block(features_d, features_d * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            self._block(features_d * <span class="number">2</span>, features_d * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            self._block(features_d * <span class="number">4</span>, features_d * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">            <span class="comment"># After all _block img output is 4x4 (Conv2d below makes into 1x1)</span></span><br><span class="line">            nn.Conv2d(features_d * <span class="number">8</span>, <span class="number">1</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(</span><br><span class="line">                in_channels,</span><br><span class="line">                out_channels,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride,</span><br><span class="line">                padding,</span><br><span class="line">                bias=<span class="literal">False</span>,</span><br><span class="line">            ),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.disc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channels_noise, channels_img, features_g</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            <span class="comment"># Input: N x channels_noise x 1 x 1</span></span><br><span class="line">            self._block(channels_noise, features_g * <span class="number">16</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>),  <span class="comment"># img: 4x4</span></span><br><span class="line">            self._block(features_g * <span class="number">16</span>, features_g * <span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 8x8</span></span><br><span class="line">            self._block(features_g * <span class="number">8</span>, features_g * <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 16x16</span></span><br><span class="line">            self._block(features_g * <span class="number">4</span>, features_g * <span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>),  <span class="comment"># img: 32x32</span></span><br><span class="line">            nn.ConvTranspose2d(</span><br><span class="line">                features_g * <span class="number">2</span>, channels_img, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span></span><br><span class="line">            ),</span><br><span class="line">            <span class="comment"># Output: N x channels_img x 64 x 64</span></span><br><span class="line">            nn.Tanh(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(</span><br><span class="line">                in_channels,</span><br><span class="line">                out_channels,</span><br><span class="line">                kernel_size,</span><br><span class="line">                stride,</span><br><span class="line">                padding,</span><br><span class="line">                bias=<span class="literal">False</span>,</span><br><span class="line">            ),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># Initializes weights according to the DCGAN paper</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):</span><br><span class="line">            nn.init.normal_(m.weight.data, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)</span><br><span class="line">disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)</span><br><span class="line">initialize_weights(gen)</span><br><span class="line">initialize_weights(disc)</span><br><span class="line"></span><br><span class="line">opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"></span><br><span class="line">fixed_noise = torch.randn(<span class="number">32</span>, NOISE_DIM, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">writer_real = SummaryWriter(<span class="string">f&quot;logs/real&quot;</span>)</span><br><span class="line">writer_fake = SummaryWriter(<span class="string">f&quot;logs/fake&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">gen.train()</span><br><span class="line">disc.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(NUM_EPOCHS):</span><br><span class="line">    <span class="comment"># Target labels not needed! &lt;3 unsupervised</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (real, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real = real.to(device)</span><br><span class="line">        noise = torch.randn(BATCH_SIZE, NOISE_DIM, <span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">        fake = gen(noise)</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            disc_real = disc(real).reshape(-<span class="number">1</span>)</span><br><span class="line">            loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))</span><br><span class="line">            disc_fake = disc(fake.detach()).reshape(-<span class="number">1</span>)</span><br><span class="line">            loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))</span><br><span class="line">            loss_disc = (loss_disc_real + loss_disc_fake) / <span class="number">2</span></span><br><span class="line">            disc.zero_grad()</span><br><span class="line">            loss_disc.backward()</span><br><span class="line">            opt_disc.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment">### Train Generator: min log(1 - D(G(z))) &lt;-&gt; max log(D(G(z))</span></span><br><span class="line"></span><br><span class="line">        output = disc(fake).reshape(-<span class="number">1</span>)</span><br><span class="line">        loss_gen = criterion(output, torch.ones_like(output))</span><br><span class="line">        gen.zero_grad()</span><br><span class="line">        loss_gen.backward()</span><br><span class="line">        opt_gen.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print losses occasionally and print to tensorboard</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                <span class="string">f&quot;Epoch [<span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;NUM_EPOCHS&#125;</span>] Batch <span class="subst">&#123;batch_idx&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span> \</span></span><br><span class="line"><span class="string">                  D(x): <span class="subst">&#123;disc_real.mean().item():<span class="number">.4</span>f&#125;</span>, D(G(z)): <span class="subst">&#123;disc_fake.mean().item():<span class="number">.4</span>f&#125;</span>, output: <span class="subst">&#123;output.mean().item():<span class="number">.4</span>f&#125;</span>, Loss D: <span class="subst">&#123;loss_disc:<span class="number">.4</span>f&#125;</span>, loss G: <span class="subst">&#123;loss_gen:<span class="number">.4</span>f&#125;</span>&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = gen(fixed_noise)</span><br><span class="line">                <span class="comment"># take out (up to) 32 examples</span></span><br><span class="line">                img_grid_real = torchvision.utils.make_grid(</span><br><span class="line">                    real[:<span class="number">32</span>], normalize=<span class="literal">True</span></span><br><span class="line">                )</span><br><span class="line">                img_grid_fake = torchvision.utils.make_grid(</span><br><span class="line">                    fake[:<span class="number">32</span>], normalize=<span class="literal">True</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                writer_fake.add_image(<span class="string">&quot;Fake&quot;</span>, img_grid_fake, global_step=step)</span><br><span class="line"></span><br><span class="line">            step += <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>blog-markdown书写规则</title>
      <link href="/2022/03/29/blog/blog-markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/"/>
      <url>/2022/03/29/blog/blog-markdown%E4%B9%A6%E5%86%99%E8%A7%84%E5%88%99/</url>
      
        <content type="html"><![CDATA[<h1 id="blog-markdown书写规则"><a href="#blog-markdown书写规则" class="headerlink" title=" blog-markdown书写规则 "></a><center> blog-markdown书写规则 </center></h1><ul><li>站内文章链接：<code>&#123;% post_link file_name 'show_name' %&#125;</code><ul><li>post_link: 固定参数（不要删除）</li><li>file_name: 在文件夹中的名称，不需要带后缀</li><li>show_name: 不显示文件名字，自定义显示文字，默认显示title</li><li>检索关键词：站内文章</li></ul></li><li>文字效果：&lt;<code>b/i/u/s/big/mark</code>&gt; ******* &lt;&#x2F;<code>b/i/u/s/big/mark</code>&gt;</li><li>文章首行缩进：<code>&lt;p style=&quot;text-indent:2em&quot; &gt;</code></li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TOC</title>
      <link href="/2022/03/29/LIFE%20TOC/"/>
      <url>/2022/03/29/LIFE%20TOC/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="d46d88f3f5914b623715509d7832039254faa82e49f25fe8d62f84da79d6e49e">901015936e2ec1bc2a4d01c822635a06dbabc75c330bdf0d123f64dbd819e43430d0a05c3025834c03c24ebf318f6df515738cb63f9a216b3cae01030f30249b4c39b0efee78df4317fd591b23e5ad8076fd989204ca0663875ca43ddf17ac047e07e2008653afaaedb91f3cc83954156c99bb710833be778b8c37fc25407c487afe0884174bec091d37da6a9571d6638822e59a719bbf7009d1aafcb89e62fa86a2ef73ad1e4584fdb376c7fedd7f639c14477c26f5eb04e7cbc7b3b12b1703d1b08113d72ea2c291fb53a6cd0e7f2e87afb2fd909082b621c3197c0d6b23bf5caa8d0876faeab6a2e299446beec4befa296bec2ab7e55da3f9e0dd6eb6dd4269442bab58c02d7f64168221ee47ccbc3b529c0245cfa8f9368ff4b86f8798dacb0b9b7f59ec6ab0b43f19920248892296383e87f503627811dc4dbd8ad70b91489be916abd58132128660b8f71b01f2f1fdbaea22ac28af0b0a987a80ccd6fb6544a8ca44bc2f1c49a8e8f8adef3cccceb3e2d1415d8bc3afa31fbec33995f0aed8dbfcce5c1ceb0c252fbc261ae3d308b7e60f6c8faa2b00aead7451cc53b32c5948c78196f1d48e991e9b04df40303ecae276b6ee81c4c18a60e639236af5148f34a2c86930ea46034082105e15826cadb587d0734847dd406bedaf99ea8311733cfdf8c12f1ab784284235bc5b3bb7cb18431ef9098b8bfd5fcbd03b333c4c7dfd0876b512cb49704fa97141fb40fb7680cc549d2660d2190e228a729b260adbde430b3f64745ba49fcc57843d8ab13bb880b0a5df4cbb5dd9220d53f9d3d7cc39b5a60d67c5fee3f34ad577aea58e4efb700fa1ff423f8c1aca80a1c1427b5e8cc9e22605d47bead6e1794f6632a3becaa56d2666048df083011f4b571c9b74bfb73b025c2d49c491637f4884da7feb91d2f954d8f3f3dedd9987630df8b492b7acbec9804ce04a216c4c68ba5d6d3dee2b49133bf73bb70daddad98add</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双向</title>
      <link href="/2022/03/29/life/%E5%8F%8C%E5%90%91/"/>
      <url>/2022/03/29/life/%E5%8F%8C%E5%90%91/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c89fb12ebdfd2ce00ed8d2b221fea91a9aa40c737ca8541c01040aa9b4742fe5">d8eb541f534e62ff3f46f2f28f0d52a5a1cc6b8838a78340aa66e10bc3a389411123a8e26eecb6d8e16d64418164ed19020aa495700bffdfd5bc9dce1c28f86effba5c06d69bb9741f76c7c8fcb95c76cdcb028fd22690e1110134dcd9df3c2e255924d15160f6dbc18803f06c762044a161653cab4d7283144587248b2870df513c38b156dcb7ac9c5f4b189b46c69cc001a6f326058c08f5b01e5c8c3770f2fdfe6f53b7df9a1d8f1b4e4e9b8a11cca3cb2e47073a25eb6d685997bb127d7b68b35fe28e9852d6d4e5daece160cc48005baa59fccd715970803a68bd82a82b29be2bdb237b0b30fa5fbf828e7ef621872ae5a7df7548765294d473a419ee0118b52a2026afdab44908f6994aca32bc7ddf7f21868fd60eda6ce13592b6b6b1368b2bc7c83f9d67895d778762136f39c4cc30632c689fd5d4e07360465d5f0eac7e34c9d22f5adafef4007830e740b777bb5021d5305ccf6d0335ab86e995bd4bdffc4dd7199a4411d20de69051c07cf0be3b6ac6ce920d551ecf7c3a96583a7c0ccf88ff5e01436fc33bd3e18b6c5e</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Review</title>
      <link href="/2022/03/28/Review/"/>
      <url>/2022/03/28/Review/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="a4759d7bcae7af1f7cccdaa9e15a4a89b11f5a419cc09e66e7d1c38b71bd1f15">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>模型调整方法探究</title>
      <link href="/2022/03/27/work/%E6%A8%A1%E5%9E%8B%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%8E%A2%E7%A9%B6/"/>
      <url>/2022/03/27/work/%E6%A8%A1%E5%9E%8B%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%E6%8E%A2%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>探究问题：在结合新idea搭建好模型后，该模型毫无疑问不会达到预期的效果，甚至是没有效果，本文欲找出问题所在之处并，进行数据分析，最后调整</p></blockquote><p>总体步骤分解为：<strong>定位——分析——调整</strong></p><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><ol><li>可视化<ol><li>可视化loss数据</li><li>可视化每个卷积层输出的特征层</li><li>可视化卷积核</li></ol></li><li>可疑之处<ol><li>loss变化减缓</li><li>特征层奇怪</li><li>卷积核奇怪</li></ol></li></ol><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><h3 id="1-出问题的部分在loss数据中如何体现？"><a href="#1-出问题的部分在loss数据中如何体现？" class="headerlink" title="1. 出问题的部分在loss数据中如何体现？"></a>1. 出问题的部分在loss数据中如何体现？</h3><h4 id="1-loss"><a href="#1-loss" class="headerlink" title="1.loss"></a>1.loss</h4><blockquote><p>主要表现为突变</p></blockquote><h4 id="2-histogram"><a href="#2-histogram" class="headerlink" title="2.histogram"></a>2.<strong>histogram</strong></h4><p>【<a href="https://blog.csdn.net/u011668104/article/details/90517879?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164846316016782094836597%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164846316016782094836597&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-90517879.142%5Ev5%5Epc_search_result_control_group,143%5Ev6%5Eregister&utm_term=add_histogram&spm=1018.2226.3001.4187">参考博客</a>】</p><ol><li>训练好的模型权重更加集中，波动范围更小</li><li>初始化很重要，查看初始化权重以及训练好的权重，差异不大的话很有可能网络没有学到任何东西（一般来说其收敛中心有变化，变换范围更小）</li></ol><h3 id="2-探究多任务损失函数系数对训练的影响"><a href="#2-探究多任务损失函数系数对训练的影响" class="headerlink" title="2. 探究多任务损失函数系数对训练的影响"></a>2. 探究多任务损失函数系数对训练的影响</h3><blockquote><p>就风格迁移模型，风格损失刚开始在1200左右，内容损失在[0,20]，</p></blockquote><ol><li><p>将风格损失的权重设置为[1,50,100]实际上在图片上的影响效果基本没有影响，各自图片变化阶段也基本相同——<mark>说明系数并不会影响对应loss约束的特征在训练中提早&#x2F;延后表征</mark></p></li><li><p>但是随着风格损失权重增大（下图1000-1-50-100），可发现风格损失权重越大，内容损失会随着训练次数逐渐升高（越来越抽象且和原图像差距越来越大）——<mark>说明调整loss权重，只是更关注某个loss约束的特征，该特征表征能力保持不变——表现为loss变化幅度基本没有，其他特征表征能力下降——表现为loss原本应该下降或平缓，在增大其他权重时，loss开始上升</mark></p></li></ol><center> content_loss </center><p><img src="/../img/1000-1-50-100.png" alt="训练图片"></p><p>3.从模型的三个loss曲线中观察，权重较大的loss基本没有变换，相反权重较小的loss从原来的收敛变成增加（从上往下风格损失权重：1-50-100）</p><center> content_loss-------------------loss--------------------style_loss </center><p><img src="/../img/1000-1-50-100-all_loss.png" alt="训练图片"></p><ol start="3"><li>直方图基本没有任何变换</li></ol>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能学习建议</title>
      <link href="/2022/03/26/work/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/"/>
      <url>/2022/03/26/work/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E5%BB%BA%E8%AE%AE/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="1783964657b2d4989af6f3221d07e0f40b81e343c8c9496ed0a2813ef2ce4e28">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper</title>
      <link href="/2022/03/25/Paper/"/>
      <url>/2022/03/25/Paper/</url>
      
        <content type="html"><![CDATA[<h1 id="文献总览"><a href="#文献总览" class="headerlink" title="文献总览"></a>文献总览</h1><h2 id="图像修复"><a href="#图像修复" class="headerlink" title="图像修复"></a>图像修复</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Generative Image Inpainting with Contextual Attention</a></td><td align="center">使用背景信息填补前景遮盖，做相似度计算</td><td align="center">None</td></tr><tr><td align="center"><a href="/2022/04/07/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D1/" title="[gan]High-Fidelity GAN Inversion for Image Attribute Editing">[gan]High-Fidelity GAN Inversion for Image Attribute Editing</a></td><td align="center">使用失真图▽的特征图加入到decoder的low space中，还提到有差异的两张图如何对齐</td><td align="center">None</td></tr><tr><td align="center"><a href="/2022/04/09/paper/%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D2/" title="Image Inpainting with External-internal Learning and Monochromic Bottleneck">Image Inpainting with External-internal Learning and Monochromic Bottleneck</a></td><td align="center">优化缺失区域来间接引导非缺失区域着色</td><td align="center">None</td></tr></tbody></table><h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="/2022/04/10/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB1/" title="StyTr2:Image Style Transfer with Transformers">StyTr2:Image Style Transfer with Transformers</a></td><td align="center">根据输入图片学习一个位置标签（硬编码-&gt;软编码）</td><td align="center">仅值得参考</td></tr><tr><td align="center"><a href="/2022/04/11/paper/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB2/" title="ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows">ArtFlow：Unbiased Image Style Transfer via Reversible Neural Flows</a></td><td align="center">特征提取与图像重建结构完全对称</td><td align="center">None</td></tr></tbody></table><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Training Generative Adversarial Networks in One Stage</a></td><td align="center">将GAN的双阶段训练改为单阶段训练，但无法多次训练D，只训练一次G</td><td align="center">训练源码，没有任何效果</td></tr></tbody></table><h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><table><thead><tr><th align="center">Paper</th><th align="center">New Idea</th><th align="center">Review</th></tr></thead><tbody><tr><td align="center"><a href="">Axial attention in multidimesional transformers</a></td><td align="center">将整幅图片进行patch转换成仅关注轴向信息，有效降低transformer计算量</td><td align="center">None</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客参考资料</title>
      <link href="/2022/03/18/blog/%E5%8D%9A%E5%AE%A2%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/"/>
      <url>/2022/03/18/blog/%E5%8D%9A%E5%AE%A2%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<p>主题配置：<a href="https://butterfly.js.org/posts/ceeb73f/">butterfly</a></p><p>学习视频：<a href="https://www.bilibili.com/video/BV1Rg41177pX/?spm_id_from=333.788">视频</a></p><p>博客推荐：<a href="https://zfe.space/">小冰博客</a></p>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练loss总览</title>
      <link href="/2022/03/17/fragments/%E8%AE%AD%E7%BB%83loss%E6%80%BB%E8%A7%88/"/>
      <url>/2022/03/17/fragments/%E8%AE%AD%E7%BB%83loss%E6%80%BB%E8%A7%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>第一次使用tensorboard可视化loss</p></blockquote><p><img src="/../img/md1.jpg" alt="训练图片"></p>]]></content>
      
      
      <categories>
          
          <category> life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Fragments </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>github响应超时</title>
      <link href="/2022/03/17/blog/github%E8%B6%85%E6%97%B6%E7%9B%B8%E5%BA%94%E9%97%AE%E9%A2%98/"/>
      <url>/2022/03/17/blog/github%E8%B6%85%E6%97%B6%E7%9B%B8%E5%BA%94%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：github相应超时，经常无法快速连接</p></blockquote><p>解决：更改本地hosts文件，文件位置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Windows\System32\drivers\etc\hosts</span><br></pre></td></tr></table></figure><h1 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h1><p>打开上述hosts文件增加如下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#           140.82.112.4       github.com</span><br></pre></td></tr></table></figure><h1 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h1><ul><li>进入<a href="https://gitee.com/doshengl/GitHub520">相关网址</a>，复制其中的hosts地址粘贴到本地文件即可——该网站会实时更新地址</li></ul>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo部署中SSH相关问题</title>
      <link href="/2022/03/17/blog/hexo%E9%83%A8%E7%BD%B2%E4%B8%ADSSH%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"/>
      <url>/2022/03/17/blog/hexo%E9%83%A8%E7%BD%B2%E4%B8%ADSSH%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<blockquote><p>问题描述：将github账户作为仓库存储代码，或者将其作为远端时，出现相关报错或者提示——存在ssh相关信息</p></blockquote><h1 id="问题一："><a href="#问题一：" class="headerlink" title="问题一："></a>问题一：</h1><p>下方是在hexo中将Github作为远端，使用<code>heox d</code>远端推送时出现ssh密钥失效的问题，具体错误代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fatal: unable to access <span class="string">&#x27;https://github.com/Reggci/Reggci.github.io/&#x27;</span>: OpenSSL SSL_read: Connection was reset, errno <span class="number">10054</span></span><br><span class="line">FATAL &#123;</span><br><span class="line">  err: Error: Spawn failed</span><br><span class="line">      at ChildProcess.&lt;anonymous&gt; (E:\blog\node_modules\hexo-util\lib\spawn.js:<span class="number">51</span>:<span class="number">21</span>)</span><br><span class="line">      at ChildProcess.emit (node:events:<span class="number">390</span>:<span class="number">28</span>)</span><br><span class="line">      at ChildProcess.cp.emit (E:\blog\node_modules\cross-spawn\lib\enoent.js:<span class="number">34</span>:<span class="number">29</span>)</span><br><span class="line">      at Process.ChildProcess._handle.onexit (node:internal/child_process:<span class="number">290</span>:<span class="number">12</span>) &#123;</span><br><span class="line">    code: <span class="number">128</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125; Something<span class="string">&#x27;s wrong. Maybe you can find the solution here: %s https://hexo.io/docs/troubleshooting.html</span></span><br></pre></td></tr></table></figure><ul><li>解决：</li></ul><blockquote><ol><li>可能是时间久了后本地与Github连接失效</li><li>可能是网络延迟无法push</li></ol></blockquote><ol><li>首先查看本地用户名以及邮箱是否是自己的Github（可跳过步骤）</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git config user.name  <span class="comment"># 查看本地用户名配置</span></span><br><span class="line">git config user.email <span class="comment"># 查看本地邮箱配置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果不是重新配置即可</span></span><br><span class="line">git config --<span class="keyword">global</span> user.name <span class="string">&#x27;xxxx&#x27;</span></span><br><span class="line">git config --<span class="keyword">global</span> user.email <span class="string">&#x27;xxxx&#x27;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>生成本地公钥</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&quot;your github 注册邮箱&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>打开 C:\Users.ssh文件夹，将公钥文件中的内容复制到GitHub的SSH中</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">id_rsa<span class="comment"># 私钥</span></span><br><span class="line">id_rsa.pub<span class="comment"># 公钥</span></span><br></pre></td></tr></table></figure><ol start="4"><li>检测SSH Key生效</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> work </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
